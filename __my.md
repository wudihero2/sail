# Sail åŸ·è¡Œæµç¨‹ï¼šå¾ PySpark åˆ°çµæœ

æˆ‘çš„ç°¡å–®ç†è§£ï¼Œå¾ PySpark client ç™¼é€è«‹æ±‚åˆ°æ‹¿åˆ°çµæœçš„å®Œæ•´æµç¨‹ã€‚

---

## 1. gRPC Requests

**ç™¼ç”Ÿä»€éº¼**ï¼šPySpark client é€é Spark Connect protocol ç™¼é€è«‹æ±‚

**ç¯„ä¾‹**ï¼š
```python
# PySpark code
spark = SparkSession.builder.remote("sc://localhost:50051").getOrCreate()
df = spark.sql("SELECT * FROM users WHERE age > 18")
df.show()
```

**å‚³è¼¸æ ¼å¼**ï¼š
- Protocol: gRPC (Spark Connect Protocol)
- Request åŒ…å«ï¼šSpark Logical Plan (protobuf æ ¼å¼)

### ğŸ”¸ ExecutePlanRequest çµæ§‹

å®Œæ•´çš„ gRPC request å…§å®¹ï¼ˆå®šç¾©åœ¨ `crates/sail-spark-connect/proto/spark/connect/base.proto:293`ï¼‰ï¼š

```rust
ExecutePlanRequest {
    session_id: "abc123",           // UUID æ ¼å¼çš„ session æ¨™è­˜ç¬¦
    user_context: Some(UserContext {
        user_id: "user1",            // ç”¨æˆ¶ ID
    }),
    operation_id: Some("op456"),     // (å¯é¸) æ“ä½œ ID
    plan: Some(Plan {                // è¦åŸ·è¡Œçš„é‚è¼¯è¨ˆåŠƒ
        op_type: Some(Root(Relation {
            rel_type: Some(Sql(Sql {
                query: "SELECT * FROM users WHERE age > 18",
            }))
        }))
    }),
    tags: [],                        // (å¯é¸) æ¨™ç±¤
    request_options: [],             // (å¯é¸) è«‹æ±‚é¸é …
}
```

**çµæ§‹è§£æ**ï¼š

1. **Plan** (`base.proto:38`)
```protobuf
message Plan {
  oneof op_type {
    Relation root = 1;      // æŸ¥è©¢è¨ˆåŠƒ
    Command command = 2;    // å‘½ä»¤ï¼ˆå¦‚ createTable, dropTableï¼‰
  }
}
```

2. **Relation** (`relations.proto:37`)
```protobuf
message Relation {
  RelationCommon common = 1;
  oneof rel_type {
    Read read = 2;          // è®€å–æ“ä½œï¼šspark.read.parquet()
    Project project = 3;    // æŠ•å½±ï¼šselect(col1, col2)
    Filter filter = 4;      // éæ¿¾ï¼šfilter(col > 10)
    Join join = 5;          // é€£æ¥ï¼šdf1.join(df2)
    SetOperation set_op = 6;  // é›†åˆæ“ä½œï¼šunion, intersect
    Sort sort = 7;          // æ’åºï¼šorderBy(col)
    Limit limit = 8;        // é™åˆ¶ï¼šlimit(10)
    Aggregate aggregate = 9;  // èšåˆï¼šgroupBy().agg()
    SQL sql = 10;           // SQL æŸ¥è©¢
    // ... é‚„æœ‰ 40+ ç¨® relation é¡å‹
  }
}
```

3. **SQL** (ç•¶ rel_type æ˜¯ SQL æ™‚)
```protobuf
message SQL {
  string query = 1;         // SQL æŸ¥è©¢å­—ä¸²
  optional Relation input = 2;  // (å¯é¸) è¼¸å…¥é—œä¿‚
}
```

**ç¯„ä¾‹å ´æ™¯å°æ‡‰**ï¼š

| PySpark ä»£ç¢¼ | rel_type å€¼ |
|-------------|------------|
| `spark.sql("SELECT ...")` | Sql |
| `df.filter(col("age") > 18)` | Filter |
| `df.select("name", "age")` | Project |
| `df.groupBy("city").count()` | Aggregate |
| `df.orderBy("age")` | Sort |
| `df1.join(df2, "id")` | Join |

**ä½ç½®**ï¼šå®¢æˆ¶ç«¯ â†’ Sail Server

---

## 2. Spark Connect Server

**ç™¼ç”Ÿä»€éº¼**ï¼šSail çš„ gRPC server æ¥æ”¶ä¸¦è§£æè«‹æ±‚

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-spark-connect/src/server.rs:54` - `SparkConnectServer::execute_plan`
- å¯¦ä½œ Spark Connect çš„ gRPC service

### ğŸ”¸ å®Œæ•´èª¿ç”¨éˆï¼šexecute_plan

**å…¥å£é»**ï¼š`SparkConnectServer::execute_plan`

```rust
#[tonic::async_trait]
impl SparkConnectService for SparkConnectServer {
    async fn execute_plan(
        &self,
        request: Request<ExecutePlanRequest>,
    ) -> Result<Response<Self::ExecutePlanStream>, Status> {
        // 1. æå– request
        let request = request.into_inner();

        // 2. æå– session è³‡è¨Š
        let session_key = SessionKey {
            user_id: request.user_context.map(|u| u.user_id).unwrap_or_default(),
            session_id: request.session_id,
        };

        // 3. æå– metadataï¼ˆoperation_id, tags, reattachableï¼‰
        let metadata = ExecutorMetadata { ... };

        // 4. å–å¾—æˆ–å‰µå»º SessionContext
        let ctx = self
            .session_manager
            .get_or_create_session_context(session_key)
            .await?;

        // 5. æå– Plan
        let Plan { op_type: op } = request.plan.required("plan")?;
        let op = op.required("plan op")?;

        // 6. æ ¹æ“š Plan é¡å‹åˆ†æ´¾
        let stream = match op {
            plan::OpType::Root(relation) => {
                // æŸ¥è©¢ï¼šSELECT, FROM, WHERE ç­‰
                service::handle_execute_relation(&ctx, relation, metadata).await?
            }
            plan::OpType::Command(Command { command_type }) => {
                // å‘½ä»¤ï¼šRegisterFunction, WriteOperation ç­‰
                // ...
            }
        };

        Ok(Response::new(stream))
    }
}
```

### ğŸ”¸ è¼¸å…¥ç¯„ä¾‹

å°æ–¼æŸ¥è©¢ `SELECT * FROM users WHERE age > 18`ï¼Œæ¥æ”¶åˆ°çš„ requestï¼š

```rust
ExecutePlanRequest {
    session_id: "abc123",
    user_context: Some(UserContext {
        user_id: "user1",
    }),
    operation_id: Some("op456"),
    plan: Some(Plan {
        op_type: Some(Root(Relation {
            rel_type: Some(Sql(Sql {
                query: "SELECT * FROM users WHERE age > 18",
            }))
        }))
    }),
    tags: [],
    request_options: [],
}
```

### ğŸ”¸ èª¿ç”¨éˆ

```
SparkConnectServer::execute_plan [server.rs:54]
  â†“
service::handle_execute_relation [plan_executor.rs:147]
  â†“
Relation::try_into() â†’ spec::Plan [proto/plan.rs]
  â†“
handle_execute_plan [plan_executor.rs:91]
  â†“
resolve_and_execute_plan [plan_executor.rs:41]
```

### ğŸ”¸ Debug è¼¸å‡ºç¯„ä¾‹

```rust
// åœ¨ execute_plan é–‹é ­åŠ å…¥
println!("=== 1. æ¥æ”¶ gRPC Request ===");
println!("Session ID: {}", request.session_id);
println!("Plan: {:#?}", request.plan);
```

è¼¸å‡ºï¼š
```
=== 1. æ¥æ”¶ gRPC Request ===
Session ID: abc123
Plan: Plan {
    op_type: Some(Root(Relation {
        rel_type: Some(Sql(Sql {
            query: "SELECT * FROM users WHERE age > 18"
        }))
    }))
}
```

**è¼¸å…¥**ï¼šgRPC ExecutePlanRequest (protobuf)
**è¼¸å‡º**ï¼šé–‹å§‹è™•ç† Relation

---

## 3. gRPC -> Spark Plan

**ç™¼ç”Ÿä»€éº¼**ï¼šå°‡ protobuf æ ¼å¼çš„ Spark plan è§£ææˆ Sail å…§éƒ¨çš„ Spark Plan çµæ§‹

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-spark-connect/src/service/plan_executor.rs:147` - `handle_execute_relation`
- `crates/sail-spark-connect/src/proto/plan.rs` - `impl TryFrom<Relation> for spec::Plan`

### ğŸ”¸ æ­¥é©Ÿ 1ï¼šè™•ç† Relation

```rust
pub(crate) async fn handle_execute_relation(
    ctx: &SessionContext,
    relation: Relation,
    metadata: ExecutorMetadata,
) -> SparkResult<ExecutePlanResponseStream> {
    // å°‡ protobuf Relation è½‰æ›æˆ Sail çš„ spec::Plan
    let plan = relation.try_into()?;

    // åŸ·è¡Œ plan
    handle_execute_plan(ctx, plan, metadata, ExecutePlanMode::Lazy).await
}
```

**è¼¸å…¥ Relation**ï¼š
```rust
Relation {
    common: None,
    rel_type: Some(Sql(Sql {
        query: "SELECT * FROM users WHERE age > 18",
        args: {},
        pos_args: [],
    }))
}
```

### ğŸ”¸ æ­¥é©Ÿ 2ï¼šProtobuf â†’ Sail Plan è½‰æ›

**ä½ç½®**ï¼š`crates/sail-spark-connect/src/proto/plan.rs`

```rust
impl TryFrom<Relation> for spec::Plan {
    type Error = SparkError;

    fn try_from(relation: Relation) -> Result<Self, Self::Error> {
        let rel_type = relation.rel_type.required("relation type")?;

        match rel_type {
            RelType::Sql(sql) => {
                // è½‰æ› SQL æŸ¥è©¢
                Ok(spec::Plan::Query(spec::QueryPlan::new(
                    spec::QueryNode::Sql(spec::SqlNode {
                        query: sql.query,
                        args: sql.args,
                        pos_args: sql.pos_args,
                    })
                )))
            }
            RelType::Read(read) => {
                // è½‰æ› Read (table scan)
                Ok(spec::Plan::Query(spec::QueryPlan::new(
                    spec::QueryNode::Read(read.try_into()?)
                )))
            }
            RelType::Project(project) => {
                // è½‰æ› Projection
                // ...
            }
            RelType::Filter(filter) => {
                // è½‰æ› Filter
                // ...
            }
            RelType::Join(join) => {
                // è½‰æ› Join
                // ...
            }
            RelType::Aggregate(agg) => {
                // è½‰æ› Aggregate
                // ...
            }
            // ... æ›´å¤šé¡å‹
        }
    }
}
```

### ğŸ”¸ è¼¸å‡ºï¼šspec::Plan

å°æ–¼æˆ‘å€‘çš„æŸ¥è©¢ï¼Œè½‰æ›çµæœï¼š

```rust
spec::Plan::Query(QueryPlan {
    node: QueryNode::Sql(SqlNode {
        query: "SELECT * FROM users WHERE age > 18",
        args: {},
        pos_args: [],
    })
})
```

### ğŸ”¸ Relation çš„å„ç¨®é¡å‹

| RelType | ç”¨é€” | ç¯„ä¾‹ |
|---------|------|------|
| `Sql` | SQL æŸ¥è©¢ | `SELECT * FROM users` |
| `Read` | è®€å– table | `df = spark.read.parquet("file.parquet")` |
| `Project` | é¸æ“‡æ¬„ä½ | `df.select("name", "age")` |
| `Filter` | éæ¿¾ | `df.filter("age > 18")` |
| `Join` | Join æ“ä½œ | `df1.join(df2, "id")` |
| `Aggregate` | èšåˆ | `df.groupBy("dept").count()` |
| `Sort` | æ’åº | `df.orderBy("age")` |
| `Limit` | é™åˆ¶è¡Œæ•¸ | `df.limit(10)` |

### ğŸ”¸ Debug è¼¸å‡ºç¯„ä¾‹

```rust
// åœ¨ try_into() å…§åŠ å…¥
println!("=== 2. Relation â†’ spec::Plan ===");
println!("Input relation type: {:?}", relation.rel_type);
let plan = // ... è½‰æ›é‚è¼¯
println!("Output plan: {:#?}", plan);
```

è¼¸å‡ºï¼š
```
=== 2. Relation â†’ spec::Plan ===
Input relation type: Some(Sql(Sql {
    query: "SELECT * FROM users WHERE age > 18"
}))
Output plan: Query(QueryPlan {
    node: Sql(SqlNode {
        query: "SELECT * FROM users WHERE age > 18"
    })
})
```

**è¼¸å…¥**ï¼šprotobuf Relation
**è¼¸å‡º**ï¼šspec::Plan (Sail å…§éƒ¨æ ¼å¼)

---

## 4. Spark Plan -> Sail Plan

**ç™¼ç”Ÿä»€éº¼**ï¼šå°‡ Spark çš„ logical plan è½‰æ›æˆ Sail è‡ªå·±çš„ logical plan

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-plan/src/resolver/` - Plan è§£æå’Œè½‰æ›
- `crates/sail-plan/src/resolver/plan.rs` - ä¸»è¦çš„ plan resolver

**ä¸»è¦å·¥ä½œ**ï¼š
- **Relation è½‰æ›**ï¼šSpark Relation â†’ Sail Relation
  - Scan â†’ è³‡æ–™ä¾†æºè®€å–
  - Join â†’ join é‚è¼¯
  - Aggregate â†’ èšåˆé‚è¼¯
  - Filter â†’ éæ¿¾æ¢ä»¶

- **Expression è½‰æ›**ï¼šSpark Expression â†’ Sail Expression
  - Column references
  - Functions (Spark functions â†’ DataFusion functions)
  - Literals
  - Operators

- **å‡½æ•¸å°æ‡‰**ï¼š
  - Spark å‡½æ•¸åç¨± â†’ DataFusion å‡½æ•¸
  - ä¾‹å¦‚ï¼š`array_insert` â†’ è‡ªè¨‚å¯¦ä½œ
  - ä½ç½®ï¼š`crates/sail-plan/src/function/scalar/`

**è¼¸å…¥**ï¼šSpark Plan
**è¼¸å‡º**ï¼šSail Plan (ä»æ˜¯ logical planï¼Œä½†å·²ç¶“æ˜¯ Sail çš„æ ¼å¼)

---

## 5. Sail Plan -> DataFusion Logical Plan

**ç™¼ç”Ÿä»€éº¼**ï¼šå°‡ Sail çš„ logical plan è½‰æ›æˆ DataFusion çš„ LogicalPlan

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-plan/src/` - Plan è½‰æ›é‚è¼¯
- DataFusion çš„ `LogicalPlan` é¡å‹

**ä¸»è¦å·¥ä½œ**ï¼š
- è½‰æ›æˆ DataFusion çš„æ¨™æº– LogicalPlan
- æ˜ å°„è³‡æ–™ä¾†æº (Parquet, Delta Lake, Iceberg)
- æ˜ å°„æ‰€æœ‰ operators (Projection, Filter, Join, Aggregate)
- ç¢ºä¿å‡½æ•¸éƒ½æœ‰å°æ‡‰çš„å¯¦ä½œ

**é—œéµæ¦‚å¿µ**ï¼š
```
Spark SQL: SELECT dept, COUNT(*) FROM users GROUP BY dept
    â†“
Sail Logical Plan: è™•ç† Spark èªç¾©
    â†“
DataFusion Logical Plan:
  - Aggregate [dept], [COUNT(*)]
    - TableScan [users]
```

**è¼¸å…¥**ï¼šSail Plan
**è¼¸å‡º**ï¼šDataFusion LogicalPlan

---

## 6. DataFusion Logical Plan -> Physical Plan

**ç™¼ç”Ÿä»€éº¼**ï¼šDataFusion çš„ query optimizer å°‡ logical plan è½‰æ›æˆ physical plan

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- DataFusion å…§å»ºçš„ optimizer å’Œ planner
- `crates/sail-execution/` - Sail çš„åŸ·è¡Œå±¤

**ä¸»è¦å·¥ä½œ**ï¼š

ğŸ”¸ **Optimization (å„ªåŒ–)**
- Predicate pushdownï¼šå°‡ filter å¾€ä¸‹æ¨
- Projection pushdownï¼šåªè®€å–éœ€è¦çš„æ¬„ä½
- Join reorderingï¼šèª¿æ•´ join é †åº
- Constant foldingï¼šè¨ˆç®—å¸¸æ•¸è¡¨é”å¼

ğŸ”¸ **Partitioning (åˆ†å€)**
- æ±ºå®šè³‡æ–™å¦‚ä½•åˆ†å€
- æ±ºå®šå¹³è¡Œåº¦ (parallelism)

ğŸ”¸ **Physical Operators**
- LogicalPlan operators â†’ PhysicalPlan operators
- ä¾‹å¦‚ï¼š
  - `Filter` â†’ `FilterExec`
  - `Aggregate` â†’ `AggregateExec`
  - `Join` â†’ `HashJoinExec` æˆ– `SortMergeJoinExec`
  - `Scan` â†’ `ParquetExec` / `DeltaScanExec`

**ç¯„ä¾‹è½‰æ›**ï¼š
```
Logical Plan:
  Aggregate [dept], [COUNT(*)]
    Filter [age > 18]
      TableScan [users]

Physical Plan:
  AggregateExec (final)
    CoalescePartitionsExec
      AggregateExec (partial) [4 partitions]
        FilterExec [age > 18]
          ParquetExec [users.parquet] [4 partitions]
```

**è¼¸å…¥**ï¼šDataFusion LogicalPlan
**è¼¸å‡º**ï¼šDataFusion ExecutionPlan (Physical Plan)

---

## 6.5 Sail çš„é¡å¤–è™•ç†ï¼šJobGraph å»ºæ§‹ï¼ˆåˆ†æ•£å¼åŸ·è¡Œå°ˆç”¨ï¼‰

**ç™¼ç”Ÿä»€éº¼**ï¼šSail åœ¨ Physical Plan ç”Ÿæˆå¾Œï¼Œæ’å…¥ Shuffle æ©Ÿåˆ¶ä¾†æ”¯æ´åˆ†æ•£å¼åŸ·è¡Œ

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-execution/src/driver/planner.rs` - JobGraph å»ºæ§‹
- `crates/sail-execution/src/plan/shuffle_write.rs` - ShuffleWriteExec
- `crates/sail-execution/src/plan/shuffle_read.rs` - ShuffleReadExec

**æ™‚æ©Ÿ**ï¼š
- åªåœ¨ **Cluster Mode** æ‰éœ€è¦
- Local Mode ç›´æ¥åŸ·è¡Œ Physical Plan
- Physical Plan ç”Ÿæˆå¾Œã€åŸ·è¡Œå‰

### ğŸ”¸ æ ¸å¿ƒå·¥ä½œï¼šåˆ‡åˆ† Stages

**JobGraph åšä»€éº¼**ï¼š
```rust
pub struct JobGraph {
    stages: Vec<Arc<dyn ExecutionPlan>>,  // å°‡ Physical Plan åˆ‡åˆ†æˆå¤šå€‹ Stage
}
```

1. **éæ­· Physical Plan**
2. **è­˜åˆ¥ Shuffle é‚Šç•Œ**ï¼š
   - `RepartitionExec` (hash/range é‡æ–°åˆ†å€)
   - `CoalescePartitionsExec` (åˆä½µ partitions)
3. **æ’å…¥ Shuffle Operators**ï¼š
   - `ShuffleWriteExec` - å¯«å…¥ shuffle è³‡æ–™
   - `ShuffleReadExec` - è®€å– shuffle è³‡æ–™

### ğŸ”¸ è½‰æ›ç¯„ä¾‹

**DataFusion åŸå§‹ Physical Plan**ï¼š
```
AggregateExec (final)
  CoalescePartitionsExec        â† Shuffle é‚Šç•Œ
    AggregateExec (partial) [4 partitions]
      FilterExec [age > 18]
        ParquetExec [4 partitions]
```

**Sail çš„ JobGraph åˆ‡åˆ†å¾Œ**ï¼š
```
Stage 1: (æœ€çµ‚ Stage)
  AggregateExec (final)
    ShuffleReadExec (stage=0, partitions=4)
      â†‘
      | å¾ Stage 0 è®€å– shuffle è³‡æ–™
      |

â”€â”€â”€â”€â”€â”€â”€ Shuffle é‚Šç•Œ â”€â”€â”€â”€â”€â”€â”€

Stage 0:
  ShuffleWriteExec (stage=0, partitions=4)
    â†“ å¯«å…¥ shuffle storage
    AggregateExec (partial) [4 partitions]
      FilterExec [age > 18]
        ParquetExec [4 partitions]
```

### ğŸ”¸ ShuffleWriteExec

**åŠŸèƒ½**ï¼š
- åŸ·è¡Œ child plan (ä¾‹å¦‚ `AggregateExec (partial)`)
- æ ¹æ“š hash(dept) é‡æ–°åˆ†å€è³‡æ–™
- å°‡è³‡æ–™å¯«å…¥ shuffle storage
- ä¾›ä¸‹ä¸€å€‹ Stage è®€å–

**é—œéµæ¬„ä½**ï¼š
```rust
pub struct ShuffleWriteExec {
    stage: usize,                           // å±¬æ–¼å“ªå€‹ Stage
    plan: Arc<dyn ExecutionPlan>,           // è¦åŸ·è¡Œçš„ child plan
    shuffle_partitioning: Partitioning,     // å¦‚ä½•åˆ†å€ (ä¾‹å¦‚ Hash(dept, 4))
    locations: Vec<Vec<TaskWriteLocation>>, // æ¯å€‹ partition å¯«å…¥å“ªè£¡
}
```

### ğŸ”¸ ShuffleReadExec

**åŠŸèƒ½**ï¼š
- å¾ shuffle storage è®€å–è³‡æ–™
- å¾å‰ä¸€å€‹ Stage çš„å¤šå€‹ Tasks è®€å–
- åˆä½µæˆä¸€å€‹ RecordBatchStream

**é—œéµæ¬„ä½**ï¼š
```rust
pub struct ShuffleReadExec {
    stage: usize,                          // å¾å“ªå€‹ Stage è®€å–
    locations: Vec<Vec<TaskReadLocation>>, // å¾å“ªäº›ä½ç½®è®€å–
}
```

### ğŸ”¸ åŸ·è¡Œæµç¨‹

**Stage 0 (åœ¨ 4 å€‹ Workers ä¸Šä¸¦è¡ŒåŸ·è¡Œ)**ï¼š
```
Task 0: ParquetExec [partition 0]
        â†’ FilterExec
        â†’ AggregateExec (partial)
        â†’ ShuffleWriteExec
          â†’ hash(dept) â†’ å¯«å…¥ 4 å€‹è¼¸å‡º partitions

Task 1: ParquetExec [partition 1] â†’ ... â†’ ShuffleWriteExec
Task 2: ParquetExec [partition 2] â†’ ... â†’ ShuffleWriteExec
Task 3: ParquetExec [partition 3] â†’ ... â†’ ShuffleWriteExec
```

**Shuffle Storage çµ„ç¹”**ï¼š
```
Output Partition 0: {dept="IT" çš„æ‰€æœ‰è³‡æ–™}
Output Partition 1: {dept="HR" çš„æ‰€æœ‰è³‡æ–™}
Output Partition 2: {dept="Ops" çš„æ‰€æœ‰è³‡æ–™}
Output Partition 3: {å…¶ä»– dept çš„è³‡æ–™}
```

**Stage 1 (è®€å–ä¸¦å®Œæˆèšåˆ)**ï¼š
```
Task 0: ShuffleReadExec [è®€å– partition 0 from all Stage 0 tasks]
        â†’ AggregateExec (final)
        â†’ è¿”å›çµæœ
```

### ğŸ”¸ ç‚ºä»€éº¼éœ€è¦ Shuffleï¼Ÿ

**å•é¡Œ**ï¼šåˆ†æ•£å¼ç’°å¢ƒä¸­çš„ GROUP BY

```
Worker 1 æœ‰: [("IT", 50), ("HR", 20)]
Worker 2 æœ‰: [("IT", 50), ("HR", 30)]  â† ç›¸åŒ dept çš„è³‡æ–™åˆ†æ•£åœ¨ä¸åŒæ©Ÿå™¨
```

**è§£æ±º**ï¼šShuffle é‡æ–°åˆ†å€

```
Shuffle å¾Œ:
Worker 1 è² è²¬: æ‰€æœ‰ "IT" çš„è³‡æ–™   â†’ COUNT = 100
Worker 2 è² è²¬: æ‰€æœ‰ "HR" çš„è³‡æ–™   â†’ COUNT = 50
```

### ğŸ”¸ å°æ¯”ï¼šLocal Mode vs Cluster Mode

| é …ç›® | Local Mode | Cluster Mode (JobGraph) |
|------|-----------|------------------------|
| Physical Plan | ç›´æ¥åŸ·è¡Œ | æ’å…¥ ShuffleWrite/ReadExec |
| Partitions | è¨˜æ†¶é«”å…§å‚³é | é€é shuffle storage |
| åŸ·è¡Œå–®ä½ | Thread | Task (åˆ†æ•£åœ¨å¤šå€‹ Workers) |
| å”èª¿ | ä¸éœ€è¦ | DriverActor å”èª¿ |
| Shuffle | CoalescePartitionsExec | ShuffleWrite + ShuffleRead |

### ğŸ”¸ ç¸½çµ

**Sail çš„å‰µæ–°ä¹‹è™•**ï¼š
1. **ä¸ä¿®æ”¹ DataFusion optimizer**ï¼šä¿ç•™æ‰€æœ‰å„ªåŒ–èƒ½åŠ›
2. **åœ¨ Physical Plan å¾ŒåŠ å…¥ Shuffle**ï¼šæ”¯æ´åˆ†æ•£å¼åŸ·è¡Œ
3. **é€æ˜åˆ‡åˆ†**ï¼šè‡ªå‹•è­˜åˆ¥ Shuffle é‚Šç•Œï¼Œåˆ‡åˆ†æˆ Stages
4. **çµ±ä¸€ä»‹é¢**ï¼šLocal å’Œ Cluster ç”¨ç›¸åŒçš„ API

**é—œéµæ¦‚å¿µ**ï¼š
- **Stage**ï¼šå¯ä»¥ç¨ç«‹åŸ·è¡Œçš„ä¸€çµ„ operators
- **Shuffle**ï¼šStage ä¹‹é–“çš„è³‡æ–™é‡æ–°åˆ†å€å’Œå‚³è¼¸
- **Task**ï¼šStage ä¸­æ¯å€‹ partition çš„åŸ·è¡Œå–®å…ƒ

**è¼¸å…¥**ï¼šDataFusion ExecutionPlan
**è¼¸å‡º**ï¼šJobGraph (å¤šå€‹ Stagesï¼ŒåŒ…å« Shuffle operators)

---

## 7. Physical Plan -> Execution

**ç™¼ç”Ÿä»€éº¼**ï¼šå¯¦éš›åŸ·è¡Œ physical planï¼Œè®€å–è³‡æ–™ä¸¦è¨ˆç®—çµæœ

**åŸ·è¡Œæ¨¡å¼**ï¼š

### ğŸ”¸ Local Mode (æœ¬åœ°åŸ·è¡Œ)

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-execution/src/local/` - Local execution

**æµç¨‹**ï¼š
```
ExecutionPlan.execute(partition)
  â†“
è¿”å› RecordBatchStream
  â†“
é€æ‰¹æ¬¡ (batch) è™•ç†è³‡æ–™
  â†“
æ¯å€‹ batch æ˜¯ä¸€å€‹ RecordBatch (Arrow æ ¼å¼)
```

### ğŸ”¸ Cluster Mode (åˆ†æ•£å¼åŸ·è¡Œ)

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-execution/src/driver/` - Driver (å”èª¿è€…)
- `crates/sail-execution/src/worker/` - Worker (åŸ·è¡Œè€…)

**æµç¨‹**ï¼š
```
1. ClusterJobRunner æ¥æ”¶ ExecutionPlan
   â†“
2. DriverActor å»ºæ§‹ JobGraph
   - åˆ†æ Shuffle é‚Šç•Œ
   - åˆ‡åˆ†æˆ Stages
   â†“
3. DriverActor å‰µå»º Tasks
   - æ¯å€‹ Stage çš„æ¯å€‹ partition = 1 å€‹ Task
   â†“
4. DriverActor èª¿åº¦ Tasks åˆ° Workers
   - é€é gRPC ç™¼é€ RunTask
   â†“
5. Workers åŸ·è¡Œ Tasks
   - è®€å–è³‡æ–™ (Parquet, Delta, Iceberg)
   - åŸ·è¡Œè¨ˆç®— (filter, map, aggregate)
   - å¯«å…¥ Shuffle data (å¦‚æœéœ€è¦)
   â†“
6. DriverActor æ”¶é›†çµæœ
   - å¾æœ€çµ‚ Stage çš„ Tasks è®€å–çµæœ
```

**é—œéµæ©Ÿåˆ¶**ï¼š

ğŸ”¸ **Shuffle**
- ç•¶éœ€è¦é‡æ–°åˆ†å€æ™‚ç™¼ç”Ÿ
- ä¾‹å¦‚ï¼šGROUP BY, JOIN
- Stage ä¹‹é–“çš„é‚Šç•Œ
- å¯¦ä½œï¼š
  - `ShuffleWriteExec` - å¯«å…¥ shuffle è³‡æ–™
  - `ShuffleReadExec` - è®€å– shuffle è³‡æ–™
  - ä½ç½®ï¼š`crates/sail-execution/src/shuffle/`

ğŸ”¸ **Task ç‹€æ…‹**
```
Created â†’ Pending â†’ Scheduled â†’ Running â†’ Succeeded
                                       â†’ Failed (é‡è©¦)
```

ğŸ”¸ **è³‡æ–™æ ¼å¼**
- Apache Arrowï¼šè¨˜æ†¶é«”ä¸­çš„åˆ—å¼æ ¼å¼
- RecordBatchï¼šä¸€æ‰¹è³‡æ–™è¨˜éŒ„
- Schemaï¼šè³‡æ–™çš„çµæ§‹å®šç¾©

**è¼¸å…¥**ï¼šExecutionPlan
**è¼¸å‡º**ï¼šRecordBatchStream (è³‡æ–™æµ)

---

## 8. Execution -> Results

**ç™¼ç”Ÿä»€éº¼**ï¼šå°‡åŸ·è¡Œçµæœè¿”å›çµ¦ client

**å°æ‡‰ç¨‹å¼ç¢¼**ï¼š
- `crates/sail-spark-connect/src/service.rs` - å›å‚³çµæœçµ¦ client

**æµç¨‹**ï¼š

### ğŸ”¸ çµæœæ”¶é›†

```
RecordBatchStream
  â†“
é€æ‰¹æ¬¡è®€å– RecordBatch
  â†“
è½‰æ›æˆ Spark Connect æ ¼å¼ (protobuf)
  â†“
é€é gRPC stream å›å‚³çµ¦ client
```

### ğŸ”¸ è³‡æ–™è½‰æ›

```
Arrow RecordBatch (Sail å…§éƒ¨)
  â†“
Arrow IPC format (åºåˆ—åŒ–)
  â†“
Spark Connect ExecutePlanResponse (protobuf)
  â†“
gRPC stream
  â†“
PySpark client æ¥æ”¶ä¸¦è§£æ
```

### ğŸ”¸ ä¸²æµè™•ç†

- çµæœæ˜¯**ä¸²æµ**çš„ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§å…¨éƒ¨å›å‚³
- æ¯å€‹ batch å¤§ç´„ 8192 rows (å¯é…ç½®)
- Client å¯ä»¥é‚Šæ¥æ”¶é‚Šè™•ç†

**ç¯„ä¾‹è¼¸å‡º**ï¼š
```
+----+-----+
|dept|count|
+----+-----+
|  IT|  100|
|  HR|   50|
| Ops|   75|
+----+-----+
```

**è¼¸å…¥**ï¼šRecordBatchStream
**è¼¸å‡º**ï¼šgRPC response stream â†’ PySpark DataFrame

---

## å®Œæ•´æµç¨‹åœ–

```
PySpark Client
    |
    | 1. gRPC Request (Spark Connect Protocol)
    v
SparkConnectService (Sail gRPC Server)
    |
    | 2. Parse protobuf
    v
Spark Plan (Spark logical plan çµæ§‹)
    |
    | 3. Spark â†’ Sail è½‰æ›
    | (crates/sail-plan/src/resolver/)
    v
Sail Plan (Sail å…§éƒ¨çš„ logical plan)
    |
    | 4. Sail â†’ DataFusion è½‰æ›
    | (crates/sail-plan/src/)
    v
DataFusion LogicalPlan
    |
    | 5. Optimization + Planning
    | (DataFusion optimizer)
    v
DataFusion ExecutionPlan (Physical Plan)
    |
    | 6. Execution
    |
    +---> Local Mode: ç›´æ¥åŸ·è¡Œ
    |     (crates/sail-execution/src/local/)
    |
    +---> Cluster Mode: åˆ†æ•£å¼åŸ·è¡Œ
          (crates/sail-execution/src/driver/ + worker/)
          |
          | a. JobGraph åˆ‡åˆ† Stages
          | b. å‰µå»º Tasks
          | c. èª¿åº¦åˆ° Workers
          | d. Workers åŸ·è¡Œ
          | e. Shuffle (å¦‚æœéœ€è¦)
          | f. æ”¶é›†çµæœ
          v
RecordBatchStream (Arrow format)
    |
    | 7. Convert to Spark Connect format
    | (crates/sail-spark-connect/src/)
    v
gRPC Response Stream
    |
    | 8. Stream back to client
    v
PySpark Client (DataFrame)
```

---

## é—œéµ Crates å°æ‡‰

| Crate | è² è²¬æµç¨‹ | èªªæ˜ |
|-------|---------|------|
| `sail-spark-connect` | æ­¥é©Ÿ 1, 2, 3, 8 | gRPC server, protobuf è§£æ, çµæœå›å‚³ |
| `sail-plan` | æ­¥é©Ÿ 4, 5 | Spark â†’ Sail â†’ DataFusion è½‰æ› |
| `sail-sql-parser` | (SQL æƒ…æ³) | è§£æ Spark SQL |
| `sail-execution` | æ­¥é©Ÿ 7 | Local/Cluster åŸ·è¡Œ |
| `sail-delta-lake` | æ­¥é©Ÿ 7 | Delta Lake è³‡æ–™ä¾†æº |
| `sail-iceberg` | æ­¥é©Ÿ 7 | Iceberg è³‡æ–™ä¾†æº |
| `sail-python-udf` | æ­¥é©Ÿ 7 | Python UDF æ”¯æ´ |

---

## è³‡æ–™æ ¼å¼è½‰æ›

```
PySpark DataFrame
    â†“ (åºåˆ—åŒ–)
Protobuf (Spark Connect Plan)
    â†“ (è§£æ)
Spark Plan (Rust struct)
    â†“ (è½‰æ›)
Sail Plan (Rust struct)
    â†“ (è½‰æ›)
DataFusion LogicalPlan
    â†“ (å„ªåŒ– + è¦åŠƒ)
DataFusion ExecutionPlan
    â†“ (åŸ·è¡Œ)
Arrow RecordBatch (columnar data)
    â†“ (åºåˆ—åŒ–)
Arrow IPC format
    â†“ (åŒ…è£)
Protobuf (ExecutePlanResponse)
    â†“ (gRPC stream)
PySpark DataFrame
```

---

## é‡è¦æ¦‚å¿µ

### ğŸ”¸ Logical Plan vs Physical Plan

**Logical Plan**ï¼š
- æè¿°ã€Œåšä»€éº¼ã€(WHAT)
- èˆ‡å…·é«”åŸ·è¡Œç„¡é—œ
- å¯ä»¥å„ªåŒ–

**Physical Plan**ï¼š
- æè¿°ã€Œæ€éº¼åšã€(HOW)
- å…·é«”çš„åŸ·è¡Œç­–ç•¥
- åŒ…å«åˆ†å€ã€å¹³è¡Œåº¦ç­‰ç´°ç¯€

### ğŸ”¸ Spark èªç¾© vs DataFusion èªç¾©

**ç‚ºä»€éº¼éœ€è¦ Sail Plan**ï¼š
- Spark å’Œ DataFusion çš„å‡½æ•¸è¡Œç‚ºä¸åŒ
- Spark çš„ array æ˜¯ 1-basedï¼ŒDataFusion æ˜¯ 0-based
- Spark çš„ null è™•ç†é‚è¼¯ä¸åŒ
- éœ€è¦è½‰æ›å±¤ä¾†ä¿è­‰ç›¸å®¹æ€§

### ğŸ”¸ Pull-based vs Push-based Execution

**DataFusion ä½¿ç”¨ Pull-based (Volcano model)**ï¼š
- Parent operator å‘ child operator æ‹‰å–è³‡æ–™
- é€æ‰¹æ¬¡è™•ç† (streaming execution)
- è¨˜æ†¶é«”æ•ˆç‡é«˜

### ğŸ”¸ Columnar Format (åˆ—å¼å„²å­˜)

**Apache Arrow**ï¼š
- è¨˜æ†¶é«”ä¸­çš„åˆ—å¼æ ¼å¼
- CPU cache friendly
- æ”¯æ´ SIMD é‹ç®—
- é›¶æ‹·è²è·¨èªè¨€å‚³é

---

## ç¯„ä¾‹ï¼šä¸€å€‹å®Œæ•´çš„æŸ¥è©¢

**PySpark Code**ï¼š
```python
df = spark.sql("SELECT dept, COUNT(*) as cnt FROM users WHERE age > 18 GROUP BY dept")
df.show()
```

**æµç¨‹è¿½è¹¤**ï¼š

1. **gRPC Request**: åŒ…å« SQL string
2. **Spark Plan**:
   - Filter(age > 18)
   - Aggregate(groupBy=[dept], agg=[COUNT(*)])
3. **Sail Plan**: è½‰æ› Spark èªç¾©
4. **DataFusion Logical**:
   - Aggregate[dept], [COUNT(*)]
     - Filter[age > 18]
       - TableScan[users]
5. **DataFusion Physical**:
   - AggregateExec(final)
     - CoalescePartitionsExec
       - AggregateExec(partial) [4 partitions]
         - FilterExec[age > 18]
           - ParquetExec[users.parquet] [4 partitions]
6. **Execution**:
   - 4 å€‹ tasks è®€å– Parquet ä¸¦åš partial aggregation
   - Shuffle
   - 1 å€‹ task åš final aggregation
7. **Results**:
   - RecordBatch 1: [("IT", 100), ("HR", 50)]
   - RecordBatch 2: [("Ops", 75)]
8. **gRPC Response**: ä¸²æµå›å‚³çµ¦ client

---

## ç¸½çµ

Sail çš„æ ¸å¿ƒåƒ¹å€¼åœ¨æ–¼ï¼š

1. **ç›¸å®¹æ€§**ï¼šå®Œæ•´æ”¯æ´ Spark Connect protocol
2. **æ•ˆèƒ½**ï¼šä½¿ç”¨ Rust + DataFusion ç²å¾—æ›´å¥½çš„æ•ˆèƒ½
3. **é€æ˜æ€§**ï¼šPySpark code ä¸éœ€è¦ä¿®æ”¹
4. **æ“´å±•æ€§**ï¼šæ”¯æ´ local å’Œ cluster mode
5. **æ•´åˆæ€§**ï¼šæ”¯æ´ Delta Lake, Iceberg ç­‰ lakehouse formats

æ•´å€‹æµç¨‹çš„é—œéµå°±æ˜¯**å¤šå±¤è½‰æ›**ï¼š
- Spark â†’ Sail â†’ DataFusion
- æ¯ä¸€å±¤éƒ½è² è²¬ç‰¹å®šçš„èªç¾©è½‰æ›
- æœ€çµ‚ç”± DataFusion åŸ·è¡Œï¼Œåˆ©ç”¨ Rust çš„æ•ˆèƒ½å„ªå‹¢
