# ç¬¬é›¶éšæ®µï¼šPySpark å®¢æˆ¶ç«¯é€£æ¥å»ºç«‹

## ç¯„ä¾‹ SQL

```python
# PySpark Client
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .remote("sc://localhost:50051") \
    .getOrCreate()

result = spark.sql("SELECT 1 + 1 AS sum").collect()
print(result)  # [Row(sum=2)]
```

## ç¯„ä¾‹ä»£ç¢¼

```python
# Line 10-12: å»ºç«‹é€£æ¥
spark = SparkSession.builder \
    .remote("sc://localhost:50051") \
    .getOrCreate()
```

é€™ä¸‰è¡Œä»£ç¢¼çœ‹ä¼¼ç°¡å–®ï¼Œå¯¦éš›ä¸Šæœƒè§¸ç™¼ä¸€ç³»åˆ—è¤‡é›œçš„åˆå§‹åŒ–æµç¨‹ã€‚

---

## å®Œæ•´èª¿ç”¨éˆ

```
PySpark Client                        Network                      Sail Server
     |                                   |                              |
     | SparkSession.builder              |                              |
     |   .remote("sc://localhost:50051") |                              |
     |   .getOrCreate()                  |                              |
     |-----------------------------------|                              |
     | 1. è§£æé€£æ¥å­—ä¸²                    |                              |
     | 2. å‰µå»º gRPC Channel (HTTP/2)     |                              |
     | 3. ç”Ÿæˆ session_id & user_id      |                              |
     | 4. å‰µå»º SparkSession å°è±¡          |                              |
     |   (æ­¤æ™‚æœªç™¼é€ä»»ä½•è«‹æ±‚)              |                              |
     |                                   |                              |
     | spark.sql("SELECT ...")           |                              |
     | (ç¬¬ä¸€æ¬¡çœŸæ­£çš„æ“ä½œ)                  |                              |
     |                                   |                              |
     | 5. ConfigRequest (gRPC)           |                              |
     |---------------------------------->|----------------------------->|
     |    session_id, user_id            |  TCP/IP (port 50051)         | 6. entrypoint.rs::serve()
     |                                   |  HTTP/2 Stream                |    â†“
     |                                   |                              | 7. server.rs::config()
     |                                   |                              |    SparkConnectService trait
     |                                   |                              |    â†“
     |                                   |                              | 8. session_manager.rs
     |                                   |                              |    ::get_or_create_session_context()
     |                                   |                              |    â†“
     |                                   |                              | 9. SessionManagerActor
     |                                   |                              |    (Actor æ¨¡å‹)
     |                                   |                              |    â†“
     |                                   |                              | 10. create_session_context()
     |                                   |                              |     â”œâ”€ å‰µå»º JobRunner
     |                                   |                              |     â”œâ”€ é…ç½® SessionConfig
     |                                   |                              |     â”œâ”€ è¨­ç½® Catalog
     |                                   |                              |     â”œâ”€ é…ç½® RuntimeEnv
     |                                   |                              |     â”œâ”€ è¨­ç½®ç·©å­˜ (3ç¨®)
     |                                   |                              |     â””â”€ è¨»å†Šå„ªåŒ–è¦å‰‡
     |                                   |                              |    â†“
     |                                   |                              | 11. å­˜å„² SessionContext
     |                                   |                              |     self.sessions.insert(key, ctx)
     |                                   |                              |    â†“
     | 12. ConfigResponse                |                              | 13. è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬
     |<----------------------------------|<-----------------------------|     send_with_delay(timeout)
     |    (é…ç½®ä¿¡æ¯)                      |                              |
     |                                   |                              |
     | ç¾åœ¨å¯ä»¥åŸ·è¡Œ SQL äº†                 |                              | SessionContext æº–å‚™å°±ç·’
```

---

## è©³ç´°æºç¢¼è§£æ

### æ­¥é©Ÿ 1-4ï¼šPySpark å®¢æˆ¶ç«¯åˆå§‹åŒ–ï¼ˆæœ¬åœ°æ“ä½œï¼‰

ğŸ”¸ **å®¢æˆ¶ç«¯åšäº†ä»€éº¼**

é›–ç„¶é€™ä¸æ˜¯ Sail çš„ä»£ç¢¼ï¼Œä½†äº†è§£å®¢æˆ¶ç«¯é‚è¼¯æœ‰åŠ©æ–¼ç†è§£æ•´å€‹æµç¨‹ï¼š

```python
# PySpark å…§éƒ¨å¯¦ä½œï¼ˆç°¡åŒ–ç‰ˆï¼‰
class Builder:
    def remote(self, url: str):
        # è§£æ "sc://localhost:50051"
        # sc:// â†’ Spark Connect å”è­°
        # localhost â†’ ä¸»æ©Ÿå
        # 50051 â†’ gRPC ç«¯å£
        self._connection_string = url
        return self

    def getOrCreate(self):
        # å‰µå»º gRPC channel (HTTP/2 æŒä¹…é€£æ¥)
        channel = grpc.insecure_channel('localhost:50051')

        # ç”Ÿæˆ session ID (UUID)
        session_id = str(uuid.uuid4())

        # ç²å– user ID
        user_id = os.getenv('USER') or getpass.getuser()

        # å‰µå»º SparkSession å°è±¡ï¼ˆæ­¤æ™‚é‚„æ²’ç™¼é€ä»»ä½•è«‹æ±‚ï¼ï¼‰
        return SparkSession(
            client=SparkConnectClient(
                channel=channel,
                session_id=session_id,
                user_id=user_id
            )
        )
```

ğŸ”¸ **é—œéµé»**

- **å»¶é²åˆå§‹åŒ–ï¼ˆLazy Initializationï¼‰**ï¼š`.getOrCreate()` ä¸æœƒç«‹å³å‘æœå‹™å™¨ç™¼é€è«‹æ±‚
- **Session ID ç”Ÿæˆ**ï¼šå®¢æˆ¶ç«¯ç”Ÿæˆ UUIDï¼ˆä¾‹å¦‚ï¼š`"abc-123-def-456"`ï¼‰
- **User Context**ï¼šå¾ç’°å¢ƒè®Šé‡ç²å–ç•¶å‰ç”¨æˆ¶å

---

### æ­¥é©Ÿ 5ï¼šç¬¬ä¸€å€‹ gRPC è«‹æ±‚ï¼ˆConfigRequestï¼‰

ç•¶ä½ åŸ·è¡Œ `spark.sql("SELECT ...")` æ™‚ï¼ŒPySpark æœƒç™¼é€ç¬¬ä¸€å€‹ gRPC è«‹æ±‚ï¼š

```protobuf
ConfigRequest {
  session_id: "abc-123-def-456",
  user_context: UserContext {
    user_id: "stanhsu",
  },
  operation: GetAll {
    prefix: ""  // ç²å–æ‰€æœ‰é…ç½®
  }
}
```

---

### æ­¥é©Ÿ 6ï¼šSail æœå‹™å™¨æ¥æ”¶ gRPC è«‹æ±‚

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/entrypoint.rs:13-36`**

é€™æ˜¯ Sail æœå‹™å™¨çš„å…¥å£é»ï¼Œè² è²¬å•Ÿå‹• gRPC æœå‹™å™¨ï¼š

```rust
pub async fn serve<F>(
    listener: TcpListener,         // TCP ç›£è½å™¨ï¼ˆç¶å®šåˆ° 0.0.0.0:50051ï¼‰
    signal: F,                      // å„ªé›…é—œé–‰ä¿¡è™Ÿï¼ˆCtrl-Cï¼‰
    options: SessionManagerOptions, // é…ç½®é¸é …
) -> Result<(), Box<dyn std::error::Error>>
where
    F: Future<Output = ()>,
{
    // å‰µå»º SessionManagerï¼ˆç®¡ç†æ‰€æœ‰ sessionï¼‰
    let session_manager = SessionManager::new(options);

    // å‰µå»º SparkConnectServerï¼ˆå¯¦ä½œ gRPC service traitï¼‰
    let server = SparkConnectServer::new(session_manager);

    // åŒ…è£æˆ Tonic gRPC service
    let service = SparkConnectServiceServer::new(server)
        .max_decoding_message_size(GRPC_MAX_MESSAGE_LENGTH_DEFAULT)  // 128MB
        .accept_compressed(CompressionEncoding::Gzip)                 // æ”¯æ´ Gzip å£“ç¸®
        .accept_compressed(CompressionEncoding::Zstd)                 // æ”¯æ´ Zstd å£“ç¸®
        .send_compressed(CompressionEncoding::Gzip)
        .send_compressed(CompressionEncoding::Zstd);

    // ä½¿ç”¨ ServerBuilder å•Ÿå‹•æœå‹™å™¨
    ServerBuilder::new("sail_spark_connect", Default::default())
        .add_service(service, Some(crate::spark::connect::FILE_DESCRIPTOR_SET))
        .await
        .serve(listener, signal)    // é–‹å§‹ç›£è½ TCP é€£æ¥
        .await
}
```

ğŸ”¸ **Tonic æ¡†æ¶çš„é­”æ³•**

- `SparkConnectServiceServer::new(server)` æœƒè‡ªå‹•å°‡ä½ çš„ `SparkConnectServer` åŒ…è£æˆ gRPC service
- Tonic æœƒæ ¹æ“š `.proto` æ–‡ä»¶ç”Ÿæˆçš„ä»£ç¢¼ï¼Œè‡ªå‹•è·¯ç”±è«‹æ±‚åˆ°å°æ‡‰çš„æ–¹æ³•ï¼ˆå¦‚ `config()`, `execute_plan()`ï¼‰

ğŸ”¸ **ServerBuilder åšäº†ä»€éº¼**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-server/src/builder.rs:101-124`

```rust
pub async fn serve<F>(
    self,
    listener: TcpListener,  // å¾ tokio::net::TcpListener
    signal: F,
) -> Result<(), Box<dyn std::error::Error>>
{
    // æ·»åŠ  gRPC reflectionï¼ˆè®“ grpcurl ç­‰å·¥å…·å¯ä»¥æ¢ç´¢ APIï¼‰
    let reflection_server = self.reflection_server_builder.build_v1()?;
    let router = self.router.add_service(reflection_server);

    // é…ç½® TCP é¸é …
    let incoming = TcpIncoming::from(listener)
        .with_nodelay(Some(true))              // ç¦ç”¨ Nagle ç®—æ³•ï¼ˆä½å»¶é²ï¼‰
        .with_keepalive(Some(Duration::from_secs(60)));  // TCP keepalive

    // å•Ÿå‹• HTTP/2 æœå‹™å™¨ï¼Œä¸¦æ”¯æ´å„ªé›…é—œé–‰
    router.serve_with_incoming_shutdown(incoming, signal).await?;

    Ok(())
}
```

---

### æ­¥é©Ÿ 7ï¼šè·¯ç”±åˆ° config() æ–¹æ³•

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/server.rs:247-289`**

Tonic è‡ªå‹•å°‡ `ConfigRequest` è·¯ç”±åˆ°é€™å€‹æ–¹æ³•ï¼š

```rust
#[tonic::async_trait]
impl SparkConnectService for SparkConnectServer {
    async fn config(
        &self,
        request: Request<ConfigRequest>,
    ) -> Result<Response<ConfigResponse>, Status> {
        let request = request.into_inner();  // å–å‡º ConfigRequest

        // æ§‹å»º SessionKeyï¼ˆuser_id + session_idï¼‰
        let session_key = SessionKey {
            user_id: request.user_context.map(|u| u.user_id).unwrap_or_default(),
            session_id: request.session_id.clone(),
        };

        // ã€é—œéµã€‘ç²å–æˆ–å‰µå»º SessionContext
        // ç¬¬ä¸€æ¬¡è«‹æ±‚æ™‚ï¼Œæœƒè§¸ç™¼ SessionContext å‰µå»º
        let ctx = self.session_manager.get_or_create_session_context(session_key).await?;

        // è™•ç†é…ç½®è«‹æ±‚ï¼ˆçœç•¥å…·é«”é‚è¼¯ï¼‰
        let response = match request.operation.op_type {
            OpType::GetAll(GetAll { prefix }) => {
                service::handle_config_get_all(&ctx, prefix)?
            }
            OpType::Set(Set { pairs, .. }) => {
                service::handle_config_set(&ctx, pairs)?
            }
            // ... å…¶ä»–é…ç½®æ“ä½œ
        };

        Ok(Response::new(response))
    }

    // å…¶ä»–æ–¹æ³•ï¼šexecute_plan(), analyze_plan(), ...
}
```

---

### æ­¥é©Ÿ 8ï¼šSessionManager::get_or_create_session_context()

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:70-83`**

é€™å€‹æ–¹æ³•æ˜¯ `SessionManager` çš„å¤–è§€ï¼ˆFacadeï¼‰ï¼Œå¯¦éš›å·¥ä½œç”± `SessionManagerActor` å®Œæˆã€‚

```rust
pub async fn get_or_create_session_context(
    &self,
    key: SessionKey,
) -> SparkResult<SessionContext> {
    // å‰µå»º oneshot channelï¼ˆç”¨æ–¼æ¥æ”¶çµæœï¼‰
    let (tx, rx) = oneshot::channel();

    // æ§‹å»º Actor äº‹ä»¶
    let event = SessionManagerEvent::GetOrCreateSession {
        key,
        system: self.system.clone(),  // Arc<Mutex<ActorSystem>>
        result: tx,                   // å°‡ç™¼é€ç«¯äº¤çµ¦ Actor
    };

    // å°‡äº‹ä»¶ç™¼é€çµ¦ SessionManagerActorï¼ˆéé˜»å¡ï¼‰
    self.handle.send(event).await?;

    // ç­‰å¾… Actor è™•ç†å®Œæˆä¸¦è¿”å›çµæœï¼ˆé˜»å¡ï¼‰
    rx.await.map_err(|e| SparkError::internal(format!("failed to get session: {e}")))?
}
```

ğŸ”¸ **ç‚ºä»€éº¼ä½¿ç”¨ Actor æ¨¡å‹ï¼Ÿ**

1. **ä¸¦ç™¼å®‰å…¨**ï¼šå¤šå€‹å®¢æˆ¶ç«¯åŒæ™‚è«‹æ±‚æ™‚ï¼ŒActor ç¢ºä¿ session å‰µå»ºæ˜¯åºåˆ—åŒ–çš„ï¼ˆä¸€æ¬¡ä¸€å€‹ï¼‰
2. **ç‹€æ…‹å°è£**ï¼š`self.sessions: HashMap<SessionKey, SessionContext>` æ˜¯ Actor çš„ç§æœ‰ç‹€æ…‹ï¼Œå¤–éƒ¨ç„¡æ³•ç›´æ¥è¨ªå•
3. **ç•°æ­¥æ¶ˆæ¯å‚³é**ï¼šè«‹æ±‚æ–¹ä¸æœƒé˜»å¡ Actorï¼ŒActor å¯ä»¥æŒ‰è‡ªå·±çš„ç¯€å¥è™•ç†æ¶ˆæ¯

---

### æ­¥é©Ÿ 9ï¼šSessionManagerActor è™•ç†äº‹ä»¶

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:348-386`**

`SessionManagerActor` æ”¶åˆ° `GetOrCreateSession` äº‹ä»¶å¾Œï¼š

```rust
fn handle_get_or_create_session(
    &mut self,
    ctx: &mut ActorContext<Self>,
    key: SessionKey,
    system: Arc<Mutex<ActorSystem>>,
    result: oneshot::Sender<SparkResult<SessionContext>>,
) -> ActorAction {
    // æª¢æŸ¥ session æ˜¯å¦å·²å­˜åœ¨
    let context = if let Some(context) = self.sessions.get(&key) {
        Ok(context.clone())  // ç›´æ¥è¿”å›å·²å­˜åœ¨çš„ session
    } else {
        // Session ä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°çš„
        info!("creating session {key}");
        match self.create_session_context(system, key.clone()) {
            Ok(context) => {
                // å°‡æ–°å‰µå»ºçš„ session å­˜å„²åˆ° HashMap
                self.sessions.insert(key.clone(), context.clone());
                Ok(context)
            }
            Err(e) => Err(e),
        }
    };

    // è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬ï¼ˆæ­¥é©Ÿ 13ï¼‰
    if let Ok(context) = &context {
        if let Ok(active_at) = context
            .extension::<SparkSession>()
            .map_err(|e| e.into())
            .and_then(|spark| spark.track_activity())
        {
            // åœ¨ N ç§’å¾Œç™¼é€ ProbeIdleSession äº‹ä»¶çµ¦è‡ªå·±
            ctx.send_with_delay(
                SessionManagerEvent::ProbeIdleSession {
                    key,
                    instant: active_at,
                },
                Duration::from_secs(self.options.config.spark.session_timeout_secs),
            );
        }
    }

    // å°‡çµæœç™¼é€å›ç­‰å¾…çš„èª¿ç”¨è€…
    let _ = result.send(context);
    ActorAction::Continue  // ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹æ¶ˆæ¯
}
```

---

### æ­¥é©Ÿ 10ï¼šcreate_session_context() - 7 å€‹å­æ­¥é©Ÿ

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:87-288`**

é€™æ˜¯æ•´å€‹é€£æ¥å»ºç«‹éç¨‹ä¸­æœ€è¤‡é›œçš„éƒ¨åˆ†ï¼ŒåŒ…å« 7 å€‹å­æ­¥é©Ÿã€‚

#### æ­¥é©Ÿ 10.1ï¼šå‰µå»º JobRunner

```rust
// session_manager.rs:93-100
let job_runner: Box<dyn JobRunner> = match options.config.mode {
    ExecutionMode::Local => {
        // Local Modeï¼šå–®é€²ç¨‹ï¼Œä½¿ç”¨ DataFusion å¤šç·šç¨‹åŸ·è¡Œ
        Box::new(LocalJobRunner::new())
    }
    ExecutionMode::LocalCluster | ExecutionMode::KubernetesCluster => {
        // Cluster Modeï¼šå¤šé€²ç¨‹ï¼Œä½¿ç”¨ DriverActor å”èª¿ Worker
        let options = DriverOptions::try_new(&options.config, options.runtime.clone())?;
        let mut system = system.lock()?;
        Box::new(ClusterJobRunner::new(system.deref_mut(), options))
    }
};

```

ğŸ”¸ **JobRunner æ˜¯ä»€éº¼ï¼Ÿ**

`JobRunner` æ˜¯ä¸€å€‹ traitï¼Œå®šç¾©äº†å¦‚ä½•åŸ·è¡ŒæŸ¥è©¢è¨ˆç•«ï¼š

```rust
// crates/sail-execution/src/job/runner.rs:11-22
#[tonic::async_trait]
pub trait JobRunner: Send + Sync + 'static {
    async fn execute(
        &self,
        ctx: &SessionContext,
        plan: Arc<dyn ExecutionPlan>,  // DataFusion ç‰©ç†è¨ˆç•«
    ) -> ExecutionResult<SendableRecordBatchStream>;  // è¿”å›çµæœæµ

    async fn stop(&self);
}
```

- **LocalJobRunner**ï¼šç›´æ¥èª¿ç”¨ DataFusion çš„ `execute_stream()`ï¼Œå–®é€²ç¨‹å¤šç·šç¨‹åŸ·è¡Œ
- **ClusterJobRunner**ï¼šé€šé `DriverActor` å°‡è¨ˆç•«åˆ†ç™¼çµ¦å¤šå€‹ Worker åŸ·è¡Œ

#### æ­¥é©Ÿ 10.2ï¼šé…ç½® SessionConfig

```rust
// session_manager.rs:103-120
let mut session_config = SessionConfig::new()
    .with_create_default_catalog_and_schema(false)  // ä¸ä½¿ç”¨ DataFusion é»˜èª catalog
    .with_information_schema(false)                 // ä¸ä½¿ç”¨ information_schema
    .with_extension(Arc::new(create_catalog_manager(
        &options.config,
        options.runtime.clone()
    )?))  // Sail è‡ªå®šç¾© catalogï¼ˆæ”¯æ´ Unityã€Iceberg REST ç­‰ï¼‰
    .with_extension(Arc::new(SparkSession::try_new(
        key.user_id,
        key.session_id,
        job_runner,
        SparkSessionOptions {
            execution_heartbeat_interval: Duration::from_secs(
                options.config.spark.execution_heartbeat_interval_secs,  // é»˜èª 10 ç§’
            ),
        },
    )?));
```

ğŸ”¸ **SessionConfig çš„ Extension æ©Ÿåˆ¶**

DataFusion å…è¨±åœ¨ `SessionConfig` ä¸­å­˜å„²è‡ªå®šç¾©ç‹€æ…‹ï¼š
- `CatalogManager`ï¼šç®¡ç†æ•¸æ“šåº«ã€è¡¨ã€è¦–åœ–çš„å…ƒæ•¸æ“š
- `SparkSession`ï¼šå­˜å„² Spark ç‰¹æœ‰çš„ç‹€æ…‹ï¼ˆå¦‚è‡¨æ™‚è¦–åœ–ã€JobRunnerï¼‰

#### æ­¥é©Ÿ 10.3ï¼šé…ç½®åŸ·è¡Œé¸é …

```rust
// session_manager.rs:123-133
{
    let execution = &mut session_config.options_mut().execution;
    execution.batch_size = options.config.execution.batch_size;  // é»˜èª 8192
    execution.collect_statistics = options.config.execution.collect_statistics;
    execution.use_row_number_estimates_to_optimize_partitioning =
        options.config.execution.use_row_number_estimates_to_optimize_partitioning;
    execution.listing_table_ignore_subdirectory = false;  // Spark ä¸å¿½ç•¥å­ç›®éŒ„
}
```

#### æ­¥é©Ÿ 10.4ï¼šé…ç½® Parquet é¸é …

```rust
// session_manager.rs:136-175
{
    let parquet = &mut session_config.options_mut().execution.parquet;
    parquet.created_by = concat!("sail version ", env!("CARGO_PKG_VERSION")).into();
    parquet.enable_page_index = options.config.parquet.enable_page_index;
    parquet.pruning = options.config.parquet.pruning;
    parquet.pushdown_filters = options.config.parquet.pushdown_filters;
    parquet.compression = Some(options.config.parquet.compression.clone());  // é»˜èª Snappy
    parquet.max_row_group_size = options.config.parquet.max_row_group_size;  // é»˜èª 1M è¡Œ
    // ... æ›´å¤š Parquet é…ç½®
}
```

#### æ­¥é©Ÿ 10.5ï¼šé…ç½® RuntimeEnvï¼ˆå°è±¡å­˜å„²èˆ‡ä¸‰å±¤ç·©å­˜ï¼‰

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:177-259`**

é€™æ˜¯æœ€è¤‡é›œçš„éƒ¨åˆ†ï¼Œé…ç½®ä¸‰ç¨®ç·©å­˜å’Œå°è±¡å­˜å„²ï¼š

```rust
let runtime = {
    // è¨»å†Šå°è±¡å­˜å„²ï¼ˆS3/GCS/HDFS/æœ¬åœ°æ–‡ä»¶ç³»çµ±ï¼‰
    let registry = DynamicObjectStoreRegistry::new(options.runtime.clone());

    // ===== ç·©å­˜ 1ï¼šæ–‡ä»¶çµ±è¨ˆç·©å­˜ï¼ˆFileStatisticsCacheï¼‰ =====
    // ç”¨é€”ï¼šç·©å­˜ Parquet æ–‡ä»¶çš„çµ±è¨ˆä¿¡æ¯ï¼ˆmin/max/null_countï¼‰
    // å¥½è™•ï¼šè·³éä¸ç›¸é—œçš„æ–‡ä»¶ï¼ˆè¬‚è©ä¸‹æ¨ï¼‰
    let file_statistics_cache: Option<FileStatisticsCache> =
        match &options.config.parquet.file_statistics_cache.r#type {
            CacheType::None => None,
            CacheType::Global => Some(
                self.global_file_statistics_cache
                    .get_or_insert_with(|| {
                        Arc::new(MokaFileStatisticsCache::new(ttl, max_entries))
                    })
                    .clone()
            ),
            CacheType::Session => Some(
                Arc::new(MokaFileStatisticsCache::new(ttl, max_entries))
            ),
        };

    // ===== ç·©å­˜ 2ï¼šæ–‡ä»¶åˆ—è¡¨ç·©å­˜ï¼ˆFileListingCacheï¼‰ =====
    // ç”¨é€”ï¼šç·©å­˜ S3 LIST æ“ä½œçš„çµæœï¼ˆç›®éŒ„ä¸‹æœ‰å“ªäº›æ–‡ä»¶ï¼‰
    // å¥½è™•ï¼šæ¸›å°‘ S3 LIST èª¿ç”¨ï¼ˆLIST å¾ˆæ…¢ä¸”æœ‰è²»ç”¨ï¼‰
    let file_listing_cache: Option<ListFilesCache> =
        match &options.config.execution.file_listing_cache.r#type {
            CacheType::None => None,
            CacheType::Global => Some(
                self.global_file_listing_cache
                    .get_or_insert_with(|| {
                        Arc::new(MokaFileListingCache::new(ttl, max_entries))
                    })
                    .clone()
            ),
            CacheType::Session => Some(
                Arc::new(MokaFileListingCache::new(ttl, max_entries))
            ),
        };

    // ===== ç·©å­˜ 3ï¼šæ–‡ä»¶å…ƒæ•¸æ“šç·©å­˜ï¼ˆFileMetadataCacheï¼‰ =====
    // ç”¨é€”ï¼šç·©å­˜ Parquet footerï¼ˆåŒ…å« schemaã€row group å…ƒæ•¸æ“šï¼‰
    // å¥½è™•ï¼šæ¸›å°‘ S3 GET èª¿ç”¨ï¼ˆè®€å–å°æ–‡ä»¶æœ«å°¾ï¼‰
    let file_metadata_cache: Arc<dyn FileMetadataCache> =
        match options.config.parquet.file_metadata_cache.r#type {
            CacheType::None => Arc::new(MokaFileMetadataCache::new(ttl, Some(0))),  // å¤§å°ç‚º 0ï¼Œä¸ç·©å­˜
            CacheType::Global =>
                self.global_file_metadata_cache
                    .get_or_insert_with(|| {
                        Arc::new(MokaFileMetadataCache::new(ttl, size_limit))
                    })
                    .clone(),
            CacheType::Session => Arc::new(MokaFileMetadataCache::new(ttl, size_limit)),
        };

    // çµ„è£ç·©å­˜é…ç½®
    let cache_config = CacheManagerConfig::default()
        .with_files_statistics_cache(file_statistics_cache)
        .with_list_files_cache(file_listing_cache)
        .with_file_metadata_cache(Some(file_metadata_cache));

    // æ§‹å»º RuntimeEnv
    let builder = RuntimeEnvBuilder::default()
        .with_object_store_registry(Arc::new(registry))
        .with_cache_manager(cache_config);

    Arc::new(builder.build()?)
};
```

ğŸ”¸ **ä¸‰ç¨®ç·©å­˜é¡å‹çš„å€åˆ¥**

| ç·©å­˜é¡å‹ | å­˜å„²ä½ç½® | ç”Ÿå‘½é€±æœŸ | é©ç”¨å ´æ™¯ |
|---------|---------|---------|---------|
| `CacheType::None` | ç„¡ | ç„¡ | é–‹ç™¼ç’°å¢ƒã€æ•¸æ“šé »ç¹è®ŠåŒ– |
| `CacheType::Session` | `SessionContext` | Session çµæŸæ™‚æ¸…ç† | å–®ç”¨æˆ¶ã€ç¨ç«‹æŸ¥è©¢ |
| `CacheType::Global` | `SessionManagerActor` | æœå‹™å™¨é‡å•Ÿæ™‚æ¸…ç† | å¤šç”¨æˆ¶ã€å…±äº«æ•¸æ“š |

#### æ­¥é©Ÿ 10.6ï¼šæ§‹å»º SessionStateï¼ˆè¨»å†Šå„ªåŒ–è¦å‰‡ï¼‰

```rust
// session_manager.rs:261-271
let state = SessionStateBuilder::new()
    .with_config(session_config)
    .with_runtime_env(runtime)
    .with_default_features()  // DataFusion é»˜èªåŠŸèƒ½ï¼ˆå…§å»ºå‡½æ•¸ã€èšåˆå‡½æ•¸ç­‰ï¼‰
    .with_analyzer_rules(default_analyzer_rules())  // Sail è‡ªå®šç¾©åˆ†æè¦å‰‡
    .with_optimizer_rules(default_optimizer_rules())  // Sail è‡ªå®šç¾©å„ªåŒ–è¦å‰‡
    .with_physical_optimizer_rules(get_physical_optimizers(PhysicalOptimizerOptions {
        enable_join_reorder: options.config.optimizer.enable_join_reorder,
    }))
    .with_query_planner(new_query_planner())  // Sail è‡ªå®šç¾©æŸ¥è©¢è¦åŠƒå™¨
    .build();

let context = SessionContext::new_with_state(state);
```

ğŸ”¸ **å„ªåŒ–è¦å‰‡çš„ä½œç”¨**

- **åˆ†æè¦å‰‡ï¼ˆAnalyzer Rulesï¼‰**ï¼šè™•ç†æœªè§£æçš„å¼•ç”¨ï¼ˆè¡¨åã€åˆ—åï¼‰
- **é‚è¼¯å„ªåŒ–è¦å‰‡ï¼ˆOptimizer Rulesï¼‰**ï¼šå„ªåŒ–é‚è¼¯è¨ˆç•«ï¼ˆè¬‚è©ä¸‹æ¨ã€æŠ•å½±è£å‰ªã€å¸¸é‡æŠ˜ç–Šï¼‰
- **ç‰©ç†å„ªåŒ–è¦å‰‡ï¼ˆPhysical Optimizer Rulesï¼‰**ï¼šå„ªåŒ–ç‰©ç†è¨ˆç•«ï¼ˆJoin é‡æ’åºã€ç®¡é“åŒ–ï¼‰
- **æŸ¥è©¢è¦åŠƒå™¨ï¼ˆQuery Plannerï¼‰**ï¼šå°‡é‚è¼¯è¨ˆç•«è½‰æ›ç‚ºç‰©ç†è¨ˆç•«

#### æ­¥é©Ÿ 10.7ï¼šåè¨»å†Šå…§å»ºå‡½æ•¸

```rust
// session_manager.rs:277-285
for (&name, _function) in BUILT_IN_SCALAR_FUNCTIONS.iter() {
    context.deregister_udf(name);  // ç§»é™¤ DataFusion çš„å…§å»ºå‡½æ•¸
}
for (&name, _function) in BUILT_IN_GENERATOR_FUNCTIONS.iter() {
    context.deregister_udf(name);  // å¦‚ explode()
}
for (&name, _function) in BUILT_IN_TABLE_FUNCTIONS.iter() {
    context.deregister_udtf(name);  // å¦‚ range()
}
```

ğŸ”¸ **ç‚ºä»€éº¼è¦åè¨»å†Šï¼Ÿ**

Sail å¯¦ä½œäº†è‡ªå·±çš„ Spark å‡½æ•¸ï¼ˆèªç¾©èˆ‡ DataFusion ä¸åŒï¼‰ï¼Œéœ€è¦ç§»é™¤ DataFusion çš„é»˜èªå¯¦ä½œï¼Œé¿å…è¡çªã€‚

ä¾‹å¦‚ï¼š
- Spark çš„ `concat()` å‡½æ•¸é‡åˆ° NULL æœƒè¿”å› NULL
- DataFusion çš„ `concat()` å‡½æ•¸æœƒè·³é NULL

---

### æ­¥é©Ÿ 11ï¼šå­˜å„² SessionContext

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:364`**

```rust
self.sessions.insert(key, context.clone());
```

å°‡æ–°å‰µå»ºçš„ `SessionContext` å­˜å„²åˆ° `SessionManagerActor` çš„ `HashMap` ä¸­ã€‚

ğŸ”¸ **SessionContext çš„å…§å®¹ç¸½çµ**

| çµ„ä»¶ | ä½œç”¨ | é…ç½®ä¾†æº |
|------|------|----------|
| `JobRunner` | æ±ºå®šæŸ¥è©¢åŸ·è¡Œæ–¹å¼ï¼ˆLocal/Clusterï¼‰ | `AppConfig.mode` |
| `CatalogManager` | ç®¡ç†æ•¸æ“šåº«ã€è¡¨ã€è¦–åœ–çš„å…ƒæ•¸æ“š | `AppConfig.catalog` |
| `SparkSession` | å­˜å„² session ç´šåˆ¥çš„ç‹€æ…‹ï¼ˆå¦‚è‡¨æ™‚è¦–åœ–ï¼‰ | ç”¨æˆ¶ ID + Session ID |
| `DynamicObjectStoreRegistry` | è¨»å†Šå°è±¡å­˜å„²ï¼ˆS3/GCS/HDFSï¼‰ | `AppConfig.runtime` |
| `FileStatisticsCache` | ç·©å­˜ Parquet æ–‡ä»¶çµ±è¨ˆä¿¡æ¯ | `AppConfig.parquet.file_statistics_cache` |
| `FileListingCache` | ç·©å­˜ç›®éŒ„åˆ—è¡¨ï¼ˆæ¸›å°‘ S3 LIST èª¿ç”¨ï¼‰ | `AppConfig.execution.file_listing_cache` |
| `FileMetadataCache` | ç·©å­˜ Parquet footerï¼ˆæ¸›å°‘ S3 GET èª¿ç”¨ï¼‰ | `AppConfig.parquet.file_metadata_cache` |
| åˆ†æè¦å‰‡ï¼ˆAnalyzer Rulesï¼‰ | è™•ç†æœªè§£æçš„å¼•ç”¨ï¼ˆè¡¨åã€åˆ—åï¼‰ | `sail-logical-optimizer` |
| å„ªåŒ–è¦å‰‡ï¼ˆOptimizer Rulesï¼‰ | å„ªåŒ–é‚è¼¯è¨ˆç•«ï¼ˆè¬‚è©ä¸‹æ¨ã€æŠ•å½±è£å‰ªï¼‰ | `sail-logical-optimizer` |
| ç‰©ç†å„ªåŒ–è¦å‰‡ | å„ªåŒ–ç‰©ç†è¨ˆç•«ï¼ˆJoin é‡æ’åºï¼‰ | `sail-physical-optimizer` |
| æŸ¥è©¢è¦åŠƒå™¨ï¼ˆQuery Plannerï¼‰ | å°‡é‚è¼¯è¨ˆç•«è½‰æ›ç‚ºç‰©ç†è¨ˆç•« | `sail-plan/planner` |

å¾ç¾åœ¨é–‹å§‹ï¼Œé€™å€‹ `SessionContext` å¯ä»¥ç”¨ä¾†åŸ·è¡Œ SQL æŸ¥è©¢äº†ï¼

---

### æ­¥é©Ÿ 12ï¼šè¿”å› ConfigResponse

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/server.rs:250-260`**

ç•¶ `SessionContext` å‰µå»ºå®Œæˆå¾Œï¼Œ`config()` æ–¹æ³•ç¹¼çºŒè™•ç†é…ç½®è«‹æ±‚ï¼š

```rust
// è™•ç†é…ç½®è«‹æ±‚ï¼ˆçœç•¥å…·é«”é‚è¼¯ï¼‰
let response = match request.operation.op_type {
    OpType::GetAll(GetAll { prefix }) => {
        service::handle_config_get_all(&ctx, prefix)?
    }
    OpType::Set(Set { pairs, .. }) => {
        service::handle_config_set(&ctx, pairs)?
    }
    // ... å…¶ä»–é…ç½®æ“ä½œ
};

Ok(Response::new(response))
```

ğŸ”¸ **ConfigResponse çš„å…§å®¹**

```protobuf
ConfigResponse {
  session_id: "abc-123-def-456",
  pairs: [
    {key: "spark.sql.shuffle.partitions", value: "200"},
    {key: "spark.executor.memory", value: "1g"},
    // ... æ›´å¤šé…ç½®
  ]
}
```

é€™äº›é…ç½®æœƒè¿”å›çµ¦ PySpark å®¢æˆ¶ç«¯ï¼Œä½†å¤§å¤šæ•¸æ™‚å€™å®¢æˆ¶ç«¯ä¸æœƒä½¿ç”¨é€™äº›é…ç½®ï¼ˆSpark Connect æ¨¡å¼ä¸‹ï¼Œé…ç½®ç”±æœå‹™å™¨ç®¡ç†ï¼‰ã€‚

---

### æ­¥é©Ÿ 13ï¼šè¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬

ğŸ”¸ **æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:376-382`**

ç‚ºäº†é¿å…é–’ç½®çš„ session ä½”ç”¨è³‡æºï¼ŒSail æœƒè¨­ç½®è¶…æ™‚æª¢æ¸¬ï¼š

```rust
ctx.send_with_delay(
    SessionManagerEvent::ProbeIdleSession {
        key,
        instant: active_at,  // è¨˜éŒ„ç•¶å‰çš„æ´»èºæ™‚é–“
    },
    Duration::from_secs(self.options.config.spark.session_timeout_secs),  // é»˜èª 3600 ç§’ï¼ˆ1 å°æ™‚ï¼‰
);
```

ğŸ”¸ **send_with_delay() æ˜¯ä»€éº¼ï¼Ÿ**

é€™æ˜¯ Actor ç³»çµ±çš„ç‰¹æ€§ï¼Œå¯ä»¥å‘è‡ªå·±ç™¼é€å»¶é²æ¶ˆæ¯ï¼š

```rust
// 1 å°æ™‚å¾Œï¼ŒSessionManagerActor æœƒæ”¶åˆ° ProbeIdleSession äº‹ä»¶
// æª¢æŸ¥é€™å€‹ session æ˜¯å¦é‚„æ´»èºï¼š
//   - å¦‚æœæ´»èºï¼ˆactive_at æ™‚é–“æ›´æ–°äº†ï¼‰ï¼Œä¸åšä»»ä½•äº‹
//   - å¦‚æœé–’ç½®ï¼ˆactive_at æ™‚é–“æ²’è®Šï¼‰ï¼Œæ¸…ç† session
```

ğŸ”¸ **handle_probe_idle_session() å¯¦ä½œ**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/session_manager.rs:389-406`

```rust
fn handle_probe_idle_session(
    &mut self,
    ctx: &mut ActorContext<Self>,
    key: SessionKey,
    instant: Instant,  // ä¸Šæ¬¡æª¢æŸ¥æ™‚çš„æ´»èºæ™‚é–“
) -> ActorAction {
    let context = self.sessions.get(&key);
    if let Some(context) = context {
        if let Ok(spark) = context.extension::<SparkSession>() {
            // æª¢æŸ¥ session æ˜¯å¦é‚„æ´»èº
            if spark.active_at().is_ok_and(|x| x <= instant) {
                // active_at æ²’æœ‰æ›´æ–°ï¼Œèªªæ˜ session é–’ç½®äº†
                info!("removing idle session {key}");

                // åœæ­¢ JobRunnerï¼ˆé—œé–‰ Driver/Workerï¼‰
                ctx.spawn(async move {
                    spark.job_runner().stop().await
                });

                // å¾ HashMap ä¸­ç§»é™¤ session
                self.sessions.remove(&key);
            }
            // å¦‚æœ active_at æ›´æ–°äº†ï¼Œèªªæ˜ session é‚„åœ¨ä½¿ç”¨ï¼Œä¸åšä»»ä½•äº‹
        }
    }
    ActorAction::Continue
}
```

ğŸ”¸ **æ´»èºæ™‚é–“å¦‚ä½•æ›´æ–°ï¼Ÿ**

æ¯æ¬¡å®¢æˆ¶ç«¯ç™¼é€è«‹æ±‚æ™‚ï¼Œ`SessionManager::get_or_create_session_context()` æœƒèª¿ç”¨ `spark.track_activity()`ï¼š

```rust
// crates/sail-spark-connect/src/session.rs
impl SparkSession {
    pub fn track_activity(&self) -> SparkResult<Instant> {
        let now = Instant::now();
        *self.active_at.lock()? = now;  // æ›´æ–°æ´»èºæ™‚é–“
        Ok(now)
    }
}
```

---

## ç¬¬é›¶éšæ®µç¸½çµ

ç¾åœ¨ï¼Œæˆ‘å€‘å®Œæ•´åœ°èµ°éäº†å¾ `SparkSession.builder.remote().getOrCreate()` åˆ° SessionContext å‰µå»ºçš„æ•´å€‹æµç¨‹ï¼š

```
1. PySpark å®¢æˆ¶ç«¯åˆå§‹åŒ–ï¼ˆæœ¬åœ°æ“ä½œï¼‰
   - è§£æé€£æ¥å­—ä¸²
   - å‰µå»º gRPC channel (HTTP/2)
   - ç”Ÿæˆ session_id å’Œ user_id
   - å‰µå»º SparkSession å°è±¡ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰

2. ç¬¬ä¸€å€‹ gRPC è«‹æ±‚ï¼ˆConfigRequestï¼‰
   - è§¸ç™¼å¯¦éš›çš„ç¶²çµ¡é€šè¨Š

3. Sail æœå‹™å™¨æ¥æ”¶è«‹æ±‚
   - entrypoint.rs::serve() å•Ÿå‹• gRPC æœå‹™å™¨
   - Tonic æ¡†æ¶æ¥æ”¶ TCP é€£æ¥
   - HTTP/2 å”è­°è™•ç†

4. è·¯ç”±åˆ° config() æ–¹æ³•
   - SparkConnectServer å¯¦ä½œ SparkConnectService trait

5. SessionManager::get_or_create_session_context()
   - ä½¿ç”¨ oneshot channel èˆ‡ Actor é€šè¨Š

6. SessionManagerActor è™•ç†äº‹ä»¶
   - æª¢æŸ¥ session æ˜¯å¦å­˜åœ¨
   - å¦‚æœä¸å­˜åœ¨ï¼Œèª¿ç”¨ create_session_context()

7. create_session_context() - 7 å€‹å­æ­¥é©Ÿ
   7.1. å‰µå»º JobRunnerï¼ˆæ±ºå®šåŸ·è¡Œæ¨¡å¼ï¼‰
   7.2. é…ç½® SessionConfigï¼ˆExtension æ©Ÿåˆ¶ï¼‰
   7.3. é…ç½®åŸ·è¡Œé¸é …ï¼ˆbatch_size ç­‰ï¼‰
   7.4. é…ç½® Parquet é¸é …ï¼ˆå£“ç¸®ã€çµ±è¨ˆä¿¡æ¯ç­‰ï¼‰
   7.5. é…ç½® RuntimeEnvï¼ˆå°è±¡å­˜å„² + ä¸‰å±¤ç·©å­˜ï¼‰
   7.6. æ§‹å»º SessionStateï¼ˆå„ªåŒ–è¦å‰‡ã€æŸ¥è©¢è¦åŠƒå™¨ï¼‰
   7.7. åè¨»å†Šå…§å»ºå‡½æ•¸ï¼ˆä½¿ç”¨ Sail è‡ªå®šç¾©å¯¦ä½œï¼‰

8. å­˜å„² SessionContext
   - å°‡å‰µå»ºçš„ SessionContext å­˜å„²åˆ° HashMap

9. è¿”å› ConfigResponse
   - è¿”å›æœå‹™å™¨é…ç½®çµ¦å®¢æˆ¶ç«¯

10. è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬
    - 1 å°æ™‚å¾Œè‡ªå‹•æª¢æŸ¥ä¸¦æ¸…ç†é–’ç½® session
```

ç¾åœ¨ï¼ŒSessionContext æº–å‚™å°±ç·’ï¼Œå¯ä»¥æ¥æ”¶çœŸæ­£çš„ SQL æŸ¥è©¢äº†ï¼è®“æˆ‘å€‘é€²å…¥ç¬¬ä¸€éšæ®µï¼šSQL æŸ¥è©¢åŸ·è¡Œã€‚

---
| `CatalogManager` | ç®¡ç†æ•¸æ“šåº«ã€è¡¨ã€è¦–åœ–çš„å…ƒæ•¸æ“š | `AppConfig.catalog` |
| `SparkSession` | å­˜å„² session ç´šåˆ¥çš„ç‹€æ…‹ï¼ˆå¦‚è‡¨æ™‚è¦–åœ–ï¼‰ | ç”¨æˆ¶ ID + Session ID |
| `DynamicObjectStoreRegistry` | è¨»å†Šå°è±¡å­˜å„²ï¼ˆS3/GCS/HDFSï¼‰ | `AppConfig.runtime` |
| `FileStatisticsCache` | ç·©å­˜ Parquet æ–‡ä»¶çµ±è¨ˆä¿¡æ¯ | `AppConfig.parquet.file_statistics_cache` |
| `FileListingCache` | ç·©å­˜ç›®éŒ„åˆ—è¡¨ï¼ˆæ¸›å°‘ S3 LIST èª¿ç”¨ï¼‰ | `AppConfig.execution.file_listing_cache` |
| `FileMetadataCache` | ç·©å­˜ Parquet footerï¼ˆæ¸›å°‘ S3 GET èª¿ç”¨ï¼‰ | `AppConfig.parquet.file_metadata_cache` |
| åˆ†æè¦å‰‡ï¼ˆAnalyzer Rulesï¼‰ | è™•ç†æœªè§£æçš„å¼•ç”¨ï¼ˆè¡¨åã€åˆ—åï¼‰ | `sail-logical-optimizer` |
| å„ªåŒ–è¦å‰‡ï¼ˆOptimizer Rulesï¼‰ | å„ªåŒ–é‚è¼¯è¨ˆç•«ï¼ˆè¬‚è©ä¸‹æ¨ã€æŠ•å½±è£å‰ªï¼‰ | `sail-logical-optimizer` |
| ç‰©ç†å„ªåŒ–è¦å‰‡ | å„ªåŒ–ç‰©ç†è¨ˆç•«ï¼ˆJoin é‡æ’åºï¼‰ | `sail-physical-optimizer` |
| æŸ¥è©¢è¦åŠƒå™¨ï¼ˆQuery Plannerï¼‰ | å°‡é‚è¼¯è¨ˆç•«è½‰æ›ç‚ºç‰©ç†è¨ˆç•« | `sail-plan/planner` |

## ä¸‰ç¨®ç·©å­˜é¡å‹çš„å€åˆ¥

Sail æ”¯æ´ä¸‰ç¨®ç·©å­˜é¡å‹ï¼Œå½±éŸ¿å¤šå€‹ session ä¹‹é–“çš„æ•¸æ“šå…±äº«ï¼š

**CacheType::None**
- ä¸ä½¿ç”¨ç·©å­˜
- æ¯æ¬¡éƒ½é‡æ–°è®€å–å…ƒæ•¸æ“š
- é©åˆé–‹ç™¼ç’°å¢ƒæˆ–æ•¸æ“šè®ŠåŒ–é »ç¹çš„å ´æ™¯

**CacheType::Session**
- æ¯å€‹ session ç¨ç«‹ç·©å­˜
- session çµæŸæ™‚ç·©å­˜è¢«æ¸…ç†
- é©åˆå–®ç”¨æˆ¶å ´æ™¯

**CacheType::Global**
- æ‰€æœ‰ session å…±äº«åŒä¸€å€‹ç·©å­˜
- åœ¨ `SessionManagerActor` ä¸­å­˜å„²ï¼ˆ`self.global_file_statistics_cache`ï¼‰
- é©åˆå¤šç”¨æˆ¶ç”Ÿç”¢ç’°å¢ƒï¼ˆç¯€çœå…§å­˜å’Œ S3 èª¿ç”¨ï¼‰

## æ™‚åºåœ–ï¼šå¾ PySpark é€£æ¥åˆ° SessionContext å‰µå»º

```
PySpark Client                  Sail gRPC Server                SessionManagerActor
      |                                |                                |
      | SparkSession.builder           |                                |
      |   .remote("sc://...")          |                                |
      |   .getOrCreate()               |                                |
      |--------------------------------|                                |
      |   å‰µå»º gRPC channel             |                                |
      |   ç”Ÿæˆ session_id & user_id     |                                |
      |                                |                                |
      | spark.sql("SELECT ...")        |                                |
      |--------------------------------|                                |
      | ConfigRequest (å¯èƒ½å…ˆç™¼é€)       |                                |
      |------------------------------->|                                |
      |                                | SparkConnectService::config()  |
      |                                |------------------------------->|
      |                                |  GetOrCreateSession event      |
      |                                |                                |
      |                                |  æª¢æŸ¥ self.sessions            |
      |                                |  session ä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°çš„       |
      |                                |                                |
      |                                |  create_session_context()      |
      |                                |  â”œâ”€ å‰µå»º JobRunner             |
      |                                |  â”œâ”€ é…ç½® SessionConfig         |
      |                                |  â”œâ”€ è¨­ç½®åŸ·è¡Œé¸é …               |
      |                                |  â”œâ”€ é…ç½® Parquet é¸é …          |
      |                                |  â”œâ”€ å‰µå»º RuntimeEnv            |
      |                                |  â”‚  â”œâ”€ è¨»å†Šå°è±¡å­˜å„²            |
      |                                |  â”‚  â””â”€ é…ç½®ç·©å­˜ç®¡ç†å™¨          |
      |                                |  â”œâ”€ æ§‹å»º SessionState          |
      |                                |  â””â”€ åè¨»å†Šå…§å»ºå‡½æ•¸             |
      |                                |                                |
      |                                |  self.sessions.insert(key, ctx)|
      |                                |                                |
      |                                |  è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬               |
      |                                |  ctx.send_with_delay(          |
      |                                |    ProbeIdleSession,           |
      |                                |    timeout_secs                |
      |                                |  )                             |
      |                                |                                |
      |                                |<-------------------------------|
      |                                |  è¿”å› SessionContext           |
      |<-------------------------------|                                |
      | ConfigResponse                 |                                |
      |                                |                                |
      | ExecutePlanRequest (SQL æŸ¥è©¢)  |                                |
      |------------------------------->|                                |
      |                                | ä½¿ç”¨å·²å‰µå»ºçš„ SessionContext    |
      |                                | åŸ·è¡ŒæŸ¥è©¢...                    |
```

## PySpark ç«¯çš„å¯¦éš›ä»£ç¢¼ï¼ˆåƒè€ƒï¼‰

é›–ç„¶é€™ä¸æ˜¯ Sail çš„ä»£ç¢¼ï¼Œä½†äº†è§£ PySpark å®¢æˆ¶ç«¯çš„é‹ä½œæœ‰åŠ©æ–¼ç†è§£æ•´å€‹æµç¨‹ï¼š

```python
# pyspark/sql/connect/session.pyï¼ˆç°¡åŒ–ç‰ˆï¼‰
class SparkSession:
    @classmethod
    def builder(cls):
        return Builder()

class Builder:
    def remote(self, url: str):
        # è§£æ "sc://localhost:50051"
        self._connection_string = url
        return self

    def getOrCreate(self):
        # å‰µå»º gRPC channel
        channel = grpc.insecure_channel('localhost:50051')

        # ç”Ÿæˆ session IDï¼ˆUUIDï¼‰
        session_id = str(uuid.uuid4())

        # ç²å– user ID
        user_id = os.getenv('USER') or getpass.getuser()

        # å‰µå»º SparkConnectClient
        client = SparkConnectClient(
            channel=channel,
            session_id=session_id,
            user_id=user_id
        )

        # å‰µå»º SparkSessionï¼ˆæ­¤æ™‚é‚„æ²’ç™¼é€ä»»ä½•è«‹æ±‚ï¼‰
        return SparkSession(client)

    def sql(self, query: str):
        # ç¬¬ä¸€æ¬¡åŸ·è¡Œæ“ä½œæ™‚ï¼Œæ‰æœƒç™¼é€ gRPC è«‹æ±‚
        plan = Plan(
            root=Relation(
                sql=SQL(query=query)
            )
        )

        # ç™¼é€ ExecutePlanRequest
        request = ExecutePlanRequest(
            session_id=self._client.session_id,
            user_context=UserContext(user_id=self._client.user_id),
            plan=plan
        )

        # é€šé gRPC ç™¼é€è«‹æ±‚
        response_stream = self._client.stub.ExecutePlan(request)

        # è¿”å› DataFrame
        return DataFrame(response_stream, self)
```

## ç¸½çµï¼šé€£æ¥å»ºç«‹çš„ä¸‰å€‹éšæ®µ

**éšæ®µ 0.1ï¼šå®¢æˆ¶ç«¯åˆå§‹åŒ–ï¼ˆæœ¬åœ°æ“ä½œï¼‰**
- è§£æé€£æ¥å­—ä¸²
- å‰µå»º gRPC channel
- ç”Ÿæˆ session_id å’Œ user_id
- å‰µå»º SparkSession å°è±¡ï¼ˆä½†ä¸ç™¼é€è«‹æ±‚ï¼‰

**éšæ®µ 0.2ï¼šç¬¬ä¸€å€‹ gRPC è«‹æ±‚ï¼ˆConfigRequest æˆ– ExecutePlanRequestï¼‰**
- PySpark ç™¼é€ç¬¬ä¸€å€‹è«‹æ±‚åˆ° Sail æœå‹™å™¨
- Sail çš„ `SessionManager` æ”¶åˆ°è«‹æ±‚ï¼Œç™¼ç¾ session ä¸å­˜åœ¨

**éšæ®µ 0.3ï¼šSessionContext å‰µå»ºï¼ˆæœå‹™å™¨ç«¯ï¼‰**
- `SessionManagerActor` å‰µå»ºæ–°çš„ SessionContext
- é…ç½®åŸ·è¡Œå¼•æ“ï¼ˆJobRunnerï¼‰
- è¨­ç½® catalogã€ç·©å­˜ã€å„ªåŒ–è¦å‰‡
- å°‡ SessionContext å­˜å„²åˆ° `self.sessions` HashMap
- è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬ï¼ˆé»˜èª 1 å°æ™‚å¾Œè‡ªå‹•æ¸…ç†ï¼‰

ç¾åœ¨ï¼ŒSessionContext å·²ç¶“æº–å‚™å¥½è™•ç† SQL æŸ¥è©¢äº†ï¼æ¥ä¸‹ä¾†è®“æˆ‘å€‘çœ‹çœ‹çœŸæ­£çš„ SQL åŸ·è¡Œæµç¨‹...

---

## å®Œæ•´èª¿ç”¨éˆæ¦‚è¦½

```
PySpark Client
    |
    | (gRPC: Spark Connect Protocol)
    v
Sail Connect Server (crates/sail-spark-connect)
    |
    â”œâ”€> 1. æ¥æ”¶ gRPC è«‹æ±‚ (server.rs)
    â”œâ”€> 2. ç²å–/å‰µå»º Session (session_manager.rs)
    â”œâ”€> 3. è·¯ç”±åˆ°è™•ç†å‡½æ•¸ (plan_executor.rs)
    â”œâ”€> 4. è§£æèˆ‡è¨ˆç•«è½‰æ› (sail-plan)
    |      â”œâ”€> SQL å­—ä¸² â†’ Spark Spec
    |      â”œâ”€> Spark Spec â†’ DataFusion LogicalPlan
    |      â””â”€> LogicalPlan â†’ PhysicalPlan
    â”œâ”€> 5. åŸ·è¡Œè¨ˆç•« (JobRunner)
    |      â”œâ”€> Local Mode: DataFusion ç›´æ¥åŸ·è¡Œ
    |      â””â”€> Cluster Mode: Driver â†’ Workers (Actor é€šè¨Š)
    â”œâ”€> 6. çµæœä¸²æµåŒ– (executor.rs)
    |      â””â”€> RecordBatch â†’ Arrow IPC â†’ gRPC Response
    v
PySpark Client æ¥æ”¶çµæœ
```

---

# ç¬¬ä¸€éšæ®µï¼šgRPC è«‹æ±‚é€²å…¥

## å…¥å£é»ï¼šSparkConnectService::execute_plan()

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/server.rs:54-161`

ğŸ”¸ **Rust åŸºç¤çŸ¥è­˜**
- `#[tonic::async_trait]`: é€™æ˜¯ Rust çš„å±¬æ€§å®ï¼ˆattribute macroï¼‰ï¼Œç”¨ä¾†æ¨™è¨˜é€™å€‹ trait å¯¦ä½œæ”¯æ´ç•°æ­¥æ–¹æ³•
- `async fn`: ç•°æ­¥å‡½æ•¸ï¼Œè¿”å› `Future<Output = Result<...>>`
- `Request<T>`: Tonic gRPC æ¡†æ¶çš„è«‹æ±‚åŒ…è£å™¨
- `into_inner()`: æ¶ˆè²»ï¼ˆconsumeï¼‰Request åŒ…è£å™¨ï¼Œå–å‡ºå…§éƒ¨çš„ `ExecutePlanRequest` çµæ§‹

```rust
// server.rs:54
async fn execute_plan(
    &self,
    request: Request<ExecutePlanRequest>,
) -> Result<Response<Self::ExecutePlanStream>, Status>
```

ğŸ”¸ **é€™å€‹å‡½æ•¸åšä»€éº¼**

ç•¶ PySpark å®¢æˆ¶ç«¯ç™¼é€ `spark.sql("SELECT 1 + 1")` æ™‚ï¼Œæœƒé€šé gRPC å‚³é€ä¸€å€‹ `ExecutePlanRequest`ã€‚é€™å€‹è«‹æ±‚åŒ…å«ï¼š

```protobuf
message ExecutePlanRequest {
  string session_id = 1;           // æœƒè©± ID
  UserContext user_context = 2;    // ç”¨æˆ¶è³‡è¨Š
  optional string operation_id = 6; // æ“ä½œ IDï¼ˆç”¨æ–¼é‡é€£ï¼‰
  Plan plan = 3;                   // æŸ¥è©¢è¨ˆç•«ï¼ˆå¯èƒ½æ˜¯ SQL å­—ä¸²æˆ–å·²åºåˆ—åŒ–çš„è¨ˆç•«ï¼‰
}
```

ğŸ”¸ **æºç¢¼è§£æ**

```rust
// server.rs:60-76
let request = request.into_inner();  // å–å‡º ExecutePlanRequest
debug!("{request:?}");               // è¨˜éŒ„è«‹æ±‚å…§å®¹

// æ§‹å»º SessionKeyï¼ˆç”¨æ–¼è­˜åˆ¥ä¸åŒç”¨æˆ¶çš„ä¸åŒæœƒè©±ï¼‰
let session_key = SessionKey {
    user_id: request.user_context.map(|u| u.user_id).unwrap_or_default(),
    session_id: request.session_id,
};

// æ§‹å»ºåŸ·è¡Œå™¨å…ƒæ•¸æ“š
let metadata = ExecutorMetadata {
    operation_id: request.operation_id.unwrap_or_else(|| Uuid::new_v4().to_string()),
    tags: request.tags,
    reattachable: is_reattachable(&request.request_options),
};

// ç²å–æˆ–å‰µå»º SessionContextï¼ˆé€™è£¡æœƒè§¸ç™¼ Actor é€šè¨Šï¼‰
let ctx = self.session_manager.get_or_create_session_context(session_key).await?;
```

ğŸ”¸ **request.plan çš„å…©ç¨®å½¢å¼**

```rust
// server.rs:77-79
let Plan { op_type: op } = request.plan.required("plan")?;
let op = op.required("plan op")?;
match op {
    plan::OpType::Root(relation) => { ... }    // DataFrame API å‘¼å«
    plan::OpType::Command(Command { ... }) => { ... }  // SQL æˆ–å‘½ä»¤
}
```

å°æ–¼æˆ‘å€‘çš„ `SELECT 1 + 1` ç¯„ä¾‹ï¼Œæœƒèµ° `Command` åˆ†æ”¯ï¼Œå…·é«”æ˜¯ `CommandType::SqlCommand`ã€‚

---

# ç¬¬äºŒéšæ®µï¼šSessionManager èˆ‡ Actor ç³»çµ±

## SessionManager::get_or_create_session_context()

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/session_manager.rs:70-83`

ğŸ”¸ **Rust åŸºç¤çŸ¥è­˜**
- `oneshot::channel()`: å‰µå»ºä¸€å€‹å–®æ¬¡ä½¿ç”¨çš„é€šé“ï¼ˆchannelï¼‰ï¼Œç”¨æ–¼ç•°æ­¥ä»»å‹™ä¹‹é–“å‚³éçµæœ
- `tx` (transmitter): ç™¼é€ç«¯
- `rx` (receiver): æ¥æ”¶ç«¯
- `await?`: ç­‰å¾…ç•°æ­¥æ“ä½œå®Œæˆï¼Œå¦‚æœç™¼ç”ŸéŒ¯èª¤å‰‡æ—©æœŸè¿”å›ï¼ˆé¡ä¼¼ `try-catch` çš„èªæ³•ç³–ï¼‰

```rust
// session_manager.rs:70-83
pub async fn get_or_create_session_context(
    &self,
    key: SessionKey,
) -> SparkResult<SessionContext> {
    let (tx, rx) = oneshot::channel();  // å‰µå»ºå–®æ¬¡é€šé“

    // æ§‹å»º Actor äº‹ä»¶
    let event = SessionManagerEvent::GetOrCreateSession {
        key,
        system: self.system.clone(),  // Arc<Mutex<ActorSystem>>
        result: tx,                   // å°‡ç™¼é€ç«¯äº¤çµ¦ Actor
    };

    // å°‡äº‹ä»¶ç™¼é€çµ¦ SessionManagerActor
    self.handle.send(event).await?;

    // ç­‰å¾… Actor è™•ç†å®Œæˆä¸¦è¿”å›çµæœ
    rx.await.map_err(|e| SparkError::internal(format!("failed to get session: {e}")))?
}
```

ğŸ”¸ **Actor è™•ç†é‚è¼¯**

`SessionManagerActor` æ”¶åˆ° `GetOrCreateSession` äº‹ä»¶å¾Œï¼š

```rust
// session_manager.rs:348-386
fn handle_get_or_create_session(
    &mut self,
    ctx: &mut ActorContext<Self>,
    key: SessionKey,
    system: Arc<Mutex<ActorSystem>>,
    result: oneshot::Sender<SparkResult<SessionContext>>,
) -> ActorAction {
    // å¦‚æœ session å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    let context = if let Some(context) = self.sessions.get(&key) {
        Ok(context.clone())
    } else {
        // å‰µå»ºæ–°çš„ SessionContext
        info!("creating session {key}");
        match self.create_session_context(system, key.clone()) {
            Ok(context) => {
                self.sessions.insert(key, context.clone());
                Ok(context)
            }
            Err(e) => Err(e),
        }
    };

    // è¨­ç½®é–’ç½®è¶…æ™‚æª¢æ¸¬
    if let Ok(context) = &context {
        if let Ok(active_at) = context.extension::<SparkSession>().map_err(|e| e.into()).and_then(|spark| spark.track_activity()) {
            ctx.send_with_delay(
                SessionManagerEvent::ProbeIdleSession { key, instant: active_at },
                Duration::from_secs(self.options.config.spark.session_timeout_secs),
            );
        }
    }

    let _ = result.send(context);  // å°‡çµæœç™¼é€å›ç­‰å¾…çš„èª¿ç”¨è€…
    ActorAction::Continue
}
```

ğŸ”¸ **ç‚ºä»€éº¼è¦ç”¨ Actor æ¨¡å‹**

1. **ä¸¦ç™¼å®‰å…¨**ï¼šå¤šå€‹å®¢æˆ¶ç«¯åŒæ™‚è«‹æ±‚æ™‚ï¼ŒActor ç¢ºä¿ session å‰µå»ºæ˜¯ç·šç¨‹å®‰å…¨çš„
2. **ç‹€æ…‹ç®¡ç†**ï¼š`self.sessions: HashMap<SessionKey, SessionContext>` æ˜¯ Actor çš„ç§æœ‰ç‹€æ…‹
3. **è¶…æ™‚ç®¡ç†**ï¼šActor å¯ä»¥å‘è‡ªå·±ç™¼é€å»¶é²æ¶ˆæ¯ï¼ˆ`send_with_delay`ï¼‰ï¼Œå¯¦ç¾é–’ç½® session è‡ªå‹•æ¸…ç†

---

# ç¬¬ä¸‰éšæ®µï¼šSQL å‘½ä»¤è™•ç†

## è·¯ç”±åˆ° handle_execute_sql_command()

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/service/plan_executor.rs:198-231`

```rust
// server.rs:100-101
CommandType::SqlCommand(sql) => {
    service::handle_execute_sql_command(&ctx, sql, metadata).await?
}
```

ğŸ”¸ **handle_execute_sql_command æºç¢¼**

```rust
// plan_executor.rs:198-231
pub(crate) async fn handle_execute_sql_command(
    ctx: &SessionContext,
    sql: SqlCommand,
    metadata: ExecutorMetadata,
) -> SparkResult<ExecutePlanResponseStream> {
    let spark = ctx.extension::<SparkSession>()?;  // å–å¾— SparkSession

    // å°‡ SQL å‘½ä»¤åŒ…è£æˆ Relation
    let relation = if let Some(input) = sql.input {
        input  // å¦‚æœæœ‰ input DataFrameï¼Œä½¿ç”¨å®ƒ
    } else {
        // å¦å‰‡å‰µå»ºä¸€å€‹ SQL Relation
        Relation {
            common: None,
            #[expect(deprecated)]
            rel_type: Some(relation::RelType::Sql(crate::spark::connect::Sql {
                query: sql.sql,           // "SELECT 1 + 1 AS sum"
                args: sql.args,           // SQL åƒæ•¸ï¼ˆå¦‚æœæœ‰ï¼‰
                pos_args: sql.pos_args,
                named_arguments: sql.named_arguments,
                pos_arguments: sql.pos_arguments,
            })),
        }
    };

    // å°‡ Relation è½‰æ›ç‚º Sail å…§éƒ¨çš„ spec::Plan
    let plan = relation.try_into()?;

    // é€²å…¥çµ±ä¸€çš„è¨ˆç•«åŸ·è¡Œæµç¨‹
    handle_execute_plan(ctx, plan, metadata, ExecutePlanMode::Lazy).await
}
```

ğŸ”¸ **é—œéµè½‰æ›ï¼šProtobuf Relation â†’ spec::Plan**

`relation.try_into()?` é€™è¡Œæœƒèª¿ç”¨ `impl TryFrom<Relation> for spec::Plan`ï¼Œå°‡ gRPC çš„ protobuf æ¶ˆæ¯è½‰æ›ç‚º Sail å…§éƒ¨çš„è¨ˆç•«è¡¨ç¤ºã€‚

å°æ–¼ SQL æŸ¥è©¢ï¼Œæœƒç”Ÿæˆï¼š
```rust
spec::Plan::Query(spec::QueryPlan {
    plan_id: ...,
    node: spec::QueryNode::Read {
        read_type: spec::ReadType::Sql { query: "SELECT 1 + 1 AS sum", ... },
        is_streaming: false,
    },
})
```

---

# ç¬¬å››éšæ®µï¼šè¨ˆç•«è§£æèˆ‡å„ªåŒ–

## handle_execute_plan() â†’ resolve_and_execute_plan()

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/service/plan_executor.rs:109-144`

```rust
// plan_executor.rs:109-144
async fn handle_execute_plan(
    ctx: &SessionContext,
    plan: spec::Plan,
    metadata: ExecutorMetadata,
    mode: ExecutePlanMode,
) -> SparkResult<ExecutePlanResponseStream> {
    let spark = ctx.extension::<SparkSession>()?;
    let operation_id = metadata.operation_id.clone();

    // æ ¸å¿ƒï¼šè§£æä¸¦åŸ·è¡Œè¨ˆç•«ï¼ˆé€™è£¡æœƒç¶“éå¤šå±¤è½‰æ›ï¼‰
    let (plan, _) = resolve_and_execute_plan(ctx, spark.plan_config()?, plan).await?;

    // èª¿ç”¨ JobRunner åŸ·è¡Œç‰©ç†è¨ˆç•«
    let stream = spark.job_runner().execute(ctx, plan).await?;

    // æ ¹æ“šæ¨¡å¼å‰µå»ºä¸åŒçš„åŸ·è¡Œå™¨
    let rx = match mode {
        ExecutePlanMode::Lazy => {
            // æ‡¶åŸ·è¡Œï¼šå®¢æˆ¶ç«¯è®€å–æ™‚æ‰çœŸæ­£è¨ˆç®—
            let executor = Executor::new(metadata, stream, spark.options().execution_heartbeat_interval);
            let rx = executor.start()?;
            spark.add_executor(executor)?;  // è¨»å†Š executor ä»¥ä¾¿ä¸­æ–·/é‡é€£
            rx
        }
        ExecutePlanMode::EagerSilent => {
            // æ€¥åˆ‡åŸ·è¡Œï¼šç«‹å³åŸ·è¡Œä½†ä¸è¿”å›æ•¸æ“šï¼ˆç”¨æ–¼ DDLï¼‰
            let _ = read_stream(stream).await?;
            let (tx, rx) = tokio::sync::mpsc::channel(1);
            if metadata.reattachable {
                tx.send(ExecutorOutput::complete()).await?;
            }
            ReceiverStream::new(rx)
        }
    };

    Ok(ExecutePlanResponseStream::new(spark.session_id().to_string(), operation_id, Box::pin(rx)))
}
```

## resolve_and_execute_plan() - ä¸‰å±¤è½‰æ›

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-plan/src/lib.rs:55-87`

ğŸ”¸ **Rust åŸºç¤çŸ¥è­˜**
- `Arc<dyn ExecutionPlan>`: Arc æ˜¯åŸå­å¼•ç”¨è¨ˆæ•¸æ™ºèƒ½æŒ‡é‡ï¼Œ`dyn` è¡¨ç¤ºå‹•æ…‹åˆ†ç™¼çš„ trait å°è±¡
- `StringifiedPlan`: ç”¨æ–¼ EXPLAIN çš„è¨ˆç•«å­—ä¸²è¡¨ç¤º
- `LogicalPlan`: DataFusion çš„é‚è¼¯è¨ˆç•«ï¼ˆè¡¨é”ã€Œåšä»€éº¼ã€ï¼‰
- `ExecutionPlan`: DataFusion çš„ç‰©ç†è¨ˆç•«ï¼ˆè¡¨é”ã€Œå¦‚ä½•åšã€ï¼‰

```rust
// sail-plan/src/lib.rs:55-87
pub async fn resolve_and_execute_plan(
    ctx: &SessionContext,
    config: Arc<PlanConfig>,
    plan: spec::Plan,
) -> PlanResult<(Arc<dyn ExecutionPlan>, Vec<StringifiedPlan>)> {
    let mut info = vec![];
    let resolver = PlanResolver::new(ctx, config);

    // ========== ç¬¬ä¸€æ­¥ï¼šSpark Spec â†’ DataFusion LogicalPlan ==========
    let NamedPlan { plan, fields } = resolver.resolve_named_plan(plan).await?;
    info.push(plan.to_stringified(PlanType::InitialLogicalPlan));

    // ========== ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œ DDL ä¸¦å–å¾— DataFrame ==========
    // é€™è£¡æœƒè™•ç† Extension ç¯€é»ï¼ˆå¦‚ CatalogCommandï¼‰
    let df = execute_logical_plan(ctx, plan).await?;
    let (session_state, plan) = df.into_parts();

    // ========== ç¬¬ä¸‰æ­¥ï¼šé‚è¼¯è¨ˆç•«å„ªåŒ– ==========
    let plan = session_state.optimize(&plan)?;

    // è™•ç† streaming è¨ˆç•«ï¼ˆå¦‚æœæ˜¯æµå¼æŸ¥è©¢ï¼‰
    let plan = if is_streaming_plan(&plan)? {
        rewrite_streaming_plan(plan)?
    } else {
        plan
    };
    info.push(plan.to_stringified(PlanType::FinalLogicalPlan));

    // ========== ç¬¬å››æ­¥ï¼šLogicalPlan â†’ PhysicalPlan ==========
    let plan = session_state.query_planner().create_physical_plan(&plan, &session_state).await?;

    // ========== ç¬¬äº”æ­¥ï¼šé‡å‘½åå­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
    let plan = if let Some(fields) = fields {
        rename_physical_plan(plan, &fields)?
    } else {
        plan
    };

    info.push(StringifiedPlan::new(PlanType::FinalPhysicalPlan, displayable(plan.as_ref()).indent(true).to_string()));

    Ok((plan, info))
}
```

ğŸ”¸ **ç¬¬ä¸€æ­¥è©³è§£ï¼šresolve_named_plan()**

å°æ–¼ SQL æŸ¥è©¢ `SELECT 1 + 1 AS sum`ï¼š

```rust
// sail-plan/src/resolver/plan.rs:16-29
pub async fn resolve_named_plan(&self, plan: spec::Plan) -> PlanResult<NamedPlan> {
    let mut state = PlanResolverState::new();
    match plan {
        spec::Plan::Query(query) => {
            // éè¿´è§£ææŸ¥è©¢è¨ˆç•«
            let plan = self.resolve_query_plan(query, &mut state).await?;
            let fields = Some(Self::get_field_names(plan.schema(), &state)?);
            Ok(NamedPlan { plan, fields })
        }
        spec::Plan::Command(command) => {
            // è§£æå‘½ä»¤ï¼ˆå¦‚ DDLã€DMLï¼‰
            let plan = self.resolve_command_plan(command, &mut state).await?;
            Ok(NamedPlan { plan, fields: None })
        }
    }
}
```

SQL æŸ¥è©¢æœƒç¶“éï¼š
1. **SQL å­—ä¸²è§£æ**ï¼ˆåœ¨ `spec::Plan` å‰µå»ºæ™‚å·²å®Œæˆï¼Œä½¿ç”¨ `sail-sql-parser`ï¼‰
2. **è¡¨å¼•ç”¨è§£æ**ï¼ˆå¦‚æœæœ‰ FROM å­å¥ï¼‰
3. **è¡¨é”å¼è§£æ**ï¼ˆ`1 + 1` æœƒè¢«è§£æç‚º `Add(Literal(1), Literal(1))`ï¼‰
4. **åˆ¥åè™•ç†**ï¼ˆ`AS sum`ï¼‰

ç”Ÿæˆçš„ DataFusion `LogicalPlan` å¤§è‡´å¦‚ä¸‹ï¼š

```
Projection: 1 + 1 AS sum
  EmptyRelation
```

ğŸ”¸ **ç¬¬å››æ­¥è©³è§£ï¼šcreate_physical_plan()**

ä½¿ç”¨ Sail çš„è‡ªå®šç¾© `QueryPlanner`ï¼ˆåœ¨ `sail-plan/src/planner.rs` ä¸­å®šç¾©ï¼‰ï¼Œå°‡é‚è¼¯è¨ˆç•«è½‰æ›ç‚ºç‰©ç†è¨ˆç•«ï¼š

```
ProjectionExec: expr=[1 + 1 AS sum]
  EmptyExec: produce_one_row=true
```

---

# ç¬¬äº”éšæ®µï¼šè¨ˆç•«åŸ·è¡Œï¼ˆJobRunnerï¼‰

## JobRunner Trait èˆ‡å…©ç¨®å¯¦ä½œ

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-execution/src/job/runner.rs:11-93`

```rust
// runner.rs:11-22
#[tonic::async_trait]
pub trait JobRunner: Send + Sync + 'static {
    async fn execute(
        &self,
        ctx: &SessionContext,
        plan: Arc<dyn ExecutionPlan>,
    ) -> ExecutionResult<SendableRecordBatchStream>;

    async fn stop(&self);
}
```

ğŸ”¸ **Local Mode: LocalJobRunner**

```rust
// runner.rs:44-60
async fn execute(
    &self,
    ctx: &SessionContext,
    plan: Arc<dyn ExecutionPlan>,
) -> ExecutionResult<SendableRecordBatchStream> {
    if self.stopped.load(Ordering::Relaxed) {
        return Err(ExecutionError::InternalError("job runner is stopped".to_string()));
    }
    // ç›´æ¥ä½¿ç”¨ DataFusion åŸ·è¡Œ
    Ok(execute_stream(plan, ctx.task_ctx())?)
}
```

`execute_stream()` æ˜¯ DataFusion çš„å‡½æ•¸ï¼Œæœƒï¼š
1. å‘¼å« `plan.execute()` å–å¾— `SendableRecordBatchStream`
2. é€™æ˜¯ä¸€å€‹ç•°æ­¥æµï¼ˆStreamï¼‰ï¼Œæ¯æ¬¡ poll æœƒç”¢ç”Ÿä¸€å€‹ `RecordBatch`

å°æ–¼ `SELECT 1 + 1`ï¼š
- `EmptyExec` ç”¢ç”Ÿä¸€è¡Œç©ºè¡Œ
- `ProjectionExec` è¨ˆç®— `1 + 1 = 2`ï¼Œç”¢ç”Ÿ `RecordBatch { schema: [sum: Int32], rows: [[2]] }`

ğŸ”¸ **Cluster Mode: ClusterJobRunner**

```rust
// runner.rs:75-93
async fn execute(
    &self,
    _ctx: &SessionContext,
    plan: Arc<dyn ExecutionPlan>,
) -> ExecutionResult<SendableRecordBatchStream> {
    let (tx, rx) = oneshot::channel();

    // å‘ DriverActor ç™¼é€åŸ·è¡Œä»»å‹™
    self.driver.send(DriverEvent::ExecuteJob { plan, result: tx }).await?;

    // ç­‰å¾… Driver è¿”å›çµæœæµ
    rx.await.map_err(|e| ExecutionError::InternalError(format!("failed to create job stream: {e}")))?
}
```

åœ¨ Cluster Mode ä¸‹ï¼Œ`DriverActor` æœƒï¼š
1. å°‡ç‰©ç†è¨ˆç•«åˆ†å‰²æˆå¤šå€‹ Stageï¼ˆåŸºæ–¼ shuffle é‚Šç•Œï¼‰
2. ç‚ºæ¯å€‹ Stage å‰µå»º Task
3. å°‡ Task åˆ†é…çµ¦ Workerï¼ˆé€šé gRPC èª¿ç”¨ `WorkerService::RunTask`ï¼‰
4. Worker åŸ·è¡Œ Taskï¼Œå°‡çµæœå¯«å…¥ shuffle å­˜å„²
5. Driver æ”¶é›†æœ€çµ‚çµæœï¼Œè¿”å›æµçµ¦å®¢æˆ¶ç«¯

è©³ç´°çš„ Cluster åŸ·è¡Œæµç¨‹è«‹åƒè€ƒ `SAIL_ARCHITECTURE.md` çš„ã€ŒCluster Mode æŸ¥è©¢åŸ·è¡Œæµç¨‹ï¼ˆ8 éšæ®µï¼‰ã€ç« ç¯€ã€‚

---

# ç¬¬å…­éšæ®µï¼šçµæœä¸²æµåŒ–

## Executorï¼šå°‡ RecordBatch æµè½‰æ›ç‚º gRPC éŸ¿æ‡‰æµ

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/executor.rs:96-149`

```rust
// executor.rs:96-122
pub(crate) struct Executor {
    pub(crate) metadata: ExecutorMetadata,
    state: Mutex<ExecutorState>,
}

enum ExecutorState {
    Idle,
    Pending(ExecutorTaskContext),  // ç­‰å¾…é–‹å§‹åŸ·è¡Œ
    Running(ExecutorTask),          // æ­£åœ¨åŸ·è¡Œ
    Pausing,
    Failed(SparkError),
}

struct ExecutorTaskContext {
    stream: SendableRecordBatchStream,  // DataFusion çš„çµæœæµ
    heartbeat_interval: Duration,       // å¿ƒè·³é–“éš”ï¼ˆé˜²æ­¢å®¢æˆ¶ç«¯è¶…æ™‚ï¼‰
    buffer: Arc<Mutex<ExecutorBuffer>>, // ç·©è¡å€ï¼ˆç”¨æ–¼é‡é€£ï¼‰
}
```

ğŸ”¸ **Executor::start() å•Ÿå‹•ç•°æ­¥ä»»å‹™**

```rust
impl Executor {
    pub fn start(&self) -> SparkResult<ReceiverStream<ExecutorOutput>> {
        let mut state = self.state.lock()?;
        match mem::replace(state.deref_mut(), ExecutorState::Idle) {
            ExecutorState::Pending(mut context) => {
                let (tx, rx) = mpsc::channel(8);  // å‰µå»º channel
                let (notifier, notified) = oneshot::channel();

                // åœ¨èƒŒæ™¯ spawn ä¸€å€‹ç•°æ­¥ä»»å‹™
                let handle = tokio::spawn(async move {
                    // ä¸»è¿´åœˆï¼šæŒçºŒå¾ stream è®€å– RecordBatch
                    loop {
                        tokio::select! {
                            // ç­‰å¾… notifier é—œé–‰ä¿¡è™Ÿ
                            _ = &mut notified => {
                                break Ok(());
                            }
                            // å¾ DataFusion stream è®€å–ä¸‹ä¸€æ‰¹æ•¸æ“š
                            batch = context.next() => {
                                let batch = batch?;
                                match batch {
                                    Some(batch) => {
                                        // å°‡ RecordBatch åºåˆ—åŒ–ç‚º Arrow IPC æ ¼å¼
                                        let output = ExecutorOutput::new(ExecutorBatch::ArrowBatch(to_arrow_batch(batch)?));
                                        context.save_output(&output)?;  // ä¿å­˜åˆ°ç·©è¡å€
                                        if tx.send(output).await.is_err() {
                                            break Ok(());  // å®¢æˆ¶ç«¯æ–·é–‹é€£æ¥
                                        }
                                    }
                                    None => {
                                        // æµçµæŸï¼Œç™¼é€å®Œæˆä¿¡è™Ÿ
                                        let output = ExecutorOutput::complete();
                                        context.save_output(&output)?;
                                        let _ = tx.send(output).await;
                                        break Ok(());
                                    }
                                }
                            }
                        }
                    }
                });

                *state = ExecutorState::Running(ExecutorTask { notifier, handle, buffer: context.buffer });
                Ok(ReceiverStream::new(rx))
            }
            _ => Err(SparkError::internal("executor is not pending")),
        }
    }
}
```

ğŸ”¸ **to_arrow_batch() - åºåˆ—åŒ–ç‚º Arrow IPC**

```rust
pub fn to_arrow_batch(batch: RecordBatch) -> SparkResult<ArrowBatch> {
    let mut cursor = Cursor::new(Vec::new());
    {
        let mut writer = StreamWriter::try_new(&mut cursor, &batch.schema())?;
        writer.write(&batch)?;
        writer.finish()?;
    }
    let data = cursor.into_inner();
    let row_count = batch.num_rows() as i64;
    Ok(ArrowBatch { row_count, data })
}
```

é€™è£¡ä½¿ç”¨ Apache Arrow çš„ IPCï¼ˆInter-Process Communicationï¼‰æ ¼å¼ï¼Œé€™æ˜¯ä¸€ç¨®é›¶æ‹·è²çš„åˆ—å¼æ•¸æ“šæ ¼å¼ï¼š
- ä¿ç•™ Arrow çš„å…§å­˜ä½ˆå±€
- å¯ä»¥ç›´æ¥åœ¨ PySpark ç«¯è§£æï¼Œç„¡éœ€ååºåˆ—åŒ–

---

# ç¬¬ä¸ƒéšæ®µï¼šgRPC éŸ¿æ‡‰æµè¿”å›

## ExecutePlanResponseStream

ğŸ”¸ **æª”æ¡ˆä½ç½®**
`crates/sail-spark-connect/src/service/plan_executor.rs:33-99`

```rust
// plan_executor.rs:33-47
pub struct ExecutePlanResponseStream {
    session_id: String,
    operation_id: String,
    inner: ExecutorOutputStream,  // Pin<Box<dyn Stream<Item = ExecutorOutput> + Send>>
}

impl Stream for ExecutePlanResponseStream {
    type Item = Result<ExecutePlanResponse, Status>;

    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Result<ExecutePlanResponse, Status>>> {
        // å¾ inner æµä¸­ poll ä¸‹ä¸€å€‹ ExecutorOutput
        self.inner.as_mut().poll_next(cx).map(|poll| {
            poll.map(|item| {
                // æ§‹å»º gRPC response
                let mut response = ExecutePlanResponse::default();
                response.session_id.clone_from(&self.session_id);
                response.server_side_session_id.clone_from(&self.session_id);
                response.operation_id.clone_from(&self.operation_id);
                response.response_id = item.id;

                // æ ¹æ“š batch é¡å‹è¨­ç½® response_type
                match item.batch {
                    ExecutorBatch::ArrowBatch(batch) => {
                        response.response_type = Some(ResponseType::ArrowBatch(batch));
                    }
                    ExecutorBatch::Complete => {
                        response.response_type = Some(ResponseType::ResultComplete(ResultComplete::default()));
                    }
                    // ... å…¶ä»–é¡å‹
                }

                Ok(response)
            })
        })
    }
}
```

ğŸ”¸ **gRPC Response çš„å…§å®¹**

å°æ–¼ `SELECT 1 + 1`ï¼Œæœƒè¿”å›å…©å€‹ `ExecutePlanResponse`ï¼š

**Response 1 (æ•¸æ“šæ‰¹æ¬¡)**
```protobuf
ExecutePlanResponse {
  session_id: "...",
  operation_id: "...",
  response_id: "uuid-1",
  response_type: ArrowBatch {
    row_count: 1,
    data: <Arrow IPC bytes containing [Row(sum=2)]>
  }
}
```

**Response 2 (å®Œæˆä¿¡è™Ÿ)**
```protobuf
ExecutePlanResponse {
  session_id: "...",
  operation_id: "...",
  response_id: "uuid-2",
  response_type: ResultComplete {}
}
```

---

# ç¬¬å…«éšæ®µï¼šPySpark å®¢æˆ¶ç«¯æ¥æ”¶çµæœ

PySpark çš„ Spark Connect å®¢æˆ¶ç«¯æœƒï¼š
1. å¾ gRPC æµä¸­è®€å–å¤šå€‹ `ExecutePlanResponse`
2. å°‡ `ArrowBatch.data` ååºåˆ—åŒ–ç‚º Arrow RecordBatch
3. å°‡å¤šå€‹ RecordBatch çµ„åˆæˆ DataFrame
4. ç•¶æ”¶åˆ° `ResultComplete` æ™‚çµæŸ

æœ€çµ‚ç”¨æˆ¶æ”¶åˆ°ï¼š
```python
[Row(sum=2)]
```

---

# é—œéµ Crate åŠŸèƒ½ç¸½çµ

| Crate                  | è·è²¬                                      | ä¸»è¦é¡å‹/å‡½æ•¸                                              |
|------------------------|-------------------------------------------|-----------------------------------------------------------|
| `sail-spark-connect`   | Spark Connect gRPC æœå‹™å¯¦ä½œ               | `SparkConnectServer`, `SessionManager`, `Executor`       |
| `sail-plan`            | Spark è¨ˆç•« â†’ DataFusion è¨ˆç•«è½‰æ›           | `PlanResolver`, `resolve_and_execute_plan()`             |
| `sail-sql-parser`      | Spark SQL è§£æå™¨ï¼ˆåŸºæ–¼ chumskyï¼‰           | `parser::parse()`, AST å®šç¾©                               |
| `sail-execution`       | åˆ†æ•£å¼åŸ·è¡Œå”èª¿ï¼ˆDriver/Workerï¼‰             | `JobRunner`, `DriverActor`, `WorkerActor`                |
| `sail-session`         | Session èˆ‡ Catalog ç®¡ç†                   | `CatalogManager`, `SparkSession`                         |
| `sail-logical-optimizer` | é‚è¼¯è¨ˆç•«å„ªåŒ–è¦å‰‡                         | `default_optimizer_rules()`                              |
| `sail-physical-optimizer`| ç‰©ç†è¨ˆç•«å„ªåŒ–è¦å‰‡                         | `get_physical_optimizers()`                              |
| `sail-common`          | å…±äº«çš„ spec å®šç¾©èˆ‡é…ç½®                    | `spec::Plan`, `spec::Expr`, `AppConfig`                  |

---

# å»ºè­°çš„é–±è®€é †åº

å¦‚æœä½ æƒ³æ·±å…¥å­¸ç¿’ Sail çš„æºç¢¼ï¼Œå»ºè­°æŒ‰ä»¥ä¸‹é †åºé–±è®€ï¼š

## ç¬¬ä¸€éšæ®µï¼šå…¥å£èˆ‡æ¶æ§‹ï¼ˆ1-2 å¤©ï¼‰

1. `crates/sail-cli/README.md` - äº†è§£å•Ÿå‹•æµç¨‹
2. `SAIL_ARCHITECTURE.md` - ç†è§£æ•´é«”æ¶æ§‹
3. `SAIL_ACTOR_MODEL.md` - ç†è§£ Actor ä¸¦ç™¼æ¨¡å‹
4. `crates/sail-cli/src/main.rs` - çœ‹ CLI å¦‚ä½•è§£æå‘½ä»¤
5. `crates/sail-cli/src/spark/server.rs` - çœ‹æœå‹™å™¨å¦‚ä½•å•Ÿå‹•

## ç¬¬äºŒéšæ®µï¼šgRPC å±¤ï¼ˆ2-3 å¤©ï¼‰

6. `crates/sail-spark-connect/src/server.rs` - gRPC service å¯¦ä½œ
7. `crates/sail-spark-connect/src/session_manager.rs` - Session ç®¡ç†
8. `crates/sail-spark-connect/src/service/plan_executor.rs` - è¨ˆç•«åŸ·è¡Œå…¥å£
9. `crates/sail-spark-connect/src/executor.rs` - çµæœä¸²æµåŒ–

## ç¬¬ä¸‰éšæ®µï¼šè¨ˆç•«è§£æèˆ‡è½‰æ›ï¼ˆ3-5 å¤©ï¼‰

10. `crates/sail-common/src/spec/mod.rs` - Sail å…§éƒ¨è¨ˆç•«è¡¨ç¤º
11. `crates/sail-plan/src/lib.rs` - è¨ˆç•«è§£æä¸»å…¥å£
12. `crates/sail-plan/src/resolver/plan.rs` - è¨ˆç•«è§£æå™¨æ¡†æ¶
13. `crates/sail-plan/src/resolver/query/mod.rs` - æŸ¥è©¢è¨ˆç•«è§£æ
14. `crates/sail-plan/src/resolver/query/read.rs` - è¡¨è®€å–è§£æ
15. `crates/sail-plan/src/resolver/query/project.rs` - æŠ•å½±è§£æ
16. `crates/sail-plan/src/resolver/expression/mod.rs` - è¡¨é”å¼è§£æ
17. `crates/sail-plan/src/planner.rs` - è‡ªå®šç¾© QueryPlanner

## ç¬¬å››éšæ®µï¼šSQL è§£æå™¨ï¼ˆ2-3 å¤©ï¼‰

18. `crates/sail-sql-parser/src/parser/mod.rs` - SQL è§£æå™¨å…¥å£
19. `crates/sail-sql-parser/src/parser/statement.rs` - SQL èªå¥è§£æ
20. `crates/sail-sql-parser/src/parser/query.rs` - SELECT æŸ¥è©¢è§£æ
21. `crates/sail-sql-parser/src/parser/expression.rs` - è¡¨é”å¼è§£æ
22. `crates/sail-sql-parser/src/ast/mod.rs` - AST å®šç¾©

## ç¬¬äº”éšæ®µï¼šåŸ·è¡Œå±¤ï¼ˆ3-4 å¤©ï¼‰

23. `crates/sail-execution/src/job/runner.rs` - JobRunner trait
24. `crates/sail-execution/src/driver/mod.rs` - Driver æ¶æ§‹
25. `crates/sail-execution/src/driver/server.rs` - Driver gRPC æœå‹™
26. `crates/sail-execution/src/driver/event.rs` - Driver äº‹ä»¶å®šç¾©
27. `crates/sail-execution/src/driver/planner.rs` - åˆ†æ•£å¼è¨ˆç•«å™¨
28. `crates/sail-execution/src/worker/mod.rs` - Worker æ¶æ§‹
29. `crates/sail-execution/src/worker/server.rs` - Worker gRPC æœå‹™

## ç¬¬å…­éšæ®µï¼šCatalog èˆ‡æ•¸æ“šæºï¼ˆ2-3 å¤©ï¼‰

30. `crates/sail-session/src/catalog/mod.rs` - Catalog ç®¡ç†
31. `crates/sail-catalog/src/provider/mod.rs` - Catalog provider trait
32. `crates/sail-data-source/src/lib.rs` - æ•¸æ“šæºè¨»å†Šè¡¨
33. `crates/sail-delta-lake/src/lib.rs` - Delta Lake æ”¯æ´
34. `crates/sail-iceberg/src/lib.rs` - Iceberg æ”¯æ´

## ç¬¬ä¸ƒéšæ®µï¼šé«˜ç´šåŠŸèƒ½ï¼ˆé¸è®€ï¼‰

35. `crates/sail-python-udf/src/lib.rs` - Python UDF æ”¯æ´
36. `crates/sail-logical-optimizer/src/lib.rs` - é‚è¼¯å„ªåŒ–è¦å‰‡
37. `crates/sail-physical-optimizer/src/lib.rs` - ç‰©ç†å„ªåŒ–è¦å‰‡
38. `crates/sail-cache/src/lib.rs` - ç·©å­˜å¯¦ä½œ

---

# æ¯ä¸€å±¤çš„æ•¸æ“šæµè½‰æ›ç¸½çµ

```
PySpark SQL String
  |
  | gRPC Protobuf (Spark Connect Protocol)
  v
ExecutePlanRequest {
  plan: Plan {
    op_type: Command(SqlCommand { sql: "SELECT 1 + 1 AS sum" })
  }
}
  |
  | TryFrom<Relation> for spec::Plan
  v
spec::Plan::Query(spec::QueryPlan {
  node: QueryNode::Read { read_type: Sql { query: "SELECT 1 + 1 AS sum" } }
})
  |
  | PlanResolver::resolve_named_plan()
  v
DataFusion LogicalPlan
  Projection: 1 + 1 AS sum
    EmptyRelation
  |
  | SessionState::optimize()
  v
Optimized LogicalPlan (ç›¸åŒï¼Œå› ç‚ºé€™å€‹æŸ¥è©¢å¤ªç°¡å–®äº†)
  |
  | QueryPlanner::create_physical_plan()
  v
DataFusion PhysicalPlan (Arc<dyn ExecutionPlan>)
  ProjectionExec: expr=[1 + 1 AS sum]
    EmptyExec: produce_one_row=true
  |
  | JobRunner::execute()
  v
SendableRecordBatchStream (å¯¦ä½œäº† Stream<Item = Result<RecordBatch>>)
  |
  | Executor ç•°æ­¥ä»»å‹™ poll stream
  v
RecordBatch {
  schema: Schema([Field { name: "sum", data_type: Int32, ... }]),
  columns: [Int32Array([2])],
  num_rows: 1
}
  |
  | to_arrow_batch() - Arrow IPC åºåˆ—åŒ–
  v
ArrowBatch {
  row_count: 1,
  data: Vec<u8> (Arrow IPC format)
}
  |
  | ExecutePlanResponseStream
  v
ExecutePlanResponse {
  response_type: Some(ResponseType::ArrowBatch(ArrowBatch { ... }))
}
  |
  | gRPC Stream
  v
PySpark Client æ¥æ”¶ Arrow bytes
  |
  | PyArrow ååºåˆ—åŒ–
  v
PySpark DataFrame / Row objects
  [Row(sum=2)]
```

---

# å¸¸è¦‹å•é¡Œ

## Q1: ç‚ºä»€éº¼è¦å¾ Spark Plan è½‰æ›åˆ° DataFusion Planï¼Ÿ

A: Spark çš„è¨ˆç•«è¡¨ç¤ºèˆ‡ DataFusion çš„ä¸åŒï¼š
- Spark çš„ `Relation` æ˜¯é¢å‘ API çš„ï¼ˆRDD/DataFrame èªç¾©ï¼‰
- DataFusion çš„ `LogicalPlan` æ˜¯é¢å‘æŸ¥è©¢å„ªåŒ–çš„ï¼ˆé—œä¿‚ä»£æ•¸ï¼‰
- è½‰æ›å±¤ï¼ˆ`sail-plan`ï¼‰è² è²¬èªç¾©æ˜ å°„å’Œé¡å‹è½‰æ›

## Q2: Local Mode å’Œ Cluster Mode çš„ä¸»è¦å€åˆ¥æ˜¯ä»€éº¼ï¼Ÿ

A:
- **Local Mode**: `LocalJobRunner` ç›´æ¥èª¿ç”¨ `execute_stream()`ï¼ŒDataFusion åœ¨æœ¬åœ°å¤šç·šç¨‹åŸ·è¡Œ
- **Cluster Mode**: `ClusterJobRunner` é€šé `DriverActor` å°‡è¨ˆç•«åˆ†å‰²æˆ Stagesï¼Œåˆ†ç™¼çµ¦ Workers åŸ·è¡Œï¼Œé€šé gRPC é€šè¨Š

## Q3: Executor çš„ä½œç”¨æ˜¯ä»€éº¼ï¼Ÿ

A: Executor è² è²¬ï¼š
1. å°‡ DataFusion çš„ `RecordBatch` æµè½‰æ›ç‚º gRPC éŸ¿æ‡‰æµ
2. å®šæœŸç™¼é€å¿ƒè·³ï¼ˆç©º RecordBatchï¼‰é˜²æ­¢å®¢æˆ¶ç«¯è¶…æ™‚
3. ç·©è¡çµæœä»¥æ”¯æ´å®¢æˆ¶ç«¯é‡é€£ï¼ˆreattachable executionï¼‰
4. è™•ç†åŸ·è¡Œä¸­æ–·å’ŒéŒ¯èª¤

## Q4: Actor æ¨¡å‹åœ¨ Sail ä¸­çš„æ‡‰ç”¨å ´æ™¯ï¼Ÿ

A:
1. **SessionManager**: ç®¡ç†å¤šç”¨æˆ¶çš„å¤šå€‹ sessionï¼Œç¢ºä¿ä¸¦ç™¼å®‰å…¨
2. **DriverActor**: ç®¡ç†åˆ†æ•£å¼æŸ¥è©¢çš„ç‹€æ…‹ï¼ˆWorker è¨»å†Šã€Task èª¿åº¦ã€çµæœæ”¶é›†ï¼‰
3. **WorkerActor**: è™•ç† Task åŸ·è¡Œè«‹æ±‚ï¼Œç®¡ç† shuffle æ•¸æ“š

è©³è¦‹ `SAIL_ACTOR_MODEL.md`ã€‚

## Q5: SQL è§£æç™¼ç”Ÿåœ¨å“ªè£¡ï¼Ÿ

A:
- SQL å­—ä¸²åœ¨ `spec::Plan` å‰µå»ºæ™‚å°±å·²ç¶“è§£æå®Œæˆï¼ˆå¯èƒ½åœ¨ protobuf è½‰æ›å±¤ï¼‰
- `sail-sql-parser` ä½¿ç”¨ chumsky parser combinator æ¡†æ¶è§£æ Spark SQL èªæ³•
- è§£æçµæœæ˜¯ `sail-sql-parser::ast::Statement`ï¼Œä¹‹å¾Œè½‰æ›ç‚º `spec::Plan`

---

# ç¸½çµ

é€™ç¯‡æ–‡ç« è¿½è¹¤äº†ä¸€å€‹ SQL æŸ¥è©¢å¾å®¢æˆ¶ç«¯åˆ°æœå‹™å™¨ã€å¾å­—ä¸²åˆ°çµæœçš„å®Œæ•´æ—…ç¨‹ã€‚é—œéµæ­¥é©ŸåŒ…æ‹¬ï¼š

1. gRPC è«‹æ±‚æ¥æ”¶èˆ‡è·¯ç”±
2. Session ç®¡ç†ï¼ˆActor æ¨¡å‹ï¼‰
3. è¨ˆç•«è§£æèˆ‡è½‰æ›ï¼ˆSpark â†’ DataFusionï¼‰
4. è¨ˆç•«å„ªåŒ–èˆ‡ç‰©ç†è¨ˆç•«ç”Ÿæˆ
5. è¨ˆç•«åŸ·è¡Œï¼ˆLocal æˆ– Clusterï¼‰
6. çµæœä¸²æµåŒ–èˆ‡ Arrow IPC åºåˆ—åŒ–
7. gRPC éŸ¿æ‡‰æµè¿”å›

é€šéç†è§£é€™å€‹æµç¨‹ï¼Œä½ å°‡æŒæ¡ Sail çš„æ ¸å¿ƒæ¶æ§‹ï¼Œä¸¦èƒ½å¤ æ·±å…¥ç ”ç©¶ä»»ä½•æ„Ÿèˆˆè¶£çš„æ¨¡å¡Šã€‚

å»ºè­°çµåˆæºç¢¼é–±è®€ï¼Œä½¿ç”¨ `cargo doc --open` ç”Ÿæˆä¸¦æŸ¥çœ‹ Rust æ–‡æª”ï¼Œä¸¦å˜—è©¦åœ¨é—œéµä½ç½®æ·»åŠ æ—¥èªŒä»¥è§€å¯Ÿå¯¦éš›åŸ·è¡Œæµç¨‹ã€‚
