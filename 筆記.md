# èª¿ç”¨éˆ
```
sail-spark-connect/src/entrypoint.rs çš„ .serve
-> /Users/stanhsu/projects/sail/crates/sail-spark-connect/src/server.rs çš„ async fn execute_plan
  -> handle_execute_relation
    -> relation.try_into()
      -> impl TryFrom<Relation> for spec::Plan çš„ try_from
        -> rel_type.try_into()
          -> match RelType::Sql(sql) ç‚ºä¾‹ ä¸­ from_ast_statement(parse_one_statement(query.as_str())?)
            -> parse_one_statement
              -> parse_statements
                -> parse! macro
                  -> create_lexer é–‹å§‹è©æ³•åˆ†ææµç¨‹è½‰æˆ tokens
                    -> create_parser é–‹å§‹èªæ³•åˆ†ææµç¨‹è½‰æˆ AST
                      -> parser.parse(tokens)
                        -> statement(options)
            -> parse_one_statement
            -> from_ast_statement å‚³å› spec::Plan
  -> å›åˆ° handle_execute_plan
    -> resolve_and_execute_plan è½‰æˆ datafusion physical plan
        -> resolver.resolve_named_plan(plan).await?;
            -> resolve_query_plan()
                -> resolve_query_plan_with_hidden_fields()
                    -> å„ç¨® resolver æ ¹æ“š spec::QueryNode è½‰æˆ datafusion logical plan
        -> execute_logical_plan è¿”å› dataframe
    -> resolve_and_execute_plan è¿”å›ç‰©ç†è¨ˆåŠƒ


    -> spark.job_runner().execute åŸ·è¡Œç‰©ç†è¨ˆåŠƒ
        -> ç‰©ç†è¨ˆåŠƒä»€éº¼æ™‚å€™è®Šåˆ†æ•£å¼? Actor æ”¶åˆ°å¾Œæ”¹å¯« let graph = JobGraph::try_new(plan)?;
        -> æ·±åº¦ç†è§£ Actor ClusterJobRunner.execute() é–‹å§‹
            -> self.driver.send(DriverEvent::ExecuteJob { plan, result: tx })
                -> ActorHandle.send()
                    -> ActorRunner.run() è£¡é¢æ¥æ”¶åˆ°è¨Šæ¯ self.actor.receive()
                        -> DriverActor.receive() è£¡é¢æ¥æ”¶åˆ° DriverEvent::ExecuteJob
                            -> self.handle_execute_job(ctx, plan, result)
                                -> self.accept_job()
                                    -> let graph = JobGraph::try_new(plan)?;
                                        -> build_job_graph(plan, &mut graph)?;
                                            -> å°‡ RepartitionExec & CoalescePartitionsExec è½‰æˆ sail ShuffleWriteExec & ShuffleReadExec
                                        -> æ ¹æ“šæ–°å‚³å›çš„ graph ä¸­çš„ stage ä¸­çš„ partitions å»ºç«‹ tasks èˆ‡ job(stage) æ„æ€æ˜¯ stage æœƒæ ¹æ“š partitions åˆ†å‰²æˆå¤šå€‹ task
                            -> self.scale_up_workers(ctx); åˆ¤æ–·æ˜¯å¦éœ€è¦ éœ€è¦çš„è©±é€é k8s api å‰µå»º pod
                            -> self.schedule_tasks(ctx);
                                -> schedule_task é€é grpc è«‹æ±‚ç™¼é€åˆ° WorkerServer DriverActor å°±çµæŸäº†


                    -> WorkerServer.run_task() ç™¼é€ event åˆ° WorkerActor
                    -> WorkerActor.receive()
                        -> handle_run_task()
                            -> execute_plan()
                                -> datafusion physical plan execute
                                    -> SailExec æ‰æ˜¯çœŸæ­£åŸ·è¡Œè¨ˆåŠƒçš„åœ°æ–¹
                                        -> è¿”å› RecordBatchStream çµ¦ä¸Šå±¤
                        -> å‰µå»º TaskStreamMonitor ç›£æ§ task ç‹€æ…‹
                            -> run()
                                -> execute or cancel
                                    -> execute
                                        -> stream.next()
                                            -> æ‹¿åˆ° batch å¾Œå†çœ‹æœ‰æ²’æœ‰ sink æ±ºå®šè¦ä¸è¦ sink(æœ€å¾Œä¸€å€‹ stage)
                                    -> æˆåŠŸç™¼é€ workerEvent:Success
                    -> WorkerActor.receive()
                        -> handle_report_task_status()
                    -> DriverActor.receive()
                        -> handle_report_task_status
                            -> try_build_job_output
    -> job_runner.execute å›å‚³ SendableRecordBatchStream
    -> Executor::new(metadata, stream, heartbeat_interval)
        -> Executor::start()

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç”¨æˆ¶å±¤ï¼šPySpark Client                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“ gRPC
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ server.rs:execute_plan                              â”‚
  â”‚   æ¥æ”¶ ExecutePlanRequest                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ plan_executor.rs:handle_execute_relation            â”‚
  â”‚   â†’ handle_execute_plan                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ resolve_and_execute_plan                            â”‚
  â”‚   å°‡ Spark Relation è½‰æ›æˆ DataFusion Plan            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ spark.job_runner().execute(ctx, plan)               â”‚
  â”‚   â†“                                                 â”‚
  â”‚ ClusterJobRunner::execute                           â”‚
  â”‚   â†“                                                 â”‚
  â”‚ DriverActor.send(ExecuteJob { plan, result: tx })   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ DriverActor::handle_execute_job                     â”‚
  â”‚   â†“ build_job_graph (æ’å…¥ shuffle boundaries)        â”‚
  â”‚   â†“ schedule_tasks (åˆ†é…ä»»å‹™çµ¦ Workers)               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Workers åŸ·è¡Œä»»å‹™                                     â”‚
  â”‚   â†“ execute_plan                                    â”‚
  â”‚   â†“ rewrite_shuffle (æ³¨å…¥ stream accessors)          â”‚
  â”‚   â†“ Arrow Flight (worker-to-worker shuffle)         â”‚
  â”‚   â†“ å°‡çµæœå¯«å…¥ local_streams                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Workers å›å ±ä»»å‹™å®Œæˆ                                  â”‚
  â”‚   â†“ ReportTaskStatus { status: Finished }           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ DriverActor::handle_report_task_status              â”‚
  â”‚   â†“ try_build_job_output                            â”‚
  â”‚   â†“ é€é Arrow Flight å¾æ‰€æœ‰ Workers è®€å–è³‡æ–™          â”‚
  â”‚   â†“ MergedRecordBatchStream::new(streams)           â”‚
  â”‚   â†“ å›å‚³ SendableRecordBatchStream                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ job_runner.execute å›å‚³ SendableRecordBatchStream    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Executor::new(metadata, stream, heartbeat_interval) â”‚
  â”‚   â†“ ExecutorTaskContext::new(stream)                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Executor::start()                                   â”‚
  â”‚   â†“ mpsc::channel(1) å»ºç«‹ (tx, rx)                   â”‚
  â”‚   â†“ tokio::spawn(Executor::run(context, tx))        â”‚
  â”‚   â†“ å›å‚³ ReceiverStream::new(rx)                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (èƒŒæ™¯ task)              â†“ (ä¸»åŸ·è¡Œç·’)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Executor::run      â”‚    â”‚ ExecutePlanResponseStreamâ”‚
  â”‚   â†“ run_internal   â”‚    â”‚   åŒ…è£ ReceiverStream     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                           â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ while let Some(    â”‚    â”‚ impl Stream              â”‚
  â”‚   batch) =         â”‚    â”‚   poll_next()            â”‚
  â”‚   stream.next()    â”‚    â”‚     â†“ å¾ ReceiverStream  â”‚
  â”‚ {                  â”‚    â”‚       è®€å– ExecutorOutputâ”‚
  â”‚   tx.send(out)â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â†’ è½‰æ›æˆ              â”‚
  â”‚ }                  â”‚    â”‚       ExecutePlanResponseâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ gRPC streaming response  â”‚
                            â”‚   è‡ªå‹•åºåˆ—åŒ–ä¸¦ç™¼é€          â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â†“ gRPC
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ PySpark Client           â”‚
                            â”‚   æ¥æ”¶è³‡æ–™æµ               â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# Step1
grpc è«‹æ±‚æ‰“åˆ° /Users/stanhsu/projects/sail/crates/sail-spark-connect/src/entrypoint.rs
```rust
    ServerBuilder::new("sail_spark_connect", Default::default())
        .add_service(service, Some(crate::spark::connect::FILE_DESCRIPTOR_SET))
        .await
        .serve(listener, signal)
        .await
```

# Step2 ä»¥ execute_plan ç‚ºä¾‹
æœƒè½‰çµ¦ /Users/stanhsu/projects/sail/crates/sail-spark-connect/src/server.rs

```rust
#[tonic::async_trait]
impl SparkConnectService for SparkConnectServer {
    type ExecutePlanStream = ExecutePlanResponseStream;

    async fn execute_plan(ï¼‰...
```

æœƒè™•ç†é¡ä¼¼é€™ç¨® grpc å…§å®¹
```rust
ExecutePlanRequest {
    session_id: "abc123",           // UUID æ ¼å¼çš„ session æ¨™è­˜ç¬¦
    user_context: Some(UserContext {
        user_id: "user1",            // ç”¨æˆ¶ ID
    }),
    operation_id: Some("op456"),     // (å¯é¸) æ“ä½œ ID
    plan: Some(Plan {                // è¦åŸ·è¡Œçš„é‚è¼¯è¨ˆåŠƒ
        op_type: Some(Root(Relation {
            rel_type: Some(Sql(Sql {
                query: "SELECT * FROM users WHERE age > 18",
            }))
        }))
    }),
    tags: [],                        // (å¯é¸) æ¨™ç±¤
    request_options: [],             // (å¯é¸) è«‹æ±‚é¸é …
}
```

# Step3
async fn execute_plan å…§ get_or_create_session_context èˆ‡ handle_execute_relation

```rust
let ctx = self
            .session_manager
            .get_or_create_session_context(session_key)
            .await?;
let Plan { op_type: op } = request.plan.required("plan")?;
let op = op.required("plan op")?;
let stream = match op {
    plan::OpType::Root(relation) => {
        service::handle_execute_relation(&ctx, relation, metadata).await?
    }
```

# Step4
let plan = relation.try_into()?; æœƒæ ¹æ“šhandle_execute_planæ‰€éœ€è¦çš„ plan çš„é¡å‹ å…ˆè½‰æ› grpc to sail plan

```rust
pub(crate) async fn handle_execute_relation(
    ctx: &SessionContext,
    relation: Relation,
    metadata: ExecutorMetadata,
) -> SparkResult<ExecutePlanResponseStream> {
    let plan = relation.try_into()?;
    handle_execute_plan(ctx, plan, metadata, ExecutePlanMode::Lazy).await
}
```

```rust
async fn handle_execute_plan(
    ctx: &SessionContext,
    plan: spec::Plan,
```

```rust
impl TryFrom<Relation> for spec::Plan {
    type Error = SparkError;

    fn try_from(relation: Relation) -> SparkResult<spec::Plan> {
        let Relation { common, rel_type } = relation;
        let rel_type = rel_type.required("relation type")?;
        let node: RelationNode = rel_type.try_into()?;
        let metadata: RelationMetadata = common.into();
        match node {
            RelationNode::Query(query) => Ok(spec::Plan::Query(spec::QueryPlan {
                node: query,
                plan_id: metadata.plan_id,
            })),
            RelationNode::Command(command) => Ok(spec::Plan::Command(spec::CommandPlan {
                node: command,
                plan_id: metadata.plan_id,
            })),
        }
    }
}
```
let node: RelationNode = rel_type.try_into()?; æ ¹æ“š relation å…§çš„ rel_type ä¾†è½‰æ›æˆ sail plan çš„ RelationNode
æœ€å¾Œå†æ ¹æ“š match node è¿”å›å°æ‡‰çš„ spec::Plan

# Step5
æ‰€ä»¥éœ€è¦çœ‹çœ‹ let node: RelationNode = rel_type.try_into()?; çš„å¯¦ä½œ

```rust
impl TryFrom<RelType> for RelationNode {
    type Error = SparkError;

    fn try_from(rel_type: RelType) -> SparkResult<RelationNode> {
        match rel_type {
            ...
            ...
            ...
            RelType::Sql(sql) => {
                #[allow(deprecated)]
                let sc::Sql {
                    query,
                    args,
                    pos_args,
                    named_arguments,
                    pos_arguments,
                } = sql;
                match from_ast_statement(parse_one_statement(query.as_str())?)? {
                    spec::Plan::Query(input) => {
                        let positional_arguments =
                            match (pos_args.is_empty(), pos_arguments.is_empty()) {
                                (false, false) => {
                                    return Err(SparkError::invalid(
                                        "conflicting positional arguments",
                                    ))
                                }
                                (false, true) => pos_args
                                    .into_iter()
                                    .map(|x| Ok(spec::Expr::Literal(x.try_into()?)))
                                    .collect::<SparkResult<Vec<_>>>()?,
                                (true, false) => pos_arguments
                                    .into_iter()
                                    .map(|x| x.try_into())
                                    .collect::<SparkResult<Vec<_>>>()?,
                                (true, true) => vec![],
                            };
                        let named_arguments = match (args.is_empty(), named_arguments.is_empty()) {
                            (false, false) => {
                                return Err(SparkError::invalid("conflicting named arguments"))
                            }
                            (false, true) => args
                                .into_iter()
                                .map(|(k, v)| Ok((k, spec::Expr::Literal(v.try_into()?))))
                                .collect::<SparkResult<Vec<_>>>()?,
                            (true, false) => named_arguments
                                .into_iter()
                                .map(|(k, v)| Ok((k, v.try_into()?)))
                                .collect::<SparkResult<Vec<_>>>()?,
                            (true, true) => vec![],
                        };
                        Ok(RelationNode::Query(spec::QueryNode::WithParameters {
                            input: Box::new(input),
                            positional_arguments,
                            named_arguments,
                        }))
                    }
                    spec::Plan::Command(command) => {
                        if !pos_args.is_empty() || !args.is_empty() {
                            Err(SparkError::invalid("command with parameters"))
                        } else {
                            Ok(RelationNode::Command(command.node))
                        }
                    }
                }
            }
```

å…ˆä¾†çœ‹çœ‹ parse_one_statement èˆ‡ from_ast_statement


# Step6

```rust
pub fn parse_one_statement(s: &str) -> SqlResult<Statement> {
    let mut plan = parse_statements(s)?;
    match (plan.pop(), plan.is_empty()) {
        (Some(x), true) => Ok(x),
        _ => Err(SqlError::invalid("expected one statement")),
    }
}
```

```rust
pub fn parse_statements(s: &str) -> SqlResult<Vec<Statement>> {
    parse!(s, create_parser)
}
```

```rust
macro_rules! parse {
      ($input:ident, $parser:ident $(,)?) => {{
          // 1. å‰µå»ºé è¨­çš„ parser é¸é …
          let options = ParserOptions::default();
          let length = $input.len();

          // 2. å‰µå»º lexerï¼ˆè©æ³•åˆ†æå™¨ï¼‰
          let lexer = create_lexer::<_, chumsky::extra::Err<chumsky::error::Rich<_, _>>>(&options);

          // 3. åŸ·è¡Œè©æ³•åˆ†æï¼šå°‡å­—ä¸²åˆ‡æˆ tokens
          let tokens = lexer
              .parse($input)
              .into_result()
              .map_err(SqlError::parser)?;

          // 4. æº–å‚™ token streamï¼ˆä¾› parser ä½¿ç”¨ï¼‰
          let tokens = tokens
              .as_slice()
              .map((length..length).into(), map_parser_input);

          // 5. å‰µå»º parserï¼ˆèªæ³•åˆ†æå™¨ï¼‰
          let parser = $parser::<_, chumsky::extra::Err<chumsky::error::Rich<_, _>>>(&options);

          // 6. åŸ·è¡Œèªæ³•åˆ†æï¼šå°‡ tokens è§£ææˆ AST
          parser.parse(tokens).into_result().map_err(SqlError::parser)
      }};
  }
```

```rust
pub fn create_lexer<'a, I, E>(
      options: &ParserOptions,
  ) -> impl Parser<'a, I, Vec<(Token<'a>, I::Span)>, E>
  {
      choice((                           // å¾å¤šå€‹ parser ä¸­é¸ä¸€å€‹æˆåŠŸçš„
          single_line_comment(),         // è§£æ -- comment
          multi_line_comment(),          // è§£æ /* comment */
          string(options),               // è§£æå­—ä¸²å­—é¢å€¼
          word(),                        // è§£æå–®è©ï¼ˆé—œéµå­—æˆ–è­˜åˆ¥ç¬¦ï¼‰
          whitespace(' ', |count| Token::Space { count }),         // ç©ºæ ¼
          whitespace('\n', |count| Token::LineFeed { count }),     // æ›è¡Œ
          whitespace('\r', |count| Token::CarriageReturn { count }),  // å›è»Š
          whitespace('\t', |count| Token::Tab { count }),          // Tab
          punctuation(),                 // è§£ææ¨™é»ç¬¦è™Ÿï¼ˆ,;()[] ç­‰ï¼‰
      ))
      .repeated()                        // é‡è¤‡åŒ¹é…å¤šæ¬¡
      .collect()                         // æ”¶é›†æˆ Vec
      .then_ignore(end())                // æœ€å¾Œå¿…é ˆåˆ°é”è¼¸å…¥çµå°¾
  }
```

```
 ğŸ”¸ è©æ³•åˆ†ææµç¨‹åœ–

  è¼¸å…¥å­—ä¸²ï¼š
  "SELECT 1 -- comment\nFROM users"
          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  choice((                               â”‚
  â”‚    single_line_comment(),               â”‚  å˜—è©¦æ‰€æœ‰ parser
  â”‚    multi_line_comment(),                â”‚  ç›´åˆ°æœ‰ä¸€å€‹æˆåŠŸ
  â”‚    string(),                            â”‚
  â”‚    word(),                              â”‚
  â”‚    whitespace(...),                     â”‚
  â”‚    punctuation(),                       â”‚
  â”‚  ))                                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
      é€å­—å…ƒæƒæï¼Œæ¯æ¬¡åŒ¹é…ä¸€å€‹ tokenï¼š

      "SELECT " â†’ word() æˆåŠŸ
          â†“
      (Token::Word { raw: "SELECT", keyword: Some(Select) }, Span(0..6))

      " " â†’ whitespace(' ') æˆåŠŸ
          â†“
      (Token::Space { count: 1 }, Span(6..7))

      "1" â†’ word() æˆåŠŸ
          â†“
      (Token::Word { raw: "1", keyword: None }, Span(7..8))

      " -- comment" â†’ single_line_comment() æˆåŠŸ
          â†“
      (Token::SingleLineComment { raw: "-- comment" }, Span(8..19))

      "\n" â†’ whitespace('\n') æˆåŠŸ
          â†“
      (Token::LineFeed { count: 1 }, Span(19..20))

      "FROM " â†’ word() æˆåŠŸ
          â†“
      (Token::Word { raw: "FROM", keyword: Some(From) }, Span(20..24))

      ... ç¹¼çºŒç›´åˆ°çµå°¾
            â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  .repeated().collect()                  â”‚  æ”¶é›†æ‰€æœ‰ tokens
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
      Vec<(Token, Span)>
      [
          (Token::Word { raw: "SELECT", ... }, Span(0..6)),
          (Token::Space { count: 1 }, Span(6..7)),
          (Token::Word { raw: "1", ... }, Span(7..8)),
          (Token::SingleLineComment { ... }, Span(8..19)),
          (Token::LineFeed { count: 1 }, Span(19..20)),
          (Token::Word { raw: "FROM", ... }, Span(20..24)),
          ...
      ]
```

```rust
fn single_line_comment<'a, I, E>() -> impl Parser<'a, I, (Token<'a>, I::Span), E>
  where
      I: Input<'a, Token = char> + ValueInput<'a> + SliceInput<'a, Slice = &'a str>,
      E: ParserExtra<'a, I> + 'a,
  {
      just("--")                                    // 1. åŒ¹é… "--"
          .ignore_then(none_of("\n\r").repeated()) // 2. åŒ¹é…ä»»æ„éæ›è¡Œå­—å…ƒï¼ˆé‡è¤‡ï¼‰
          .map_with(|(), e| (                       // 3. è½‰æ›æˆ Token
              Token::SingleLineComment { raw: e.slice() },
              e.span()
          ))
  }

ğŸ”¸ é€æ­¥è§£æ

  1. just("--")

  - åŒ¹é…å›ºå®šçš„å­—ä¸² "--"
  - å¦‚æœè¼¸å…¥ä¸æ˜¯ -- é–‹é ­ï¼Œé€™å€‹ parser å¤±æ•—

  2. .ignore_then(none_of("\n\r").repeated())

  - ignore_thenï¼šå¿½ç•¥å‰é¢çš„çµæœï¼ˆ--ï¼‰ï¼Œåªä¿ç•™å¾Œé¢çš„çµæœ
  - none_of("\n\r")ï¼šåŒ¹é…ä»»ä½•ä¸æ˜¯ \n æˆ– \r çš„å­—å…ƒ
  - .repeated()ï¼šé‡è¤‡åŒ¹é… 0 æ¬¡æˆ–å¤šæ¬¡

  ç‚ºä»€éº¼ç”¨ none_of("\n\r")ï¼Ÿ
  - å–®è¡Œè¨»é‡‹åˆ°æ›è¡Œå°±çµæŸäº†
  - \n = Line Feed (Unix)
  - \r = Carriage Return (èˆŠ Mac æˆ– Windows çš„ \r\n)

  3. .map_with(|(), e| ...)

  - map_withï¼šè½‰æ› parser çš„çµæœï¼ŒåŒæ™‚å¯ä»¥è¨ªå•é¡å¤–è³‡è¨Š e
  - ()ï¼šå‰é¢ parser çš„çµæœï¼ˆæˆ‘å€‘å·²ç¶“ç”¨ ignore_then å¿½ç•¥äº†ï¼‰
  - eï¼šé¡å¤–è³‡è¨Šï¼ŒåŒ…å«ï¼š
    - e.slice()ï¼šåŒ¹é…çš„åŸå§‹å­—ä¸²
    - e.span()ï¼šä½ç½®è³‡è¨Š

  ç”Ÿæˆçš„ Tokenï¼š
  Token::SingleLineComment {
      raw: e.slice()  // ä¾‹å¦‚ï¼š"-- this is a comment"
  }

  ---
  ğŸ”¸ å¯¦éš›åŸ·è¡Œç¯„ä¾‹

  ç¯„ä¾‹ 1ï¼šç°¡å–®è¨»é‡‹

  è¼¸å…¥ï¼š
  -- this is a comment

  åŸ·è¡Œéç¨‹ï¼š
  ä½ç½® 0: è®€å– '-'
  ä½ç½® 1: è®€å– '-'
      â†“
  just("--") æˆåŠŸï¼
      â†“
  ä½ç½® 2: è®€å– ' ' â†’ ä¸æ˜¯ \n æˆ– \r â†’ åŒ¹é…
  ä½ç½® 3: è®€å– 't' â†’ ä¸æ˜¯ \n æˆ– \r â†’ åŒ¹é…
  ä½ç½® 4: è®€å– 'h' â†’ ä¸æ˜¯ \n æˆ– \r â†’ åŒ¹é…
  ...
  ä½ç½® 20: è®€å– 't' â†’ ä¸æ˜¯ \n æˆ– \r â†’ åŒ¹é…
  ä½ç½® 21: åˆ°é”çµå°¾æˆ–é‡åˆ° \n â†’ åœæ­¢
      â†“
  none_of("\n\r").repeated() æˆåŠŸï¼
      â†“
  map_with è½‰æ›ï¼š
      e.slice() = "-- this is a comment"
      e.span() = Span(0..21)
      â†“
  è¼¸å‡ºï¼š
  (Token::SingleLineComment { raw: "-- this is a comment" }, Span(0..21))
```

Tokens è½‰ Statements
```rust
pub fn create_parser<'a, I, E>(
      options: &'a ParserOptions,
  ) -> impl Parser<'a, I, Vec<Statement>, E> + Clone
  where
      I: Input<'a, Token = Token<'a>> + ValueInput<'a>,
      I::Span: Into<TokenSpan> + Clone,
      E: ParserExtra<'a, I> + 'a,
      E::Error: LabelError<'a, I, TokenLabel>,
  {
      let semicolon = Semicolon::parser((), options);

      // è§£ææµç¨‹ï¼š
      whitespace()                   // 1. å¿½ç•¥é–‹é ­çš„ç©ºç™½
          .repeated()
          .ignore_then(semicolon.clone().repeated())  // 2. å¿½ç•¥é–‹é ­çš„åˆ†è™Ÿ
          .ignore_then(
              statement(options)                       // 3. è§£æä¸€å€‹ statement
                  .then_ignore(semicolon.clone().ignored().or(end()))  // 4. å¿½ç•¥çµå°¾åˆ†è™Ÿæˆ– EOF
                  .then_ignore(semicolon.repeated())   // 5. å¿½ç•¥å¤šé¤˜åˆ†è™Ÿ
                  .repeated()                          // 6. é‡è¤‡è§£æå¤šå€‹ statements
                  .collect(),                          // 7. æ”¶é›†æˆ Vec<Statement>
          )
          .then_ignore(end())                          // 8. ç¢ºèªåˆ°é”çµå°¾
  }
```

æ¥çºŒçœ‹ä¸€ä¸‹ create_parser æ„Ÿè¦ºé—œéµæ˜¯ TreeParser

## statement() å‡½æ•¸è©³è§£

### ğŸ”¸ å‡½æ•¸ç°½å

```rust
fn statement<'a, I, E>(options: &'a ParserOptions)
    -> impl Parser<'a, I, Statement, E> + Clone
where
    I: Input<'a, Token = Token<'a>> + ValueInput<'a>,
    I::Span: Into<TokenSpan> + Clone,
    E: ParserExtra<'a, I> + 'a,
    E::Error: LabelError<'a, I, TokenLabel>,
```

**è¿”å›å€¼**ï¼šè¿”å›ä¸€å€‹ `Parser<Token, Statement>` é¡å‹çš„è§£æå™¨

**ä½œç”¨**ï¼šå‰µå»ºä¸€å€‹èƒ½å°‡ Token æµè§£ææˆ Statement AST çš„ parser

---

## ğŸ”¸ æ ¸å¿ƒå•é¡Œï¼šéè¿´èªæ³•çš„é›£é¡Œ

SQL èªæ³•æ˜¯**ç›¸äº’éè¿´**çš„ï¼š

```sql
-- Statement å¯ä»¥åŒ…å« Query
SELECT * FROM (SELECT 1) AS t

-- Query å¯ä»¥åŒ…å« Expression
SELECT col + 1 FROM users

-- Expression å¯ä»¥åŒ…å« Query (å­æŸ¥è©¢)
SELECT * FROM users WHERE id IN (SELECT user_id FROM orders)

-- Expression å¯ä»¥åŒ…å« Expression (åµŒå¥—)
SELECT (1 + (2 * (3 - 4))) FROM dual
```

**å•é¡Œ**ï¼šå¦‚ä½•åœ¨ Rust ä¸­å¯¦ç¾é€™ç¨®ç›¸äº’éè¿´çš„ parserï¼Ÿ

---

## ğŸ”¸ è§£æ±ºæ–¹æ¡ˆï¼šRecursive::declare() + define()

chumsky æä¾›äº† `Recursive` ä¾†è™•ç†éè¿´ parserï¼š

```rust
fn statement(options) -> impl Parser<Token, Statement> {
    // 1ï¸âƒ£ è²æ˜æ‰€æœ‰éœ€è¦éè¿´çš„ parsers
    let mut statement = Recursive::declare();
    let mut query = Recursive::declare();
    let mut expression = Recursive::declare();
    let mut data_type = Recursive::declare();
    let mut table_with_joins = Recursive::declare();

    // 2ï¸âƒ£ å®šç¾©æ¯å€‹ parser çš„å¯¦ä½œï¼ˆå¯ä»¥ç›¸äº’å¼•ç”¨ï¼‰
    statement.define(Statement::parser(
        (
            statement.clone(),    // Statement å¯ä»¥éè¿´å¼•ç”¨è‡ªå·±
            query.clone(),        // Statement å¯ä»¥åŒ…å« Query
            expression.clone(),   // Statement å¯ä»¥åŒ…å« Expression
            data_type.clone(),    // Statement å¯ä»¥åŒ…å« DataType
        ),
        options,
    ));

    query.define(Query::parser(
        (query.clone(), expression.clone(), table_with_joins.clone()),
        options,
    ));

    expression.define(Expr::parser(
        (expression.clone(), query.clone(), data_type.clone()),
        options,
    ));

    data_type.define(DataType::parser(data_type.clone(), options));

    table_with_joins.define(TableWithJoins::parser(
        (query.clone(), expression.clone(), table_with_joins.clone()),
        options,
    ));

    // 3ï¸âƒ£ è¿”å›å®šç¾©å¥½çš„ statement parser
    statement
}
```

---

## ğŸ”¸ å…©éšæ®µè¨­è¨ˆæ¨¡å¼

### éšæ®µ 1ï¼šdeclareï¼ˆè²æ˜ï¼‰

```rust
let mut statement = Recursive::declare();
//                  ^^^^^^^^^^^^^^^^^^^
//                  å‰µå»ºä¸€å€‹ã€Œç©ºçš„å ä½ç¬¦ã€
```

**ç›®çš„**ï¼šå‰µå»ºä¸€å€‹é‚„æ²’æœ‰å®šç¾©å¯¦ä½œçš„ parser å¼•ç”¨

**é¡å‹**ï¼š`Recursive<'a, I, Statement, E>`

**é¡æ¯”**ï¼šå°±åƒ C çš„å‰å‘è²æ˜ï¼ˆforward declarationï¼‰

```c
// C èªè¨€çš„å‰å‘è²æ˜
struct Node;  // è²æ˜ï¼Œä½†é‚„æ²’å®šç¾©

struct Node {
    int value;
    struct Node* next;  // å¯ä»¥å¼•ç”¨è‡ªå·±
};
```

### éšæ®µ 2ï¼šdefineï¼ˆå®šç¾©ï¼‰

```rust
statement.define(Statement::parser(
    (statement.clone(), query.clone(), ...),
    //^^^^^^^^^^^^^^^^
    //å¯ä»¥å¼•ç”¨è‡ªå·±ï¼å› ç‚ºå·²ç¶“ declare éäº†
    options,
));
```

**ç›®çš„**ï¼šå¡«å……å¯¦éš›çš„è§£æé‚è¼¯

**é—œéµ**ï¼šå› ç‚ºå·²ç¶“ `declare()` äº†ï¼Œæ‰€ä»¥å¯ä»¥åœ¨ `define()` æ™‚å¼•ç”¨è‡ªå·±

---

## ğŸ”¸ ä¾è³´é—œä¿‚åœ–

```
        statement
         â†™  â†“  â†˜
      query  expression  data_type
       â†™â†“â†˜      â†™â†“â†˜         â†“
  query expr tbl expr query  data_type
                â†“
            table_with_joins
             â†™  â†“  â†˜
        query expr tbl
```

**ç›¸äº’ä¾è³´**ï¼š
- Statement ä¾è³´ï¼šquery, expression, data_type, statement (è‡ªå·±)
- Query ä¾è³´ï¼šquery (è‡ªå·±), expression, table_with_joins
- Expression ä¾è³´ï¼šexpression (è‡ªå·±), query, data_type
- DataType ä¾è³´ï¼šdata_type (è‡ªå·±)
- TableWithJoins ä¾è³´ï¼šquery, expression, table_with_joins (è‡ªå·±)

---

## ğŸ”¸ Statement::parser å¦‚ä½•ä½¿ç”¨ä¾è³´ï¼Ÿ

```rust
statement.define(Statement::parser(
    (
        statement.clone(),    // ç¬¬ 1 å€‹åƒæ•¸
        query.clone(),        // ç¬¬ 2 å€‹åƒæ•¸
        expression.clone(),   // ç¬¬ 3 å€‹åƒæ•¸
        data_type.clone(),    // ç¬¬ 4 å€‹åƒæ•¸
    ),
    options,
));
```

é€™å€‹ tuple `(statement, query, expression, data_type)` æœƒè¢«å‚³çµ¦ `Statement::parser` çš„ç¬¬ä¸€å€‹åƒæ•¸ `args`ï¼š

```rust
// Statement çš„ TreeParser trait å¯¦ä½œï¼ˆç”± derive macro ç”Ÿæˆï¼‰
impl TreeParser for Statement {
    fn parser(
        args: (Statement, Query, Expr, DataType),
        //    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        //    é€™è£¡æ¥æ”¶ä¾è³´çš„ parsers
        options: &ParserOptions,
    ) -> impl Parser<Token, Statement> {
        // ä½¿ç”¨é€™äº› parsers ä¾†è§£æ Statement çš„å„å€‹ variant
    }
}
```

### å¯¦éš›ä½¿ç”¨ç¯„ä¾‹

```rust
// Statement çš„æŸå€‹ variant éœ€è¦è§£æ Query
Statement::Query(
    #[parser(function = |(_, q, _, _), _| q)]
    //                      ^
    //                      å¾ args çš„ç¬¬ 2 å€‹ä½ç½®æå– Query parser
    Query
)
```

**è§£é‡‹**ï¼š
- `|(_, q, _, _), _|`ï¼šé€™æ˜¯ä¸€å€‹é–‰åŒ…
- ç¬¬ 1 å€‹åƒæ•¸ï¼š`(statement, query, expression, data_type)` tuple
- æå– `q`ï¼ˆquery parserï¼‰ä¸¦è¿”å›

---

## ğŸ”¸ å®Œæ•´åŸ·è¡Œæµç¨‹ç¯„ä¾‹

### è¼¸å…¥ï¼š`"SELECT * FROM (SELECT 1) AS t"`

```rust
// 1. parser.parse(tokens) é–‹å§‹
parser.parse(tokens)
    â†“
// 2. èª¿ç”¨ statement parser
statement.parse(tokens)
    â†“
// 3. Statement::parser å˜—è©¦åŒ¹é…æ‰€æœ‰ variants
choice((
    query_parser.map(Statement::Query),  // â† å˜—è©¦é€™å€‹
    set_catalog_parser,
    // ...
))
    â†“
// 4. query_parser é–‹å§‹è§£æ
query.parse(tokens)
    â†“
// 5. Query::parser è­˜åˆ¥ SELECT
Select::parser() â†’ åŒ¹é… Token::Keyword(Select)
    â†“
// 6. è§£æ projection: *
Wildcard::parser() â†’ åŒ¹é… Token::Punctuation(Star)
    â†“
// 7. è§£æ FROM
From::parser() â†’ åŒ¹é… Token::Keyword(From)
    â†“
// 8. è§£æ table: (SELECT 1)
//    é€™è£¡é‡åˆ°æ‹¬è™Ÿï¼Œç™¼ç¾æ˜¯å­æŸ¥è©¢ï¼
SubqueryParser â†’ èª¿ç”¨ query.parse() éè¿´è§£æ
    â†“
// 9. éè¿´ï¼šå†æ¬¡èª¿ç”¨ query_parser
//    è§£æå…§éƒ¨çš„ "SELECT 1"
Query::parser() â†’
    Select â†’ Literal(1) â†’
    Query { body: Select { projection: [1], ... } }
    â†“
// 10. å›åˆ°å¤–å±¤ï¼Œç¹¼çºŒè§£æ AS t
Alias::parser() â†’ åŒ¹é… "AS t"
    â†“
// 11. çµ„åˆæˆå®Œæ•´çš„ Statement
Statement::Query(Query {
    body: Select {
        projection: [Wildcard],
        from: [SubqueryAlias {
            subquery: Query {
                body: Select {
                    projection: [Literal(1)],
                    ...
                }
            },
            alias: "t"
        }],
        ...
    }
})
```

---

## ğŸ”¸ ç‚ºä»€éº¼éœ€è¦ .clone()ï¼Ÿ

```rust
statement.define(Statement::parser(
    (
        statement.clone(),  // â† ç‚ºä»€éº¼è¦ cloneï¼Ÿ
        query.clone(),
        expression.clone(),
        data_type.clone(),
    ),
    options,
));

query.define(Query::parser(
    (query.clone(), ...),  // â† é€™è£¡åˆ clone äº†
    options,
));
```

**åŸå› **ï¼šæ¯å€‹ parser éƒ½éœ€è¦**æ“æœ‰**å…¶ä¾è³´çš„å¼•ç”¨

- `statement.define()` éœ€è¦ `query` çš„å¼•ç”¨
- `query.define()` ä¹Ÿéœ€è¦ `query` çš„å¼•ç”¨ï¼ˆéè¿´ï¼‰
- Rust ä¸å…è¨±å¤šå€‹ mutable å¼•ç”¨ï¼Œæ‰€ä»¥ç”¨ `clone()` å‰µå»ºæ–°çš„å¼•ç”¨

**å¯¦éš›æˆæœ¬**ï¼šå¾ˆä½ï¼`Recursive` å…§éƒ¨ä½¿ç”¨ `Rc`ï¼ˆå¼•ç”¨è¨ˆæ•¸ï¼‰ï¼Œclone åªæ˜¯å¢åŠ è¨ˆæ•¸

---

## ğŸ”¸ Recursive çš„å…§éƒ¨å¯¦ç¾ï¼ˆç°¡åŒ–ï¼‰

```rust
// chumsky å…§éƒ¨å¯¦ä½œï¼ˆç°¡åŒ–ç‰ˆï¼‰
pub struct Recursive<'a, I, O, E> {
    parser: Rc<RefCell<Option<BoxedParser<'a, I, O, E>>>>,
    //      ^^         ^^^^^^
    //      |          åŒ…è£ Optionï¼Œåˆå§‹ç‚º None
    //      å¼•ç”¨è¨ˆæ•¸ï¼Œclone å¾ˆä¾¿å®œ
}

impl<'a, I, O, E> Recursive<'a, I, O, E> {
    pub fn declare() -> Self {
        Recursive {
            parser: Rc::new(RefCell::new(None)),
            //                           ^^^^ åˆå§‹ç‚º None
        }
    }

    pub fn define<P>(&mut self, parser: P)
    where
        P: Parser<'a, I, O, E> + 'a,
    {
        *self.parser.borrow_mut() = Some(Box::new(parser));
        //                          ^^^^^^^^^^^^^^^^^
        //                          å¡«å……å¯¦éš›çš„ parser
    }
}

impl<'a, I, O, E> Parser<'a, I, O, E> for Recursive<'a, I, O, E> {
    fn parse(&self, input: I) -> ParseResult<O, E> {
        self.parser.borrow()
            //      ^^^^^^^ å–å¾—å…§éƒ¨çš„ parser
            .as_ref()
            .expect("parser not defined")
            .parse(input)
    }
}
```

---

## ğŸ”¸ ç¸½çµ

### statement() å‡½æ•¸åšäº†ä»€éº¼ï¼Ÿ

1. **è²æ˜ 5 å€‹éè¿´ parser**ï¼šstatement, query, expression, data_type, table_with_joins
2. **å®šç¾©å®ƒå€‘çš„å¯¦ä½œ**ï¼šæ¯å€‹éƒ½å¯ä»¥å¼•ç”¨å…¶ä»– parserï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
3. **è¿”å› statement parser**ï¼šä¸€å€‹å®Œæ•´çš„ã€èƒ½è™•ç†ç›¸äº’éè¿´èªæ³•çš„ parser

### é—œéµè¨­è¨ˆæ¨¡å¼

- **Two-Phase Definition**ï¼šå…ˆ declareï¼Œå¾Œ define
- **Dependency Injection**ï¼šå°‡ä¾è³´çš„ parsers ä½œç‚ºåƒæ•¸å‚³é
- **Derive Macro**ï¼šè‡ªå‹•ç”Ÿæˆè¤‡é›œçš„ parser çµ„åˆé‚è¼¯
- **Reference Counting**ï¼šç”¨ `Rc` è®“ clone å¾ˆä¾¿å®œ

### ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ

- **è™•ç†éè¿´èªæ³•**ï¼šSQL çš„èªæ³•å…ƒç´ ç›¸äº’åµŒå¥—
- **é¿å…ç„¡é™é¡å‹**ï¼šRust ä¸å…è¨±ç„¡é™å¤§å°çš„é¡å‹
- **ä¿æŒé¡å‹ç°¡å–®**ï¼šä¸ç”¨æ‰‹å¯«è¤‡é›œçš„éè¿´é¡å‹å®šç¾©


# ç†è§£ SQL è§£æ
é€é chumsky å¯¦ç¾è©æ³•åˆ†æèˆ‡èªæ³•åˆ†æ
create_lexer å…ˆæŠŠ SQL å­—ä¸²è½‰æˆ tokens
create_parser å†æŠŠ tokens è½‰æˆ AST (Statements)
chumsky æ˜¯å‰µå»ºä¸€é€£ä¸²çš„ parser ä¸¦çµ„åˆèµ·ä¾†å‘¼å« parse æ–¹æ³•


# -> parse_one_statement å›åˆ°é€™ç¹¼çºŒçœ‹ èª¿ç”¨éˆé‚£é‚Š



# 
  Client Request (PySpark)
           |
           v
  handle_execute_plan (å…¥å£é»)
           |
           |---> ctx.extension::<SparkSession>() [ç²å– Spark æœƒè©±]
           |
           |---> spark.plan_config() [ç²å–è¨ˆåŠƒé…ç½®]
           |
           |---> resolve_and_execute_plan [è§£æä¸¦å„ªåŒ–è¨ˆåŠƒ]
           |         |
           |         |---> PlanResolver.resolve_named_plan [è§£æé‚è¼¯è¨ˆåŠƒ]
           |         |---> execute_logical_plan [åŸ·è¡Œ DDL/ç›®éŒ„æ“ä½œ]
           |         |---> optimize [å„ªåŒ–é‚è¼¯è¨ˆåŠƒ]
           |         |---> create_physical_plan [ç”Ÿæˆç‰©ç†è¨ˆåŠƒ]
           |         â””---> rename_physical_plan [é‡å‘½åå­—æ®µ]
           |
           |---> job_runner.execute [åŸ·è¡Œç‰©ç†è¨ˆåŠƒ]
           |         |
           |         |---> LocalJobRunner: execute_stream [æœ¬åœ°åŸ·è¡Œ]
           |         â””---> ClusterJobRunner: DriverActor [åˆ†ä½ˆå¼åŸ·è¡Œ]
           |
           |---> æ ¹æ“šæ¨¡å¼åˆ†æ”¯:
           |     |
           |     |---> Lazy æ¨¡å¼:
           |     |       |---> Executor::new [å‰µå»ºåŸ·è¡Œå™¨]
           |     |       |---> executor.start [å•Ÿå‹•åŸ·è¡Œå™¨]
           |     |       |       |---> ExecutorTask::run [ç•°æ­¥ä»»å‹™]
           |     |       |       |---> run_internal [è™•ç†æ•¸æ“šæµ]
           |     |       |       â””---> è¿”å› ReceiverStream
           |     |       â””---> spark.add_executor [è¨»å†ŠåŸ·è¡Œå™¨]
           |     |
           |     â””---> EagerSilent æ¨¡å¼:
           |             |---> read_stream [ç«‹å³æ¶ˆè²»æ•¸æ“šæµ]
           |             |---> tokio::sync::mpsc::channel [å‰µå»ºé€šé“]
           |             â””---> ç™¼é€å®Œæˆä¿¡è™Ÿ
           |
           â””---> ExecutePlanResponseStream::new [å‰µå»ºéŸ¿æ‡‰æµ]
                     |
                     â””---> è¿”å›çµ¦å®¢æˆ¶ç«¯

  ç°¡å–®ä¾‹å­ï¼šSELECT 1+1 çš„åŸ·è¡Œéç¨‹

  å‡è¨­å®¢æˆ¶ç«¯åŸ·è¡Œ spark.sql("SELECT 1+1")ï¼š

  1. PySpark å®¢æˆ¶ç«¯é€šé Spark Connect å”è­°ç™¼é€è«‹æ±‚
  2. handle_execute_plan æ”¶åˆ° Spark SQL è¨ˆåŠƒ
  3. resolve_and_execute_plan å°‡ SQL è§£ææˆé‚è¼¯è¨ˆåŠƒ (Project)ï¼Œç„¶å¾Œå„ªåŒ–ä¸¦ç”Ÿæˆç‰©ç†è¨ˆåŠƒ
  4. job_runner.execute åŸ·è¡Œç‰©ç†è¨ˆåŠƒï¼Œè¿”å›æ•¸æ“šæµ
  5. Lazy æ¨¡å¼ä¸‹ï¼Œå‰µå»º Executor ä¾†ç•°æ­¥è™•ç†æ•¸æ“šæµ
  6. Executor ç™¼é€ Schema å’Œ ArrowBatch (åŒ…å«çµæœ 2)
  7. å®¢æˆ¶ç«¯æ”¶åˆ°éŸ¿æ‡‰æµï¼Œé¡¯ç¤ºçµæœ

  ---
  å»ºè­°é–±è®€é †åº

  1. å…ˆçœ‹æ•´é«”çµæ§‹ï¼šhandle_execute_plan (plan_executor.rs:110-145)
  2. ç†è§£è³‡æ–™çµæ§‹ï¼šExecutorMetadata, ExecutePlanMode, ExecutePlanResponseStream
  3. è¿½è¹¤è¨ˆåŠƒè§£æï¼šresolve_and_execute_plan (sail-plan/src/lib.rs:55-87)
  4. ç†è§£åŸ·è¡Œå¼•æ“ï¼šJobRunner trait (job/runner.rs:13-22)
  5. æ·±å…¥åŸ·è¡Œå™¨ï¼šExecutor (executor.rs:97-300)
  6. æœ€å¾Œçœ‹éŸ¿æ‡‰æµï¼šExecutePlanResponseStream (plan_executor.rs:34-100)

  ---
  è©³ç´°æºç¢¼è§£æ

  1. handle_execute_plan å‡½æ•¸ç°½å (å…¥å£é»)

  crates/sail-spark-connect/src/service/plan_executor.rs:110-115

  async fn handle_execute_plan(
      ctx: &SessionContext,
      plan: spec::Plan,
      metadata: ExecutorMetadata,
      mode: ExecutePlanMode,
  ) -> SparkResult<ExecutePlanResponseStream>

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - async fnï¼šé€™æ˜¯ä¸€å€‹ç•°æ­¥å‡½æ•¸ï¼Œå¯ä»¥ä½¿ç”¨ .await ä¾†ç­‰å¾…å…¶ä»–ç•°æ­¥æ“ä½œå®Œæˆ
  - &SessionContextï¼š& è¡¨ç¤ºå€Ÿç”¨ï¼ˆborrowï¼‰ï¼Œä¸æœƒå–å¾—æ‰€æœ‰æ¬Šï¼Œåªæ˜¯æš«æ™‚ä½¿ç”¨é€™å€‹å€¼ã€‚SessionContext æ˜¯ DataFusion çš„æœƒè©±ä¸Šä¸‹æ–‡
  - spec::Planï¼šSail å®šç¾©çš„ Spark è¨ˆåŠƒçµæ§‹ï¼Œé€™è£¡ä½¿ç”¨äº†ç§»å‹•èªç¾©ï¼ˆmoveï¼‰ï¼Œå‡½æ•¸æœƒå–å¾—æ‰€æœ‰æ¬Š
  - ExecutorMetadataï¼šåŒ…å«æ“ä½œ IDã€æ¨™ç±¤ã€æ˜¯å¦å¯é‡é€£ç­‰å…ƒæ•¸æ“š
  - ExecutePlanModeï¼šæšèˆ‰ï¼ˆenumï¼‰ï¼Œæ±ºå®šæ˜¯æ‡¶åŸ·è¡Œï¼ˆLazyï¼‰é‚„æ˜¯ç«‹å³åŸ·è¡Œï¼ˆEagerSilentï¼‰
  - -> SparkResult<ExecutePlanResponseStream>ï¼šè¿”å›é¡å‹ï¼ŒSparkResult<T> æ˜¯ Result<T, SparkError> çš„åˆ¥å

  ğŸ”¸ åŠŸèƒ½

  é€™æ˜¯åŸ·è¡Œ Spark è¨ˆåŠƒçš„ä¸»è¦å…¥å£é»ï¼Œè² è²¬å”èª¿æ•´å€‹åŸ·è¡Œæµç¨‹ï¼šæ¥æ”¶è¨ˆåŠƒã€è§£æå„ªåŒ–ã€åŸ·è¡Œã€ä¸¦è¿”å›éŸ¿æ‡‰æµçµ¦å®¢æˆ¶ç«¯ã€‚

  ---
  2. ç²å– SparkSession æ“´å±•

  crates/sail-spark-connect/src/service/plan_executor.rs:116

  let spark = ctx.extension::<SparkSession>()?;

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - ctx.extension::<SparkSession>()ï¼šä½¿ç”¨æ³›å‹æ–¹æ³• extensionï¼Œ<SparkSession> æ˜¯é¡å‹åƒæ•¸ï¼ˆtype parameterï¼‰ï¼Œç¨±ç‚º turbofish èªæ³•
  - ? é‹ç®—ç¬¦ï¼šé€™æ˜¯éŒ¯èª¤å‚³æ’­èªæ³•ï¼Œå¦‚æœè¿”å› Errï¼Œæœƒæå‰è¿”å›éŒ¯èª¤ï¼›å¦‚æœæ˜¯ Ok(value)ï¼Œæœƒè§£åŒ…å‡º value

  ğŸ”¸ èª¿ç”¨éˆè¿½è¹¤

  é€™å€‹æ–¹æ³•ä¾†è‡ª sail_common_datafusion::extension::SessionExtensionAccessor traitï¼Œå®ƒå…è¨±å¾ SessionContext ä¸­ç²å–è‡ªå®šç¾©æ“´å±•ã€‚

  crates/sail-common-datafusion/src/extension.rs (æ¨æ¸¬)

  pub trait SessionExtensionAccessor {
      fn extension<T: SessionExtension>(&self) -> Result<&T>;
  }

  ğŸ”¸ åŠŸèƒ½

  å¾ DataFusion çš„ SessionContext ä¸­æå–å‡º Sail è‡ªå®šç¾©çš„ SparkSession æ“´å±•ã€‚SparkSession å­˜å„²äº† Spark æœƒè©±çš„ç‹€æ…‹ï¼ŒåŒ…æ‹¬é…ç½®ã€åŸ·è¡Œå™¨ã€æµæŸ¥è©¢ç­‰ã€‚

  ---
  3. å…‹éš† operation_id

  crates/sail-spark-connect/src/service/plan_executor.rs:117

  let operation_id = metadata.operation_id.clone();

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - .clone()ï¼šå‰µå»ºä¸€å€‹æ·±æ‹·è²ã€‚å› ç‚º operation_id æ˜¯ String é¡å‹ï¼Œéœ€è¦å…‹éš†æ‰èƒ½åœ¨å¾ŒçºŒä½¿ç”¨ï¼ˆå› ç‚º metadata å¯èƒ½æœƒè¢«ç§»å‹•æˆ–æ¶ˆè²»ï¼‰
  - let ç¶å®šï¼šå‰µå»ºä¸€å€‹æ–°çš„ä¸å¯è®Šè®Šé‡

  ğŸ”¸ åŠŸèƒ½

  ä¿å­˜æ“ä½œ IDï¼Œç”¨æ–¼å¾ŒçºŒæ§‹å»ºéŸ¿æ‡‰æµæ™‚æ¨™è­˜é€™å€‹æ“ä½œã€‚

  ---
  4. è§£æä¸¦åŸ·è¡Œè¨ˆåŠƒ

  crates/sail-spark-connect/src/service/plan_executor.rs:118

  let (plan, _) = resolve_and_execute_plan(ctx, spark.plan_config()?, plan).await?;

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - let (plan, _) = ...ï¼šé€™æ˜¯æ¨¡å¼åŒ¹é…çš„è§£æ§‹èªæ³•ï¼Œå‡½æ•¸è¿”å›ä¸€å€‹å…ƒçµ„ (Arc<dyn ExecutionPlan>, Vec<StringifiedPlan>)ï¼Œæˆ‘å€‘åªéœ€è¦ç¬¬ä¸€å€‹å€¼ï¼Œæ‰€ä»¥ç”¨ _ å¿½ç•¥ç¬¬äºŒå€‹
  - spark.plan_config()?ï¼šèª¿ç”¨ plan_config æ–¹æ³•ç²å–é…ç½®ï¼Œ? è™•ç†å¯èƒ½çš„éŒ¯èª¤
  - .await?ï¼šå…ˆç­‰å¾…ç•°æ­¥æ“ä½œå®Œæˆï¼Œç„¶å¾Œç”¨ ? è™•ç†éŒ¯èª¤

  ğŸ”¸ èª¿ç”¨ï¼šspark.plan_config()

  crates/sail-spark-connect/src/session.rs:88-93

  pub(crate) fn plan_config(&self) -> SparkResult<Arc<PlanConfig>> {
      let state = self.state.lock()?;
      let mut config = PlanConfig::try_from(&state.config)?;
      config.session_user_id = self.user_id().to_string();
      Ok(Arc::new(config))
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - self.state.lock()?ï¼šstate æ˜¯ Mutex<SparkSessionState>ï¼Œ.lock() ç²å–äº’æ–¥é–ä¾†è¨ªå•å…§éƒ¨æ•¸æ“šï¼Œ? è™•ç†é–å¯èƒ½å¤±æ•—çš„æƒ…æ³
  - PlanConfig::try_from(&state.config)?ï¼šä½¿ç”¨ TryFrom trait å°‡ SparkRuntimeConfig è½‰æ›ç‚º PlanConfig
  - Arc::new(config)ï¼šå°‡é…ç½®åŒ…è£åœ¨ Arcï¼ˆåŸå­å¼•ç”¨è¨ˆæ•¸ï¼‰ä¸­ï¼Œé€™æ¨£å¯ä»¥åœ¨å¤šå€‹åœ°æ–¹å®‰å…¨å…±äº«

  ğŸ”¸ åŠŸèƒ½

  å¾ç•¶å‰æœƒè©±çš„é…ç½®å‰µå»ºä¸€å€‹ PlanConfigï¼ŒåŒ…å«æ™‚å€ã€ç”¨æˆ¶ ID ç­‰ä¿¡æ¯ï¼Œç”¨æ–¼å¾ŒçºŒçš„è¨ˆåŠƒè§£æã€‚

  ---
  ğŸ”¸ èª¿ç”¨ï¼šresolve_and_execute_plan()

  crates/sail-plan/src/lib.rs:55-87

  pub async fn resolve_and_execute_plan(
      ctx: &SessionContext,
      config: Arc<PlanConfig>,
      plan: spec::Plan,
  ) -> PlanResult<(Arc<dyn ExecutionPlan>, Vec<StringifiedPlan>)> {
      let mut info = vec![];
      let resolver = PlanResolver::new(ctx, config);
      let NamedPlan { plan, fields } = resolver.resolve_named_plan(plan).await?;
      info.push(plan.to_stringified(PlanType::InitialLogicalPlan));
      let df = execute_logical_plan(ctx, plan).await?;
      let (session_state, plan) = df.into_parts();
      let plan = session_state.optimize(&plan)?;
      let plan = if is_streaming_plan(&plan)? {
          rewrite_streaming_plan(plan)?
      } else {
          plan
      };
      info.push(plan.to_stringified(PlanType::FinalLogicalPlan));
      let plan = session_state
          .query_planner()
          .create_physical_plan(&plan, &session_state)
          .await?;
      let plan = if let Some(fields) = fields {
          rename_physical_plan(plan, &fields)?
      } else {
          plan
      };
      info.push(StringifiedPlan::new(
          PlanType::FinalPhysicalPlan,
          displayable(plan.as_ref()).indent(true).to_string(),
      ));
      Ok((plan, info))
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - let mut info = vec![]ï¼šå‰µå»ºä¸€å€‹å¯è®Šçš„ç©ºå‘é‡ï¼Œç”¨æ–¼å­˜å„²è¨ˆåŠƒçš„å­—ç¬¦ä¸²åŒ–ç‰ˆæœ¬ï¼ˆç”¨æ–¼èª¿è©¦/æ—¥èªŒï¼‰
  - PlanResolver::new(ctx, config)ï¼šé—œè¯å‡½æ•¸ï¼ˆassociated functionï¼‰ï¼Œé¡ä¼¼æ–¼éœæ…‹æ–¹æ³•
  - let NamedPlan { plan, fields } = ...ï¼šè§£æ§‹çµæ§‹é«”ï¼Œæå– plan å’Œ fields å­—æ®µ
  - df.into_parts()ï¼šæ¶ˆè²» DataFrameï¼Œè¿”å›å…§éƒ¨çš„ (SessionState, LogicalPlan)
  - if let Some(fields) = fieldsï¼šæ¨¡å¼åŒ¹é…ï¼Œå¦‚æœ fields æ˜¯ Some(value)ï¼Œå‰‡åŸ·è¡Œä»£ç¢¼å¡Š

  ğŸ”¸ åŠŸèƒ½

  é€™å€‹å‡½æ•¸åŸ·è¡Œå®Œæ•´çš„è¨ˆåŠƒè½‰æ›æµç¨‹ï¼š

  1. Line 61-62ï¼šå‰µå»º PlanResolver ä¸¦è§£æ Spark è¨ˆåŠƒç‚º DataFusion é‚è¼¯è¨ˆåŠƒ
  2. Line 63ï¼šè¨˜éŒ„åˆå§‹é‚è¼¯è¨ˆåŠƒ
  3. Line 64ï¼šåŸ·è¡Œé‚è¼¯è¨ˆåŠƒä¸­çš„ DDL èªå¥å’Œç›®éŒ„æ“ä½œ
  4. Line 65-66ï¼šå„ªåŒ–é‚è¼¯è¨ˆåŠƒ
  5. Line 67-71ï¼šå¦‚æœæ˜¯æµè¨ˆåŠƒï¼Œå‰‡é‡å¯«
  6. Line 72ï¼šè¨˜éŒ„æœ€çµ‚é‚è¼¯è¨ˆåŠƒ
  7. Line 73-76ï¼šå‰µå»ºç‰©ç†è¨ˆåŠƒï¼ˆå¯åŸ·è¡Œçš„è¨ˆåŠƒï¼‰
  8. Line 77-81ï¼šå¦‚æœéœ€è¦ï¼Œé‡å‘½åå­—æ®µ
  9. Line 82-85ï¼šè¨˜éŒ„ç‰©ç†è¨ˆåŠƒä¸¦è¿”å›

  ğŸ”¸ å­èª¿ç”¨ï¼šexecute_logical_plan()

  crates/sail-plan/src/lib.rs:35-53

  #[async_recursion]
  pub async fn execute_logical_plan(ctx: &SessionContext, plan: LogicalPlan) -> Result<DataFrame> {
      let plan = match plan {
          LogicalPlan::Extension(Extension { node }) => {
              if let Some(n) = node.as_any().downcast_ref::<CatalogCommandNode>() {
                  n.execute(ctx).await?
              } else if let Some(n) = node.as_any().downcast_ref::<WithPreconditionsNode>() {
                  for plan in n.preconditions() {
                      let _ = execute_logical_plan(ctx, plan.as_ref().clone()).await?;
                  }
                  n.plan().clone()
              } else {
                  LogicalPlan::Extension(Extension { node })
              }
          }
          x => x,
      };
      let df = ctx.execute_logical_plan(plan).await?;
      Ok(df)
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - #[async_recursion]ï¼šé€™æ˜¯ä¸€å€‹å®å±¬æ€§ï¼Œå…è¨±ç•°æ­¥å‡½æ•¸éæ­¸èª¿ç”¨è‡ªå·±
  - match plan { ... }ï¼šæ¨¡å¼åŒ¹é…ï¼Œé¡ä¼¼æ–¼ switch èªå¥ä½†æ›´å¼·å¤§
  - node.as_any().downcast_ref::<CatalogCommandNode>()ï¼šå‹•æ…‹é¡å‹è½‰æ›ï¼Œå˜—è©¦å°‡ trait object è½‰æ›ç‚ºå…·é«”é¡å‹
  - if let Some(n) = ...ï¼šå¦‚æœè½‰æ›æˆåŠŸï¼ˆè¿”å› Someï¼‰ï¼Œå‰‡åŸ·è¡Œä»£ç¢¼å¡Š

  ğŸ”¸ åŠŸèƒ½

  é€™å€‹å‡½æ•¸è™•ç†é‚è¼¯è¨ˆåŠƒä¸­çš„ç‰¹æ®Šç¯€é»ï¼š
  - CatalogCommandNodeï¼šåŸ·è¡Œç›®éŒ„å‘½ä»¤ï¼ˆå¦‚ CREATE TABLEã€DROP TABLE ç­‰ï¼‰
  - WithPreconditionsNodeï¼šå…ˆåŸ·è¡Œå‰ç½®æ¢ä»¶è¨ˆåŠƒï¼Œç„¶å¾ŒåŸ·è¡Œä¸»è¨ˆåŠƒ

  å°æ–¼æ™®é€šçš„è¨ˆåŠƒç¯€é»ï¼Œç›´æ¥äº¤çµ¦ DataFusion åŸ·è¡Œã€‚

  ---
  5. åŸ·è¡Œä½œæ¥­

  crates/sail-spark-connect/src/service/plan_executor.rs:119

  let stream = spark.job_runner().execute(ctx, plan).await?;

  ğŸ”¸ èª¿ç”¨ï¼šspark.job_runner()

  crates/sail-spark-connect/src/session.rs:283-285

  pub(crate) fn job_runner(&self) -> &dyn JobRunner {
      self.job_runner.as_ref()
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - &dyn JobRunnerï¼šè¿”å›ä¸€å€‹ trait object çš„å¼•ç”¨ï¼Œdyn è¡¨ç¤ºå‹•æ…‹åˆ†æ´¾ï¼ˆdynamic dispatchï¼‰
  - .as_ref()ï¼šå°‡ Box<dyn JobRunner> è½‰æ›ç‚º &dyn JobRunner

  ğŸ”¸ åŠŸèƒ½

  è¿”å›ä½œæ¥­åŸ·è¡Œå™¨çš„å¼•ç”¨ï¼Œå¯ä»¥æ˜¯ LocalJobRunner æˆ– ClusterJobRunnerã€‚

  ---
  ğŸ”¸ èª¿ç”¨ï¼šjob_runner.execute()

  æœ‰å…©ç¨®å¯¦ç¾ï¼š

  LocalJobRunner::execute

  crates/sail-execution/src/job/runner.rs:44-55

  async fn execute(
      &self,
      ctx: &SessionContext,
      plan: Arc<dyn ExecutionPlan>,
  ) -> ExecutionResult<SendableRecordBatchStream> {
      if self.stopped.load(Ordering::Relaxed) {
          return Err(ExecutionError::InternalError(
              "job runner is stopped".to_string(),
          ));
      }
      Ok(execute_stream(plan, ctx.task_ctx())?)
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - self.stopped.load(Ordering::Relaxed)ï¼šåŸå­æ“ä½œï¼Œè®€å–å¸ƒæ—å€¼ï¼ŒOrdering::Relaxed æ˜¯è¨˜æ†¶é«”é †åºèªç¾©
  - execute_stream(plan, ctx.task_ctx())?ï¼šDataFusion çš„å‡½æ•¸ï¼ŒåŸ·è¡Œç‰©ç†è¨ˆåŠƒä¸¦è¿”å›æ•¸æ“šæµ

  ğŸ”¸ åŠŸèƒ½

  åœ¨æœ¬åœ°åŸ·è¡Œç‰©ç†è¨ˆåŠƒï¼Œè¿”å›ä¸€å€‹ SendableRecordBatchStreamï¼ˆå¯ä»¥è·¨ç·šç¨‹ç™¼é€çš„ RecordBatch æµï¼‰ã€‚

  ClusterJobRunner::execute

  crates/sail-execution/src/job/runner.rs:75-88

  async fn execute(
      &self,
      _ctx: &SessionContext,
      plan: Arc<dyn ExecutionPlan>,
  ) -> ExecutionResult<SendableRecordBatchStream> {
      let (tx, rx) = oneshot::channel();
      self.driver
          .send(DriverEvent::ExecuteJob { plan, result: tx })
          .await?;
      rx.await.map_err(|e| {
          ExecutionError::InternalError(format!("failed to create job stream: {e}"))
      })?
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - oneshot::channel()ï¼šå‰µå»ºä¸€å€‹ä¸€æ¬¡æ€§é€šé“ï¼Œç”¨æ–¼æ¥æ”¶å–®å€‹å€¼
  - self.driver.send(...).await?ï¼šå‘ Driver Actor ç™¼é€åŸ·è¡Œä½œæ¥­çš„æ¶ˆæ¯
  - rx.awaitï¼šç­‰å¾…æ¥æ”¶ Driver çš„éŸ¿æ‡‰
  - .map_err(|e| ...)ï¼šå°‡éŒ¯èª¤é¡å‹è½‰æ›ç‚º ExecutionError

  ğŸ”¸ åŠŸèƒ½

  åœ¨åˆ†ä½ˆå¼é›†ç¾¤ä¸­åŸ·è¡Œç‰©ç†è¨ˆåŠƒï¼Œé€šé Actor æ¨¡å‹èˆ‡ Driver é€šä¿¡ã€‚

  ---
  6. æ ¹æ“šæ¨¡å¼è™•ç†åŸ·è¡Œçµæœ

  crates/sail-spark-connect/src/service/plan_executor.rs:120-139

  let rx = match mode {
      ExecutePlanMode::Lazy => {
          let executor = Executor::new(
              metadata,
              stream,
              spark.options().execution_heartbeat_interval,
          );
          let rx = executor.start()?;
          spark.add_executor(executor)?;
          rx
      }
      ExecutePlanMode::EagerSilent => {
          let _ = read_stream(stream).await?;
          let (tx, rx) = tokio::sync::mpsc::channel(1);
          if metadata.reattachable {
              tx.send(ExecutorOutput::complete()).await?;
          }
          ReceiverStream::new(rx)
      }
  };

  6.1 Lazy æ¨¡å¼

  ğŸ”¸ å‰µå»º Executor

  crates/sail-spark-connect/src/executor.rs:163-175

  pub(crate) fn new(
      metadata: ExecutorMetadata,
      stream: SendableRecordBatchStream,
      heartbeat_interval: Duration,
  ) -> Self {
      Self {
          metadata,
          state: Mutex::new(ExecutorState::Pending(ExecutorTaskContext::new(
              stream,
              heartbeat_interval,
          ))),
      }
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - Selfï¼šä»£è¡¨ç•¶å‰é¡å‹ Executor
  - Mutex::new(...)ï¼šå‰µå»ºäº’æ–¥é–ä¾†ä¿è­· ExecutorState
  - ExecutorState::Pending(...)ï¼šæšèˆ‰çš„ä¸€å€‹è®Šé«”ï¼Œè¡¨ç¤ºåŸ·è¡Œå™¨è™•æ–¼å¾…å•Ÿå‹•ç‹€æ…‹

  ğŸ”¸ åŠŸèƒ½

  å‰µå»ºä¸€å€‹æ–°çš„åŸ·è¡Œå™¨ï¼Œåˆå§‹ç‹€æ…‹æ˜¯ Pendingï¼ŒåŒ…å«äº†å¾…åŸ·è¡Œçš„æ•¸æ“šæµå’Œå¿ƒè·³é–“éš”ã€‚

  ---
  ğŸ”¸ å•Ÿå‹• Executor

  crates/sail-spark-connect/src/executor.rs:227-260

  pub(crate) fn start(&self) -> SparkResult<ReceiverStream<ExecutorOutput>> {
      let mut state = self.state.lock()?;
      let context = match mem::replace(state.deref_mut(), ExecutorState::Idle) {
          ExecutorState::Pending(context) => context,
          ExecutorState::Failed(e) => {
              *state = ExecutorState::Failed(SparkError::internal(
                  "task failed due to a previous error",
              ));
              return Err(e);
          }
          x @ ExecutorState::Idle => {
              *state = x;
              return Err(SparkError::internal("task context not found for operation"));
          }
          x @ ExecutorState::Running(_) => {
              *state = x;
              return Err(SparkError::internal("task is already running"));
          }
          x @ ExecutorState::Pausing => {
              *state = x;
              return Err(SparkError::internal("task is being paused"));
          }
      };
      let (tx, rx) = mpsc::channel(1);
      let (notifier, listener) = oneshot::channel();
      let buffer = Arc::clone(&context.buffer);
      let handle = tokio::spawn(async move { Executor::run(context, listener, tx).await });
      *state = ExecutorState::Running(ExecutorTask {
          notifier,
          handle,
          buffer,
      });
      Ok(ReceiverStream::new(rx))
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - mem::replace(state.deref_mut(), ExecutorState::Idle)ï¼šåŸå­æ€§åœ°æ›¿æ› state çš„å€¼ç‚º Idleï¼Œä¸¦è¿”å›èˆŠå€¼
  - x @ ExecutorState::Idleï¼šæ¨¡å¼åŒ¹é…ç¶å®šï¼Œå°‡åŒ¹é…çš„å€¼ç¶å®šåˆ° x
  - mpsc::channel(1)ï¼šå‰µå»ºå¤šç”Ÿç”¢è€…å–®æ¶ˆè²»è€…é€šé“ï¼Œå®¹é‡ç‚º 1
  - tokio::spawn(...)ï¼šåœ¨ Tokio é‹è¡Œæ™‚ä¸­ç”Ÿæˆæ–°çš„ç•°æ­¥ä»»å‹™
  - *state = ...ï¼šè§£å¼•ç”¨ä¸¦è³¦å€¼ï¼Œæ›´æ–°åŸ·è¡Œå™¨ç‹€æ…‹ç‚º Running

  ğŸ”¸ åŠŸèƒ½

  å•Ÿå‹•åŸ·è¡Œå™¨ï¼š
  1. ç²å–é–ä¸¦æå– Pending ç‹€æ…‹ä¸­çš„ä¸Šä¸‹æ–‡
  2. å‰µå»ºé€šé“ç”¨æ–¼ç™¼é€åŸ·è¡Œçµæœ
  3. å‰µå»º oneshot é€šé“ç”¨æ–¼æš«åœåŸ·è¡Œå™¨
  4. ç”Ÿæˆç•°æ­¥ä»»å‹™ä¾†é‹è¡ŒåŸ·è¡Œå™¨
  5. æ›´æ–°ç‹€æ…‹ç‚º Running
  6. è¿”å›æ¥æ”¶ç«¯ä¾›å®¢æˆ¶ç«¯æ¶ˆè²»

  ---
  ğŸ”¸ åŸ·è¡Œå™¨é‹è¡Œé‚è¼¯

  crates/sail-spark-connect/src/executor.rs:211-225

  async fn run(
      mut context: ExecutorTaskContext,
      listener: oneshot::Receiver<()>,
      tx: mpsc::Sender<ExecutorOutput>,
  ) -> ExecutorTaskResult {
      let out = tokio::select! {
          x = Executor::run_internal(&mut context, tx) => x,
          _ = listener => return ExecutorTaskResult::Paused(context),
      };
      match out {
          Ok(()) => ExecutorTaskResult::Completed,
          Err(SparkError::SendError(_)) => ExecutorTaskResult::Paused(context),
          Err(e) => ExecutorTaskResult::Failed(e),
      }
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - tokio::select!ï¼šå®ï¼Œç­‰å¾…å¤šå€‹ç•°æ­¥æ“ä½œï¼Œå“ªå€‹å…ˆå®Œæˆå°±åŸ·è¡Œå°æ‡‰çš„åˆ†æ”¯
  - x = Executor::run_internal(&mut context, tx) => xï¼šç­‰å¾…åŸ·è¡Œå®Œæˆ
  - _ = listener => ...ï¼šå¦‚æœæ”¶åˆ°æš«åœä¿¡è™Ÿï¼Œè¿”å› Paused

  ğŸ”¸ åŠŸèƒ½

  åŸ·è¡Œå™¨çš„ä¸»å¾ªç’°ï¼ŒåŒæ™‚ç›£è½å…©å€‹äº‹ä»¶ï¼š
  - åŸ·è¡Œæ•¸æ“šæµè™•ç†
  - æ¥æ”¶æš«åœä¿¡è™Ÿ

  ---
  ğŸ”¸ å…§éƒ¨åŸ·è¡Œé‚è¼¯

  crates/sail-spark-connect/src/executor.rs:177-209

  async fn run_internal(
      context: &mut ExecutorTaskContext,
      tx: mpsc::Sender<ExecutorOutput>,
  ) -> SparkResult<()> {
      for out in context.replay_outputs()? {
          tx.send(out).await?;
      }
      let schema = to_spark_schema(context.stream.schema())?;
      let out = ExecutorOutput::new(ExecutorBatch::Schema(Box::new(schema)));
      context.save_output(&out)?;
      tx.send(out).await?;

      let mut empty = true;
      while let Some(batch) = context.next().await? {
          let batch = to_arrow_batch(&batch)?;
          let out = ExecutorOutput::new(ExecutorBatch::ArrowBatch(batch));
          context.save_output(&out)?;
          tx.send(out).await?;
          empty = false;
      }
      if empty {
          let batch = RecordBatch::new_empty(context.stream.schema());
          let batch = to_arrow_batch(&batch)?;
          let out = ExecutorOutput::new(ExecutorBatch::ArrowBatch(batch));
          context.save_output(&out)?;
          tx.send(out).await?;
      }

      let out = ExecutorOutput::new(ExecutorBatch::Complete);
      context.save_output(&out)?;
      tx.send(out).await?;
      Ok(())
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - for out in context.replay_outputs()? { ... }ï¼šè¿­ä»£é‡æ”¾çš„è¼¸å‡ºï¼ˆç”¨æ–¼é‡é€£å ´æ™¯ï¼‰
  - while let Some(batch) = context.next().await? { ... }ï¼šå¾ªç’°è®€å–æ•¸æ“šæ‰¹æ¬¡ï¼Œç›´åˆ°æµçµæŸ

  ğŸ”¸ åŠŸèƒ½

  åŸ·è¡Œå™¨çš„æ ¸å¿ƒè™•ç†é‚è¼¯ï¼š
  1. Line 181-183ï¼šé‡æ”¾å·²ç·©å­˜çš„è¼¸å‡ºï¼ˆæ”¯æŒå¯é‡é€£ï¼‰
  2. Line 184-187ï¼šç™¼é€ Schema
  3. Line 189-196ï¼šå¾ªç’°è®€å–ä¸¦ç™¼é€æ•¸æ“šæ‰¹æ¬¡
  4. Line 197-203ï¼šå¦‚æœæ²’æœ‰æ•¸æ“šï¼Œç™¼é€ç©ºæ‰¹æ¬¡
  5. Line 205-207ï¼šç™¼é€å®Œæˆä¿¡è™Ÿ

  ---
  ğŸ”¸ è¨»å†Š Executor

  crates/sail-spark-connect/src/session.rs:148-153

  pub(crate) fn add_executor(&self, executor: Executor) -> SparkResult<()> {
      let mut state = self.state.lock()?;
      let id = executor.metadata.operation_id.clone();
      state.executors.insert(id, Arc::new(executor));
      Ok(())
  }

  ğŸ”¸ åŠŸèƒ½

  å°‡åŸ·è¡Œå™¨æ·»åŠ åˆ°æœƒè©±ç‹€æ…‹ä¸­ï¼Œç”¨æ–¼å¾ŒçºŒçš„ä¸­æ–·ã€é‡é€£ç­‰æ“ä½œã€‚

  ---
  6.2 EagerSilent æ¨¡å¼

  ğŸ”¸ è®€å–æ•´å€‹æµ

  crates/sail-spark-connect/src/executor.rs:302-306

  pub(crate) async fn read_stream(
      stream: SendableRecordBatchStream,
  ) -> SparkResult<Vec<RecordBatch>> {
      stream.err_into().try_collect::<Vec<_>>().await
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - .err_into()ï¼šå°‡æµçš„éŒ¯èª¤é¡å‹è½‰æ›ç‚ºç›®æ¨™éŒ¯èª¤é¡å‹
  - .try_collect::<Vec<_>>()ï¼šæ”¶é›†æµä¸­çš„æ‰€æœ‰å…ƒç´ åˆ°å‘é‡ï¼Œ<Vec<_>> ä¸­çš„ _ è®“ç·¨è­¯å™¨æ¨æ–·å…ƒç´ é¡å‹

  ğŸ”¸ åŠŸèƒ½

  ç«‹å³æ¶ˆè²»æ•´å€‹æ•¸æ“šæµï¼Œå°‡æ‰€æœ‰ RecordBatch æ”¶é›†åˆ°ä¸€å€‹å‘é‡ä¸­ã€‚é€™ç”¨æ–¼ä¸éœ€è¦æµå¼è¿”å›çš„å‘½ä»¤ï¼ˆå¦‚ DDLï¼‰ã€‚

  ---
  ğŸ”¸ å‰µå»ºå®Œæˆä¿¡è™Ÿ

  crates/sail-spark-connect/src/service/plan_executor.rs:133-137

  let (tx, rx) = tokio::sync::mpsc::channel(1);
  if metadata.reattachable {
      tx.send(ExecutorOutput::complete()).await?;
  }
  ReceiverStream::new(rx)

  ğŸ”¸ åŠŸèƒ½

  å‰µå»ºä¸€å€‹ç©ºçš„é€šé“ï¼Œå¦‚æœæ“ä½œå¯é‡é€£ï¼Œå‰‡ç™¼é€ä¸€å€‹å®Œæˆä¿¡è™Ÿã€‚é€™æ¨£å®¢æˆ¶ç«¯å¯ä»¥çŸ¥é“å‘½ä»¤å·²åŸ·è¡Œå®Œç•¢ã€‚

  ---
  7. å‰µå»ºéŸ¿æ‡‰æµ

  crates/sail-spark-connect/src/service/plan_executor.rs:140-144

  Ok(ExecutePlanResponseStream::new(
      spark.session_id().to_string(),
      operation_id,
      Box::pin(rx),
  ))

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - Box::pin(rx)ï¼šå°‡ rx å›ºå®šåœ¨å †ä¸Šï¼ŒPin ä¿è­‰ç•°æ­¥é¡å‹ä¸æœƒè¢«ç§»å‹•ï¼Œé€™æ˜¯ç•°æ­¥ Rust çš„è¦æ±‚

  ğŸ”¸ èª¿ç”¨ï¼šExecutePlanResponseStream::new

  crates/sail-spark-connect/src/service/plan_executor.rs:40-47

  pub fn new(session_id: String, operation_id: String, inner: ExecutorOutputStream) -> Self {
      Self {
          session_id,
          operation_id,
          inner,
      }
  }

  ğŸ”¸ åŠŸèƒ½

  å‰µå»ºéŸ¿æ‡‰æµåŒ…è£å™¨ï¼ŒåŒ…å«æœƒè©± ID å’Œæ“ä½œ IDï¼Œç”¨æ–¼å¾ŒçºŒå°‡åŸ·è¡Œçµæœè½‰æ›ç‚º gRPC éŸ¿æ‡‰ã€‚

  ---
  ğŸ”¸ éŸ¿æ‡‰æµçš„ Stream å¯¦ç¾

  crates/sail-spark-connect/src/service/plan_executor.rs:50-100

  impl Stream for ExecutePlanResponseStream {
      type Item = Result<ExecutePlanResponse, Status>;

      fn poll_next(
          mut self: Pin<&mut Self>,
          cx: &mut Context<'_>,
      ) -> Poll<Option<Result<ExecutePlanResponse, Status>>> {
          self.inner.as_mut().poll_next(cx).map(|poll| {
              poll.map(|item| {
                  let mut response = ExecutePlanResponse::default();
                  response.session_id.clone_from(&self.session_id);
                  response.server_side_session_id.clone_from(&self.session_id);
                  response.operation_id.clone_from(&self.operation_id.clone());
                  response.response_id = item.id;
                  match item.batch {
                      ExecutorBatch::ArrowBatch(batch) => {
                          response.response_type = Some(ResponseType::ArrowBatch(batch));
                      }
                      ExecutorBatch::SqlCommandResult(result) => {
                          response.response_type = Some(ResponseType::SqlCommandResult(*result));
                      }
                      ExecutorBatch::WriteStreamOperationStartResult(result) => {
                          response.response_type =
                              Some(ResponseType::WriteStreamOperationStartResult(*result));
                      }
                      ExecutorBatch::StreamingQueryCommandResult(result) => {
                          response.response_type =
                              Some(ResponseType::StreamingQueryCommandResult(*result));
                      }
                      ExecutorBatch::StreamingQueryManagerCommandResult(result) => {
                          response.response_type =
                              Some(ResponseType::StreamingQueryManagerCommandResult(*result));
                      }
                      ExecutorBatch::Schema(schema) => {
                          response.schema = Some(*schema);
                      }
                      ExecutorBatch::Complete => {
                          response.response_type =
                              Some(ResponseType::ResultComplete(ResultComplete::default()));
                      }
                  }
                  debug!("{response:?}");
                  Ok(response)
              })
          })
      }

      fn size_hint(&self) -> (usize, Option<usize>) {
          self.inner.size_hint()
      }
  }

  ğŸ”¸ Rust èªæ³•èªªæ˜

  - impl Stream for ExecutePlanResponseStreamï¼šç‚ºé¡å‹å¯¦ç¾ Stream trait
  - type Item = ...ï¼šé—œè¯é¡å‹ï¼Œå®šç¾©æµç”¢ç”Ÿçš„å…ƒç´ é¡å‹
  - fn poll_next(...) -> Poll<...>ï¼šç•°æ­¥æµçš„è¼ªè©¢æ–¹æ³•
  - Pin<&mut Self>ï¼šå›ºå®šçš„å¯è®Šå¼•ç”¨
  - .map(|poll| ...)ï¼šå°è¼ªè©¢çµæœé€²è¡Œè½‰æ›

  ğŸ”¸ åŠŸèƒ½

  å¯¦ç¾ Stream traitï¼Œä½¿ ExecutePlanResponseStream å¯ä»¥ä½œç‚ºç•°æ­¥æµä½¿ç”¨ã€‚æ¯æ¬¡è¼ªè©¢æ™‚ï¼š
  1. å¾å…§éƒ¨æµç²å– ExecutorOutput
  2. å‰µå»º ExecutePlanResponse ä¸¦å¡«å……æœƒè©±ä¿¡æ¯
  3. æ ¹æ“šæ‰¹æ¬¡é¡å‹è¨­ç½®éŸ¿æ‡‰é¡å‹
  4. è¿”å›çµ¦ gRPC æ¡†æ¶ç™¼é€çµ¦å®¢æˆ¶ç«¯

  ---
  å®Œæ•´èª¿ç”¨éˆç¸½çµ

  ä»¥ SELECT 1+1 ç‚ºä¾‹ï¼Œå®Œæ•´èª¿ç”¨éˆå¦‚ä¸‹ï¼š

  1. handle_execute_plan (å…¥å£)
      |
  2. ctx.extension::<SparkSession>() â†’ ç²å– Spark æœƒè©±
      |
  3. spark.plan_config() â†’ æ§‹å»º PlanConfig
      |
  4. resolve_and_execute_plan
      |
      4.1 PlanResolver::resolve_named_plan â†’ è§£æ SQL ç‚ºé‚è¼¯è¨ˆåŠƒ
      4.2 execute_logical_plan â†’ åŸ·è¡Œ DDL (ç„¡)
      4.3 SessionState::optimize â†’ å„ªåŒ–é‚è¼¯è¨ˆåŠƒ
      4.4 create_physical_plan â†’ ç”Ÿæˆç‰©ç†è¨ˆåŠƒ (ProjectionExec)
      4.5 rename_physical_plan â†’ é‡å‘½åå­—æ®µ (ç„¡éœ€)
      |
  5. job_runner.execute
      |
      5.1 LocalJobRunner â†’ execute_stream (æœ¬åœ°åŸ·è¡Œ)
      æˆ–
      5.2 ClusterJobRunner â†’ DriverActor (åˆ†ä½ˆå¼åŸ·è¡Œ)
      |
      è¿”å› SendableRecordBatchStream
      |
  6. æ ¹æ“šæ¨¡å¼è™•ç† (Lazy æ¨¡å¼)
      |
      6.1 Executor::new â†’ å‰µå»ºåŸ·è¡Œå™¨
      6.2 executor.start
          |
          6.2.1 tokio::spawn â†’ å•Ÿå‹•ç•°æ­¥ä»»å‹™
          6.2.2 Executor::run
                |
                6.2.3 run_internal
                      |
                      a. ç™¼é€ Schema
                      b. å¾ªç’°è®€å–ä¸¦ç™¼é€ ArrowBatch (1+1=2)
                      c. ç™¼é€ Complete
          |
          6.2.3 è¿”å› ReceiverStream
      |
      6.3 spark.add_executor â†’ è¨»å†ŠåŸ·è¡Œå™¨
      |
  7. ExecutePlanResponseStream::new â†’ å‰µå»ºéŸ¿æ‡‰æµ
      |
      7.1 Stream::poll_next â†’ è¼ªè©¢ä¸¦è½‰æ›ç‚º gRPC éŸ¿æ‡‰
      |
  8. è¿”å›çµ¦å®¢æˆ¶ç«¯ (PySpark)

  æ¯ä¸€å€‹å‡½æ•¸èª¿ç”¨éƒ½å·²åŒ…å«æºç¢¼å’Œè©³ç´°è§£é‡‹ï¼Œå¸Œæœ›é€™å€‹å®Œæ•´çš„èª¿ç”¨éˆè§£æèƒ½å¹«åŠ©ä½ ç†è§£ handle_execute_plan çš„å·¥ä½œåŸç†ï¼