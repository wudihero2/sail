# ä¸€å€‹ SQL åœ¨ Sail ä¸­çš„å®Œæ•´ç”Ÿå‘½é€±æœŸ

é€™ç¯‡æ–‡ç« å°‡è¿½è¹¤ä¸€å€‹ç°¡å–®çš„ SQL æŸ¥è©¢ï¼ˆä¾‹å¦‚ `SELECT 1 + 1`ï¼‰å¾ PySpark å®¢æˆ¶ç«¯ç™¼å‡ºï¼Œç¶“é Sail Spark Connect æœå‹™å™¨è™•ç†ï¼Œæœ€çµ‚è¿”å›çµæœçš„å®Œæ•´èª¿ç”¨éˆã€‚æˆ‘å€‘æœƒæ·±å…¥åˆ°æºç¢¼å±¤ç´šï¼Œè®“ä½ ç†è§£æ•´å€‹ Sail æ¶æ§‹ã€‚

## ç¯„ä¾‹ SQL

```python
# PySpark Client
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .remote("sc://localhost:50051") \
    .getOrCreate()

result = spark.sql("SELECT 1 + 1 AS sum").collect()
print(result)  # [Row(sum=2)]
```

---

# å‰ç½®çŸ¥è­˜ï¼šgRPC åŸºç¤æƒ¡è£œ

åœ¨æ·±å…¥ Sail æ¶æ§‹ä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å…ˆç†è§£ gRPCï¼Œå› ç‚ºæ•´å€‹ Spark Connect å”è­°éƒ½æ˜¯åŸºæ–¼ gRPC æ§‹å»ºçš„ã€‚

## ä»€éº¼æ˜¯ gRPCï¼Ÿ

ğŸ”¸ **å®šç¾©**

gRPC = **g**RPC **R**emote **P**rocedure **C**all

- ç”± Google é–‹ç™¼çš„é«˜æ€§èƒ½ã€é–‹æº RPC æ¡†æ¶
- è®“ä½ å¯ä»¥åƒèª¿ç”¨æœ¬åœ°å‡½æ•¸ä¸€æ¨£èª¿ç”¨é ç¨‹æœå‹™å™¨ä¸Šçš„å‡½æ•¸
- åŸºæ–¼ HTTP/2 å”è­°ï¼ˆç›¸æ¯” HTTP/1.1 æœ‰å·¨å¤§æ€§èƒ½æå‡ï¼‰

ğŸ”¸ **gRPC vs REST API**

| ç‰¹æ€§ | gRPC | REST API |
|------|------|----------|
| å”è­° | HTTP/2ï¼ˆäºŒé€²åˆ¶ï¼‰ | HTTP/1.1ï¼ˆæ–‡æœ¬ï¼‰ |
| æ•¸æ“šæ ¼å¼ | Protocol Buffersï¼ˆäºŒé€²åˆ¶ï¼‰ | JSONï¼ˆæ–‡æœ¬ï¼‰ |
| æ€§èƒ½ | å¿«ï¼ˆäºŒé€²åˆ¶åºåˆ—åŒ–ï¼‰ | æ…¢ï¼ˆJSON è§£æï¼‰ |
| ä¸²æµæ”¯æ´ | åŸç”Ÿæ”¯æ´é›™å‘æµ | éœ€è¦ WebSocket æˆ– SSE |
| ç€è¦½å™¨æ”¯æ´ | éœ€è¦ gRPC-Web | åŸç”Ÿæ”¯æ´ |
| é¡å‹å®‰å…¨ | å¼·é¡å‹ï¼ˆ.proto æ–‡ä»¶ï¼‰ | å¼±é¡å‹ï¼ˆéœ€è¦æ–‡æª”ï¼‰ |
| ä»£ç¢¼ç”Ÿæˆ | è‡ªå‹•ç”Ÿæˆå®¢æˆ¶ç«¯/æœå‹™å™¨ä»£ç¢¼ | éœ€è¦æ‰‹å‹•ç·¨å¯« |

---

## gRPC æ ¸å¿ƒæ¦‚å¿µ

### 1. Protocol Buffersï¼ˆ.proto æ–‡ä»¶ï¼‰

é€™æ˜¯ gRPC çš„ IDLï¼ˆInterface Definition Languageï¼‰ï¼Œç”¨ä¾†å®šç¾©æœå‹™å’Œæ¶ˆæ¯çµæ§‹ã€‚

ğŸ”¸ **ç¯„ä¾‹ï¼šSpark Connect çš„ ExecutePlanRequest**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/proto/spark/connect/base.proto`

```protobuf
// å®šç¾©æ¶ˆæ¯çµæ§‹
message ExecutePlanRequest {
  string session_id = 1;              // å­—æ®µç·¨è™Ÿ 1
  UserContext user_context = 2;       // åµŒå¥—æ¶ˆæ¯
  Plan plan = 3;                      // æŸ¥è©¢è¨ˆç•«
  repeated string tags = 5;           // repeated = æ•¸çµ„
  optional string operation_id = 6;   // optional = å¯é¸
}

message UserContext {
  string user_id = 1;
}

message Plan {
  oneof op_type {                     // oneof = åªèƒ½æœ‰ä¸€å€‹
    Relation root = 1;
    Command command = 2;
  }
}
```

ğŸ”¸ **ç‚ºä»€éº¼ç”¨ Protocol Buffersï¼Ÿ**

1. **é«˜æ•ˆåºåˆ—åŒ–**ï¼šäºŒé€²åˆ¶æ ¼å¼æ¯” JSON å° 3-10 å€
2. **å¼·é¡å‹**ï¼šç·¨è­¯æ™‚æª¢æŸ¥ï¼Œä¸æœƒç™¼é€éŒ¯èª¤çš„æ•¸æ“šé¡å‹
3. **å‘å¾Œå…¼å®¹**ï¼šæ–°å¢å­—æ®µä¸æœƒç ´å£èˆŠå®¢æˆ¶ç«¯
4. **è·¨èªè¨€**ï¼šå¯ä»¥ç”Ÿæˆ Pythonã€Rustã€Java ç­‰å¤šç¨®èªè¨€çš„ä»£ç¢¼

---

### 2. Service å®šç¾©ï¼ˆRPC æ–¹æ³•ï¼‰

ğŸ”¸ **ç¯„ä¾‹ï¼šSpark Connect Service**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/proto/spark/connect/base.proto`

```protobuf
service SparkConnectService {
  // åŸ·è¡ŒæŸ¥è©¢è¨ˆç•«ï¼ˆè¿”å›æµï¼‰
  rpc ExecutePlan(ExecutePlanRequest) returns (stream ExecutePlanResponse);

  // åˆ†æè¨ˆç•«ï¼ˆå–®æ¬¡è«‹æ±‚-éŸ¿æ‡‰ï¼‰
  rpc AnalyzePlan(AnalyzePlanRequest) returns (AnalyzePlanResponse);

  // é…ç½®ç®¡ç†ï¼ˆå–®æ¬¡è«‹æ±‚-éŸ¿æ‡‰ï¼‰
  rpc Config(ConfigRequest) returns (ConfigResponse);

  // æ·»åŠ è³‡æºï¼ˆå®¢æˆ¶ç«¯æµï¼‰
  rpc AddArtifacts(stream AddArtifactsRequest) returns (AddArtifactsResponse);
}
```

ğŸ”¸ **å››ç¨® RPC é¡å‹**

**Unary RPCï¼ˆä¸€å…ƒ RPCï¼‰**
```
Client  ----[Request]---->  Server
Client  <---[Response]----  Server

ç¯„ä¾‹ï¼šConfig()
```

**Server Streaming RPCï¼ˆæœå‹™å™¨æµå¼ RPCï¼‰**
```
Client  ----[Request]---->  Server
Client  <---[Response1]---  Server
Client  <---[Response2]---  Server
Client  <---[Response3]---  Server
Client  <---[End]----------  Server

ç¯„ä¾‹ï¼šExecutePlan() - æŸ¥è©¢çµæœå¯èƒ½å¾ˆå¤§ï¼Œåˆ†æ‰¹è¿”å›
```

**Client Streaming RPCï¼ˆå®¢æˆ¶ç«¯æµå¼ RPCï¼‰**
```
Client  ----[Request1]---->  Server
Client  ----[Request2]---->  Server
Client  ----[Request3]---->  Server
Client  ----[End]---------->  Server
Client  <---[Response]----   Server

ç¯„ä¾‹ï¼šAddArtifacts() - ä¸Šå‚³å¤§æ–‡ä»¶ï¼Œåˆ†å¡Šç™¼é€
```

**Bidirectional Streaming RPCï¼ˆé›™å‘æµå¼ RPCï¼‰**
```
Client  ----[Request1]---->  Server
Client  <---[Response1]---  Server
Client  ----[Request2]---->  Server
Client  <---[Response2]---  Server
Client  ----[End]---------->  Server
Client  <---[End]----------  Server

ç¯„ä¾‹ï¼šReattachExecute() - æ–·ç·šé‡é€£ï¼ŒæŒçºŒäº¤äº’
```

---

### 3. HTTP/2 çš„é—œéµç‰¹æ€§

gRPC åŸºæ–¼ HTTP/2ï¼Œäº«å—å…¶æ‰€æœ‰å„ªå‹¢ï¼š

ğŸ”¸ **å¤šè·¯å¾©ç”¨ï¼ˆMultiplexingï¼‰**

```
HTTP/1.1ï¼ˆæ¯å€‹è«‹æ±‚éœ€è¦ä¸€å€‹ TCP é€£æ¥ï¼‰:
Connection 1: [Request A] --> [Response A]
Connection 2: [Request B] --> [Response B]
Connection 3: [Request C] --> [Response C]

HTTP/2ï¼ˆå–®ä¸€ TCP é€£æ¥ï¼Œå¤šå€‹ streamï¼‰:
Connection:
  Stream 1: [Request A] --> [Response A]
  Stream 2: [Request B] --> [Response B]
  Stream 3: [Request C] --> [Response C]
```

å¥½è™•ï¼š
- æ¸›å°‘ TCP é€£æ¥æ•¸ï¼ˆç¯€çœè³‡æºï¼‰
- é¿å…éšŠé ­é˜»å¡ï¼ˆHead-of-Line Blockingï¼‰
- æ›´ä½çš„å»¶é²

ğŸ”¸ **é ­éƒ¨å£“ç¸®ï¼ˆHeader Compressionï¼‰**

HTTP/1.1 æ¯å€‹è«‹æ±‚éƒ½ç™¼é€å®Œæ•´çš„æ–‡æœ¬é ­éƒ¨ï¼š
```
POST /execute HTTP/1.1
Host: localhost:50051
Content-Type: application/json
Authorization: Bearer token123...
User-Agent: PySpark/3.5.0
...
(ç´„ 500-1000 bytes)
```

HTTP/2 ä½¿ç”¨ HPACK å£“ç¸®ï¼Œé‡è¤‡çš„é ­éƒ¨åªç™¼é€ä¸€æ¬¡ï¼š
```
[Stream 1] Full Headers (500 bytes)
[Stream 2] :path: /analyze  (åªç™¼é€è®ŠåŒ–çš„éƒ¨åˆ†ï¼Œ20 bytes)
[Stream 3] :path: /config   (20 bytes)
```

ğŸ”¸ **æœå‹™å™¨æ¨é€ï¼ˆServer Pushï¼‰**

é›–ç„¶ gRPC ä¸å¸¸ç”¨é€™å€‹ç‰¹æ€§ï¼Œä½† HTTP/2 æ”¯æ´æœå‹™å™¨ä¸»å‹•æ¨é€è³‡æºã€‚

ğŸ”¸ **æµé‡æ§åˆ¶ï¼ˆFlow Controlï¼‰**

é˜²æ­¢æœå‹™å™¨ç™¼é€éå¤šæ•¸æ“šæ·¹æ²’å®¢æˆ¶ç«¯ï¼š
```
Client: "æˆ‘åªèƒ½ç·©è¡ 64KB æ•¸æ“š"
Server: [ç™¼é€ 64KB]
Server: [ç­‰å¾… Client ç¢ºèª]
Client: "æˆ‘è™•ç†å®Œäº†ï¼Œå¯ä»¥å†ç™¼ 64KB"
Server: [ç¹¼çºŒç™¼é€]
```

---

## gRPC åœ¨ Sail ä¸­çš„ä½¿ç”¨

### Sail ä½¿ç”¨ Tonic æ¡†æ¶

ğŸ”¸ **Tonic = Rust çš„ gRPC æ¡†æ¶**

æª”æ¡ˆä½ç½®ï¼š`Cargo.toml`

```toml
[dependencies]
tonic = "0.12"           # gRPC æ ¸å¿ƒ
prost = "0.13"           # Protocol Buffers åºåˆ—åŒ–
tonic-build = "0.12"     # ç·¨è­¯æ™‚å¾ .proto ç”Ÿæˆ Rust ä»£ç¢¼
```

ğŸ”¸ **ä»£ç¢¼ç”Ÿæˆæµç¨‹**

```
1. ç·¨å¯« .proto æ–‡ä»¶
   â†“
2. Cargo build æ™‚ï¼Œtonic-build è‡ªå‹•ç”Ÿæˆ Rust ä»£ç¢¼
   â†“
3. ç”Ÿæˆçš„ä»£ç¢¼åŒ…å«ï¼š
   - Message structs (ExecutePlanRequest, ExecutePlanResponse)
   - Service traits (SparkConnectService)
   - Client stubs (SparkConnectServiceClient)
   - Server builders (SparkConnectServiceServer)
```

ğŸ”¸ **è‡ªå‹•ç”Ÿæˆçš„ä»£ç¢¼ç¯„ä¾‹**

æª”æ¡ˆä½ç½®ï¼š`target/debug/build/sail-spark-connect-xxx/out/spark.connect.rs`

```rust
// è‡ªå‹•ç”Ÿæˆçš„æ¶ˆæ¯çµæ§‹
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExecutePlanRequest {
    #[prost(string, tag = "1")]
    pub session_id: ::prost::alloc::string::String,
    #[prost(message, optional, tag = "2")]
    pub user_context: ::core::option::Option<UserContext>,
    #[prost(message, optional, tag = "3")]
    pub plan: ::core::option::Option<Plan>,
}

// è‡ªå‹•ç”Ÿæˆçš„ Service trait
#[async_trait]
pub trait SparkConnectService: Send + Sync + 'static {
    type ExecutePlanStream: futures::Stream<Item = Result<ExecutePlanResponse, Status>>
        + Send
        + 'static;

    async fn execute_plan(
        &self,
        request: tonic::Request<ExecutePlanRequest>,
    ) -> Result<tonic::Response<Self::ExecutePlanStream>, tonic::Status>;

    async fn config(
        &self,
        request: tonic::Request<ConfigRequest>,
    ) -> Result<tonic::Response<ConfigResponse>, tonic::Status>;

    // ... å…¶ä»–æ–¹æ³•
}
```

---

## gRPC é–‹ç™¼çš„ä¸‰æ­¥é©Ÿèˆ‡æºç¢¼å°æ‡‰

é€™ä¸€ç¯€è©³ç´°èªªæ˜ gRPC é–‹ç™¼çš„å®Œæ•´æµç¨‹ï¼Œä»¥åŠåœ¨ Sail ä¸­æ¯ä¸€æ­¥çš„å¯¦éš›ä»£ç¢¼ä½ç½®ã€‚

### ğŸ”¸ æ­¥é©Ÿ 1: ç·¨å¯« .proto æ–‡ä»¶ï¼ˆå”è­°å®šç¾©ï¼‰

#### 1.1 Service å®šç¾©

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/proto/spark/connect/base.proto:1092-1135`

```protobuf
// Main interface for the SparkConnect service.
service SparkConnectService {

  // Executes a request that contains the query and returns a stream of Response.
  // It is guaranteed that there is at least one ARROW batch returned even if the result set is empty.
  rpc ExecutePlan(ExecutePlanRequest) returns (stream ExecutePlanResponse) {}

  // Analyzes a query and returns a AnalyzeResponse containing metadata about the query.
  rpc AnalyzePlan(AnalyzePlanRequest) returns (AnalyzePlanResponse) {}

  // Update or fetch the configurations and returns a ConfigResponse containing the result.
  rpc Config(ConfigRequest) returns (ConfigResponse) {}

  // Add artifacts to the session and returns a AddArtifactsResponse containing metadata about
  // the added artifacts.
  rpc AddArtifacts(stream AddArtifactsRequest) returns (AddArtifactsResponse) {}

  // Check statuses of artifacts in the session and returns them in a ArtifactStatusesResponse
  rpc ArtifactStatus(ArtifactStatusesRequest) returns (ArtifactStatusesResponse) {}

  // Interrupts running executions
  rpc Interrupt(InterruptRequest) returns (InterruptResponse) {}

  // Reattach to an existing reattachable execution.
  // The ExecutePlan must have been started with ReattachOptions.reattachable=true.
  // If the ExecutePlanResponse stream ends without a ResultComplete message, there is more to
  // continue. If there is a ResultComplete, the client should use ReleaseExecute with
  rpc ReattachExecute(ReattachExecuteRequest) returns (stream ExecutePlanResponse) {}

  // Release an reattachable execution, or parts thereof.
  // The ExecutePlan must have been started with ReattachOptions.reattachable=true.
  // Non reattachable executions are released automatically and immediately after the ExecutePlan
  // RPC and ReleaseExecute may not be used.
  rpc ReleaseExecute(ReleaseExecuteRequest) returns (ReleaseExecuteResponse) {}

  // Release a session.
  // All the executions in the session will be released. Any further requests for the session with
  // that session_id for the given user_id will fail. If the session didn't exist or was already
  // released, this is a noop.
  rpc ReleaseSession(ReleaseSessionRequest) returns (ReleaseSessionResponse) {}

  // FetchErrorDetails retrieves the matched exception with details based on a provided error id.
  rpc FetchErrorDetails(FetchErrorDetailsRequest) returns (FetchErrorDetailsResponse) {}
}
```

ğŸ”¸ **é—œéµè§€å¯Ÿ**

é€™å€‹ service å®šç¾©åŒ…å«ï¼š
- **10 å€‹ RPC æ–¹æ³•**ï¼ˆExecutePlan, AnalyzePlan, Config, AddArtifacts, ArtifactStatus, Interrupt, ReattachExecute, ReleaseExecute, ReleaseSession, FetchErrorDetailsï¼‰
- **å…©ç¨®æµæ¨¡å¼**ï¼š`stream` é—œéµå­—è¡¨ç¤ºæµå¼å‚³è¼¸
  - `returns (stream ExecutePlanResponse)` = Server Streamingï¼ˆæœå‹™å™¨æµå¼è¿”å›å¤šå€‹éŸ¿æ‡‰ï¼‰
  - `rpc AddArtifacts(stream AddArtifactsRequest)` = Client Streamingï¼ˆå®¢æˆ¶ç«¯æµå¼ç™¼é€å¤šå€‹è«‹æ±‚ï¼‰

#### 1.2 Message å®šç¾©ç¯„ä¾‹

åŒæ¨£åœ¨ `base.proto` ä¸­å®šç¾©çš„æ¶ˆæ¯çµæ§‹ï¼š

```protobuf
message ExecutePlanRequest {
  // (Required) The session_id specifies a spark session for a user id
  string session_id = 1;

  // (Optional) User context
  UserContext user_context = 2;

  // (Required) The logical plan to be executed / analyzed.
  Plan plan = 3;

  // (Optional) Unique ID for the operation
  optional string operation_id = 6;

  // (Optional) Tags to attach to the query
  repeated string tags = 7;

  // (Optional) Request options
  repeated RequestOption request_options = 8;
}

message Plan {
  oneof op_type {
    Relation root = 1;      // DataFrame API æ“ä½œ
    Command command = 2;    // SQL å‘½ä»¤æˆ–å…¶ä»–å‘½ä»¤
  }
}
```

ğŸ”¸ **Protobuf èªæ³•è¦é»**

- `= 1, = 2, = 3` æ˜¯å­—æ®µç·¨è™Ÿï¼ˆç”¨æ–¼äºŒé€²åˆ¶åºåˆ—åŒ–ï¼Œä¸èƒ½æ”¹è®Šï¼‰
- `optional` = å¯é¸å­—æ®µï¼ˆProtobuf 3 æ–°å¢ï¼‰
- `repeated` = æ•¸çµ„/åˆ—è¡¨
- `oneof` = è¯åˆé¡å‹ï¼ˆåªèƒ½æœ‰ä¸€å€‹å­—æ®µè¢«è¨­ç½®ï¼‰

---

### ğŸ”¸ æ­¥é©Ÿ 2: Cargo build æ™‚è‡ªå‹•ç”Ÿæˆ Rust ä»£ç¢¼

#### 2.1 é…ç½®ä»£ç¢¼ç”Ÿæˆ

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/build.rs:7-42`

```rust
fn build_proto() -> Result<(), Box<dyn std::error::Error>> {
    // 1. è¨­ç½®è¼¸å‡ºç›®éŒ„ï¼ˆtarget/debug/build/sail-spark-connect-<hash>/out/ï¼‰
    let out_dir = PathBuf::from(std::env::var("OUT_DIR")?);
    let descriptor_path = out_dir.join("spark_connect_descriptor.bin");

    // 2. é…ç½® prostï¼ˆProtocol Buffers Rust å¯¦ä½œï¼‰
    let mut config = Config::new();
    config.skip_debug([
        "spark.connect.LocalRelation",
        "spark.connect.ExecutePlanResponse.ArrowBatch",
    ]);

    // 3. ä½¿ç”¨ tonic-build ç·¨è­¯ proto æ–‡ä»¶
    tonic_prost_build::configure()
        .protoc_arg("--experimental_allow_proto3_optional")  // æ”¯æ´ optional é—œéµå­—
        .file_descriptor_set_path(&descriptor_path)          // ç”Ÿæˆ descriptorï¼ˆç”¨æ–¼åå°„ï¼‰
        .compile_well_known_types(true)                      // åŒ…å« google.protobuf.* é¡å‹
        .extern_path(".google.protobuf", "::pbjson_types")   // ä½¿ç”¨å¤–éƒ¨ JSON é¡å‹
        .build_server(true)                                   // ğŸ”¥ ç”Ÿæˆ Server ç«¯ä»£ç¢¼
        .compile_with_config(
            config,
            &[
                "proto/spark/connect/base.proto",
                "proto/spark/connect/catalog.proto",
                "proto/spark/connect/commands.proto",
                "proto/spark/connect/common.proto",
                "proto/spark/connect/example_plugins.proto",
                "proto/spark/connect/expressions.proto",
                "proto/spark/connect/ml.proto",
                "proto/spark/connect/ml_common.proto",
                "proto/spark/connect/relations.proto",
                "proto/spark/connect/types.proto",
            ],
            &["proto"],
        )?;

    // 4. ç”Ÿæˆ JSON åºåˆ—åŒ–æ”¯æ´ï¼ˆç”¨æ–¼æ—¥èªŒå’Œèª¿è©¦ï¼‰
    let descriptors = std::fs::read(descriptor_path)?;
    pbjson_build::Builder::new()
        .register_descriptors(&descriptors)?
        .build(&[".spark.connect"])?;

    Ok(())
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("cargo:rerun-if-changed=build.rs");
    build_proto()?;       // ç”Ÿæˆ protobuf ä»£ç¢¼
    build_spark_config()?; // ç”Ÿæˆ Spark é…ç½®å¸¸é‡
    Ok(())
}
```

ğŸ”¸ **Rust build.rs çŸ¥è­˜**

- `build.rs` åœ¨ `cargo build` **ä¹‹å‰**åŸ·è¡Œï¼ˆç·¨è­¯æ™‚ä»£ç¢¼ç”Ÿæˆï¼‰
- `OUT_DIR` ç’°å¢ƒè®Šé‡æŒ‡å‘ `target/debug/build/<crate-name>-<hash>/out/`
- `tonic-build` èª¿ç”¨ `protoc` ç·¨è­¯å™¨è§£æ .proto æ–‡ä»¶ï¼Œç”Ÿæˆ Rust ä»£ç¢¼

#### 2.2 ç”Ÿæˆçš„æ–‡ä»¶

åŸ·è¡Œ `cargo build` å¾Œï¼Œç”Ÿæˆï¼š

```bash
target/debug/build/sail-spark-connect-<hash>/out/
â”œâ”€â”€ spark.connect.rs            # Message structs + Service trait + Client + Server
â”œâ”€â”€ spark.connect.serde.rs      # JSON åºåˆ—åŒ–å¯¦ä½œï¼ˆSerde traitsï¼‰
â”œâ”€â”€ spark_connect_descriptor.bin # Protobuf descriptorï¼ˆäºŒé€²åˆ¶ï¼Œç”¨æ–¼åå°„ï¼‰
â””â”€â”€ spark_config.rs             # Spark é…ç½®å¸¸é‡ï¼ˆå¾ JSON ç”Ÿæˆï¼‰
```

æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç¢¼ï¼š

```bash
$ ls -lh target/debug/build/sail-spark-connect-*/out/
-rw-r--r--  353K spark_config.rs
-rw-r--r--  237K spark_connect_descriptor.bin
-rw-r--r--  300K spark.connect.rs              # ğŸ”¥ ä¸»è¦ç”Ÿæˆæ–‡ä»¶
-rw-r--r--  1.7M spark.connect.serde.rs
```

#### 2.3 Include ç”Ÿæˆçš„ä»£ç¢¼

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/lib.rs:18-32`

```rust
pub mod spark {
    #[allow(clippy::all)]  // ç”Ÿæˆçš„ä»£ç¢¼å¯èƒ½ä¸ç¬¦åˆ Clippy è¦ç¯„
    pub mod connect {
        // ğŸ”¥ Include ç·¨è­¯æ™‚ç”Ÿæˆçš„ Rust ä»£ç¢¼
        // é€™å€‹å®å±•é–‹ç‚ºï¼šinclude!(concat!(env!("OUT_DIR"), "/spark.connect.rs"));
        tonic::include_proto!("spark.connect");

        // Include JSON åºåˆ—åŒ–ä»£ç¢¼
        tonic::include_proto!("spark.connect.serde");

        // æš´éœ² protobuf descriptorï¼ˆä¾›åå°„ä½¿ç”¨ï¼‰
        pub const FILE_DESCRIPTOR_SET: &[u8] =
            tonic::include_file_descriptor_set!("spark_connect_descriptor");
    }

    #[allow(clippy::doc_markdown)]
    pub mod config {
        // Include Spark é…ç½®å¸¸é‡
        include!(concat!(env!("OUT_DIR"), "/spark_config.rs"));
    }
}
```

ğŸ”¸ **Rust å®çŸ¥è­˜**

- `include!()` å®æœƒåœ¨ç·¨è­¯æ™‚å°‡æ–‡ä»¶å…§å®¹**åŸåœ°å±•é–‹**ï¼ˆå°±åƒ C çš„ `#include`ï¼‰
- `env!("OUT_DIR")` åœ¨ç·¨è­¯æ™‚ç²å–ç’°å¢ƒè®Šé‡
- `concat!()` å®æ‹¼æ¥å­—ä¸²

---

### ğŸ”¸ æ­¥é©Ÿ 3: ç”Ÿæˆçš„ä»£ç¢¼åŒ…å«ä¸‰å¤§éƒ¨åˆ†

#### 3.1 Message Structsï¼ˆæ¶ˆæ¯çµæ§‹é«”ï¼‰

ç”Ÿæˆä½ç½®ï¼š`target/debug/build/sail-spark-connect-*/out/spark.connect.rs`

```rust
// è‡ªå‹•ç”Ÿæˆçš„ ExecutePlanRequest struct
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExecutePlanRequest {
    /// (Required) The session_id
    #[prost(string, tag = "1")]
    pub session_id: ::prost::alloc::string::String,

    /// (Optional) User context
    #[prost(message, optional, tag = "2")]
    pub user_context: ::core::option::Option<UserContext>,

    /// (Required) The logical plan
    #[prost(message, optional, tag = "3")]
    pub plan: ::core::option::Option<Plan>,

    /// (Optional) Unique ID for the operation
    #[prost(string, optional, tag = "6")]
    pub operation_id: ::core::option::Option<::prost::alloc::string::String>,

    /// (Optional) Tags to attach to the query
    #[prost(string, repeated, tag = "7")]
    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,

    /// (Optional) Request options
    #[prost(message, repeated, tag = "8")]
    pub request_options: ::prost::alloc::vec::Vec<execute_plan_request::RequestOption>,
}

// è‡ªå‹•ç”Ÿæˆçš„ ExecutePlanResponse struct
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExecutePlanResponse {
    /// The session_id
    #[prost(string, tag = "1")]
    pub session_id: ::prost::alloc::string::String,

    /// The operation_id
    #[prost(string, tag = "3")]
    pub operation_id: ::prost::alloc::string::String,

    /// The response_id (unique for each response in the stream)
    #[prost(string, tag = "4")]
    pub response_id: ::prost::alloc::string::String,

    /// Response content (oneof = åªèƒ½æœ‰ä¸€å€‹)
    #[prost(oneof = "execute_plan_response::ResponseType", tags = "2, 5, 7, ...")]
    pub response_type: ::core::option::Option<execute_plan_response::ResponseType>,
}

// åµŒå¥—çš„ enumï¼ˆå°æ‡‰ protobuf çš„ oneofï¼‰
pub mod execute_plan_response {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ResponseType {
        #[prost(message, tag = "2")]
        ArrowBatch(super::ArrowBatch),

        #[prost(message, tag = "5")]
        SqlCommandResult(Box<super::SqlCommandResult>),

        #[prost(message, tag = "7")]
        ResultComplete(super::ResultComplete),

        // ... å…¶ä»–é¡å‹
    }
}
```

ğŸ”¸ **Prost å±¬æ€§å®èªªæ˜**

- `#[prost(string, tag = "1")]` è¡¨ç¤ºå­—æ®µ 1 æ˜¯å­—ä¸²é¡å‹
- `#[prost(message, optional, tag = "2")]` è¡¨ç¤ºå­—æ®µ 2 æ˜¯å¯é¸çš„åµŒå¥—æ¶ˆæ¯
- `#[prost(string, repeated, tag = "7")]` è¡¨ç¤ºå­—æ®µ 7 æ˜¯å­—ä¸²æ•¸çµ„
- `#[prost(oneof = "...")]` è¡¨ç¤ºé€™æ˜¯è¯åˆé¡å‹å­—æ®µ

#### 3.2 Service Traitï¼ˆServer ç«¯è¦å¯¦ä½œçš„ traitï¼‰

ç”Ÿæˆä½ç½®ï¼š`target/debug/build/sail-spark-connect-*/out/spark.connect.rs:6077+`

```rust
/// Generated trait containing gRPC methods that should be implemented
/// for use with SparkConnectServiceServer.
#[async_trait]
pub trait SparkConnectService: Send + Sync + 'static {

    /// Server streaming response type for the ExecutePlan method.
    type ExecutePlanStream: tonic::codegen::tokio_stream::Stream<
            Item = std::result::Result<super::ExecutePlanResponse, tonic::Status>,
        >
        + Send
        + 'static;

    /// Executes a request that contains the query and returns a stream of Response.
    /// It is guaranteed that there is at least one ARROW batch returned even if the result set is empty.
    async fn execute_plan(
        &self,
        request: tonic::Request<super::ExecutePlanRequest>,
    ) -> std::result::Result<
        tonic::Response<Self::ExecutePlanStream>,
        tonic::Status,
    >;

    /// Analyzes a query and returns a AnalyzeResponse containing metadata about the query.
    async fn analyze_plan(
        &self,
        request: tonic::Request<super::AnalyzePlanRequest>,
    ) -> std::result::Result<
        tonic::Response<super::AnalyzePlanResponse>,
        tonic::Status,
    >;

    /// Update or fetch the configurations and returns a ConfigResponse containing the result.
    async fn config(
        &self,
        request: tonic::Request<super::ConfigRequest>,
    ) -> std::result::Result<tonic::Response<super::ConfigResponse>, tonic::Status>;

    /// Add artifacts to the session
    async fn add_artifacts(
        &self,
        request: tonic::Request<tonic::Streaming<super::AddArtifactsRequest>>,
    ) -> std::result::Result<
        tonic::Response<super::AddArtifactsResponse>,
        tonic::Status,
    >;

    /// Check statuses of artifacts
    async fn artifact_status(
        &self,
        request: tonic::Request<super::ArtifactStatusesRequest>,
    ) -> std::result::Result<
        tonic::Response<super::ArtifactStatusesResponse>,
        tonic::Status,
    >;

    /// Interrupts running executions
    async fn interrupt(
        &self,
        request: tonic::Request<super::InterruptRequest>,
    ) -> std::result::Result<
        tonic::Response<super::InterruptResponse>,
        tonic::Status,
    >;

    /// Server streaming response type for the ReattachExecute method.
    type ReattachExecuteStream: tonic::codegen::tokio_stream::Stream<
            Item = std::result::Result<super::ExecutePlanResponse, tonic::Status>,
        >
        + Send
        + 'static;

    /// Reattach to an existing reattachable execution.
    async fn reattach_execute(
        &self,
        request: tonic::Request<super::ReattachExecuteRequest>,
    ) -> std::result::Result<
        tonic::Response<Self::ReattachExecuteStream>,
        tonic::Status,
    >;

    /// Release an reattachable execution
    async fn release_execute(
        &self,
        request: tonic::Request<super::ReleaseExecuteRequest>,
    ) -> std::result::Result<
        tonic::Response<super::ReleaseExecuteResponse>,
        tonic::Status,
    >;

    /// Release a session
    async fn release_session(
        &self,
        request: tonic::Request<super::ReleaseSessionRequest>,
    ) -> std::result::Result<
        tonic::Response<super::ReleaseSessionResponse>,
        tonic::Status,
    >;

    /// Fetch error details
    async fn fetch_error_details(
        &self,
        request: tonic::Request<super::FetchErrorDetailsRequest>,
    ) -> std::result::Result<
        tonic::Response<super::FetchErrorDetailsResponse>,
        tonic::Status,
    >;
}
```

ğŸ”¸ **é—œéµå‹åˆ¥**

- `tonic::Request<T>` åŒ…å«è«‹æ±‚æ¶ˆæ¯ + metadataï¼ˆheadersï¼‰
- `tonic::Response<T>` åŒ…å«éŸ¿æ‡‰æ¶ˆæ¯ + metadata
- `tonic::Status` è¡¨ç¤º gRPC éŒ¯èª¤ï¼ˆé¡ä¼¼ HTTP ç‹€æ…‹ç¢¼ï¼‰
- `tonic::Streaming<T>` è¡¨ç¤ºå®¢æˆ¶ç«¯æµå¼è¼¸å…¥
- `type ExecutePlanStream` æ˜¯é—œè¯é¡å‹ï¼ˆAssociated Typeï¼‰ï¼Œå¯¦ä½œè€…éœ€è¦æŒ‡å®šå…·é«”çš„æµé¡å‹

#### 3.3 å¯¦ä½œ Service Traitï¼ˆSail çš„å¯¦ä½œï¼‰

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/server.rs:24-474`

```rust
// server.rs:24-32
#[derive(Debug)]
pub struct SparkConnectServer {
    session_manager: SessionManager,
}

impl SparkConnectServer {
    pub fn new(session_manager: SessionManager) -> Self {
        Self { session_manager }
    }
}

// server.rs:50-161
#[tonic::async_trait]  // ğŸ”¥ é€™å€‹å®è®“ trait å¯ä»¥åŒ…å« async æ–¹æ³•
impl SparkConnectService for SparkConnectServer {
    // æŒ‡å®šæµé¡å‹ç‚ºæˆ‘å€‘è‡ªå®šç¾©çš„ ExecutePlanResponseStream
    type ExecutePlanStream = ExecutePlanResponseStream;

    // ğŸ”¥ å¯¦ä½œ execute_plan æ–¹æ³•
    async fn execute_plan(
        &self,
        request: Request<ExecutePlanRequest>,
    ) -> Result<Response<Self::ExecutePlanStream>, Status> {
        // 1. å–å‡ºè«‹æ±‚å…§å®¹
        let request = request.into_inner();
        debug!("{request:?}");

        // 2. æ§‹å»º SessionKey
        let session_key = SessionKey {
            user_id: request.user_context.map(|u| u.user_id).unwrap_or_default(),
            session_id: request.session_id,
        };

        // 3. æ§‹å»ºåŸ·è¡Œå™¨å…ƒæ•¸æ“š
        let metadata = ExecutorMetadata {
            operation_id: request.operation_id.unwrap_or_else(|| Uuid::new_v4().to_string()),
            tags: request.tags,
            reattachable: is_reattachable(&request.request_options),
        };

        // 4. ç²å–æˆ–å‰µå»º SessionContextï¼ˆé€™è£¡æœƒè§¸ç™¼ Actor é€šè¨Šï¼‰
        let ctx = self.session_manager.get_or_create_session_context(session_key).await?;

        // 5. è§£æ Planï¼ˆé€™æ˜¯ protobuf oneof å­—æ®µï¼‰
        let Plan { op_type: op } = request.plan.required("plan")?;
        let op = op.required("plan op")?;

        // 6. æ ¹æ“šè¨ˆç•«é¡å‹åˆ†ç™¼åˆ°ä¸åŒçš„è™•ç†å‡½æ•¸
        let stream = match op {
            plan::OpType::Root(relation) => {
                // DataFrame API èª¿ç”¨
                service::handle_execute_relation(&ctx, relation, metadata).await?
            }
            plan::OpType::Command(Command { command_type: command }) => {
                // SQL æˆ–å‘½ä»¤èª¿ç”¨
                let command = command.required("command")?;
                match command {
                    CommandType::SqlCommand(sql) => {
                        service::handle_execute_sql_command(&ctx, sql, metadata).await?
                    }
                    CommandType::WriteOperation(write) => {
                        service::handle_execute_write_operation(&ctx, write, metadata).await?
                    }
                    CommandType::CreateDataframeView(view) => {
                        service::handle_execute_create_dataframe_view(&ctx, view, metadata).await?
                    }
                    // ... å…¶ä»–å‘½ä»¤é¡å‹
                    _ => return Err(Status::unimplemented("command type not supported")),
                }
            }
        };

        // 7. è¿”å›éŸ¿æ‡‰æµ
        Ok(Response::new(stream))
    }

    // å¯¦ä½œå…¶ä»–æ–¹æ³•ï¼ˆanalyze_plan, config, add_artifacts, ...ï¼‰
    async fn analyze_plan(
        &self,
        request: Request<AnalyzePlanRequest>,
    ) -> Result<Response<AnalyzePlanResponse>, Status> {
        let request = request.into_inner();
        debug!("{request:?}");

        let session_key = SessionKey {
            user_id: request.user_context.map(|u| u.user_id).unwrap_or_default(),
            session_id: request.session_id.clone(),
        };

        let ctx = self.session_manager.get_or_create_session_context(session_key).await?;

        let analyze = request.analyze.required("analyze")?;
        let result = match analyze {
            Analyze::Schema(schema) => {
                let schema = service::handle_analyze_schema(&ctx, schema).await?;
                Some(analyze_plan_response::Result::Schema(schema))
            }
            Analyze::Explain(explain) => {
                let explain = service::handle_analyze_explain(&ctx, explain).await?;
                Some(analyze_plan_response::Result::Explain(explain))
            }
            // ... å…¶ä»–åˆ†æé¡å‹
            _ => None,
        };

        let response = AnalyzePlanResponse {
            session_id: request.session_id.clone(),
            server_side_session_id: request.session_id,
            result,
        };

        debug!("{response:?}");
        Ok(Response::new(response))
    }

    async fn config(
        &self,
        request: Request<ConfigRequest>,
    ) -> Result<Response<ConfigResponse>, Status> {
        // ... é…ç½®ç®¡ç†å¯¦ä½œ
    }

    // ... å¯¦ä½œæ‰€æœ‰å…¶ä»– RPC æ–¹æ³•
}
```

ğŸ”¸ **Rust ç•°æ­¥çŸ¥è­˜**

- `#[tonic::async_trait]` å®è™•ç† trait ä¸­çš„ async æ–¹æ³•ï¼ˆRust ç›®å‰ä¸åŸç”Ÿæ”¯æ´ trait ä¸­çš„ async fnï¼‰
- `await?` çµ„åˆäº†å…©å€‹æ“ä½œï¼š
  - `await` ç­‰å¾…ç•°æ­¥æ“ä½œå®Œæˆ
  - `?` å¦‚æœçµæœæ˜¯ `Err`ï¼Œç«‹å³è¿”å›éŒ¯èª¤ï¼ˆearly returnï¼‰

#### 3.4 Server Builderï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰

ç”Ÿæˆä½ç½®ï¼š`target/debug/build/sail-spark-connect-*/out/spark.connect.rs`

```rust
/// Generated server implementations.
pub mod spark_connect_service_server {
    use tonic::codegen::*;

    /// Server builder for SparkConnectService
    pub struct SparkConnectServiceServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }

    impl<T: SparkConnectService> SparkConnectServiceServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }

        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }

        /// Enable gzip compression
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }

        /// Enable gzip compression for responses
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }

        /// Set max message size for decoding
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }

        /// Set max message size for encoding
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }

    // ğŸ”¥ å¯¦ä½œ tonic::codegen::Service traitï¼ˆé€™æ˜¯ Tonic çš„æ ¸å¿ƒ traitï¼‰
    impl<T: SparkConnectService> tonic::codegen::Service<http::Request<Body>>
        for SparkConnectServiceServer<T>
    {
        type Response = http::Response<tonic::body::BoxBody>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;

        fn poll_ready(&mut self, _cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }

        // ğŸ”¥ æ ¹æ“š HTTP è«‹æ±‚è·¯å¾‘è·¯ç”±åˆ°å°æ‡‰çš„ RPC æ–¹æ³•
        fn call(&mut self, req: http::Request<Body>) -> Self::Future {
            let inner = self.inner.clone();
            match req.uri().path() {
                "/spark.connect.SparkConnectService/ExecutePlan" => {
                    // ååºåˆ—åŒ–è«‹æ±‚ -> èª¿ç”¨ trait æ–¹æ³• -> åºåˆ—åŒ–éŸ¿æ‡‰
                    // ... (ç”Ÿæˆçš„æ¨£æ¿ä»£ç¢¼ï¼Œè™•ç†åºåˆ—åŒ–ã€å£“ç¸®ã€éŒ¯èª¤è½‰æ›ç­‰)
                }
                "/spark.connect.SparkConnectService/AnalyzePlan" => {
                    // ... é¡ä¼¼è™•ç†
                }
                "/spark.connect.SparkConnectService/Config" => {
                    // ... é¡ä¼¼è™•ç†
                }
                // ... å…¶ä»–è·¯å¾‘
                _ => {
                    Box::pin(async move {
                        Ok(http::Response::builder()
                            .status(404)
                            .body(empty_body())
                            .unwrap())
                    })
                }
            }
        }
    }
}
```

ğŸ”¸ **HTTP è·¯å¾‘æ˜ å°„**

gRPC ä½¿ç”¨ HTTP/2 ä½œç‚ºå‚³è¼¸å±¤ï¼Œæ¯å€‹ RPC æ–¹æ³•å°æ‡‰ä¸€å€‹ HTTP è·¯å¾‘ï¼š

```
POST /spark.connect.SparkConnectService/ExecutePlan
POST /spark.connect.SparkConnectService/AnalyzePlan
POST /spark.connect.SparkConnectService/Config
...
```

è·¯å¾‘æ ¼å¼ï¼š`/<package>.<service>/<method>`

#### 3.5 Client Stubsï¼ˆè‡ªå‹•ç”Ÿæˆï¼Œç”¨æ–¼æ¸¬è©¦æˆ–å®¢æˆ¶ç«¯ï¼‰

ç”Ÿæˆä½ç½®ï¼š`target/debug/build/sail-spark-connect-*/out/spark.connect.rs:5692+`

```rust
/// Generated client implementations.
pub mod spark_connect_service_client {
    use tonic::codegen::*;

    /// Client for SparkConnectService
    pub struct SparkConnectServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }

    impl SparkConnectServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }

    impl<T> SparkConnectServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }

        /// Executes a request that contains the query and returns a stream of Response.
        pub async fn execute_plan(
            &mut self,
            request: impl tonic::IntoRequest<super::ExecutePlanRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::ExecutePlanResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;

            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/spark.connect.SparkConnectService/ExecutePlan",
            );

            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("spark.connect.SparkConnectService", "ExecutePlan"));

            self.inner.server_streaming(req, path, codec).await
        }

        /// Analyzes a query
        pub async fn analyze_plan(
            &mut self,
            request: impl tonic::IntoRequest<super::AnalyzePlanRequest>,
        ) -> std::result::Result<
            tonic::Response<super::AnalyzePlanResponse>,
            tonic::Status,
        > {
            // ... é¡ä¼¼å¯¦ä½œï¼ˆunary RPCï¼‰
        }

        /// Update or fetch configurations
        pub async fn config(
            &mut self,
            request: impl tonic::IntoRequest<super::ConfigRequest>,
        ) -> std::result::Result<tonic::Response<super::ConfigResponse>, tonic::Status> {
            // ... é¡ä¼¼å¯¦ä½œ
        }

        // ... å…¶ä»– RPC æ–¹æ³•
    }
}
```

ğŸ”¸ **Client ä½¿ç”¨ç¯„ä¾‹ï¼ˆRust å®¢æˆ¶ç«¯ï¼‰**

```rust
use spark::connect::spark_connect_service_client::SparkConnectServiceClient;
use spark::connect::{ExecutePlanRequest, Plan};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // é€£æ¥åˆ°æœå‹™å™¨
    let mut client = SparkConnectServiceClient::connect("http://localhost:50051").await?;

    // æ§‹å»ºè«‹æ±‚
    let request = ExecutePlanRequest {
        session_id: "test-session".to_string(),
        plan: Some(Plan { /* ... */ }),
        ..Default::default()
    };

    // èª¿ç”¨ RPC æ–¹æ³•
    let mut response_stream = client.execute_plan(request).await?.into_inner();

    // è™•ç†éŸ¿æ‡‰æµ
    while let Some(response) = response_stream.message().await? {
        println!("Received: {:?}", response);
    }

    Ok(())
}
```

---

### ğŸ”¸ å¯¦éš›ä½¿ç”¨ï¼šå•Ÿå‹• Sail Server

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/entrypoint.rs:13-36`

```rust
pub async fn serve(options: ServerOptions) -> Result<(), Box<dyn std::error::Error>> {
    // 1. å‰µå»º SessionManagerï¼ˆç®¡ç†æ‰€æœ‰ç”¨æˆ¶ sessionï¼‰
    let session_manager = SessionManager::new(SessionManagerOptions {
        config: options.config.clone(),
        runtime: options.runtime.clone(),
    });

    // 2. å‰µå»ºæˆ‘å€‘çš„ SparkConnectServerï¼ˆå¯¦ä½œäº† SparkConnectService traitï¼‰
    let service = SparkConnectServer::new(session_manager);

    // 3. ğŸ”¥ ä½¿ç”¨ç”Ÿæˆçš„ Server Builder å°‡æˆ‘å€‘çš„å¯¦ä½œåŒ…è£æˆ gRPC service
    use crate::spark::connect::spark_connect_service_server::SparkConnectServiceServer;
    let service = SparkConnectServiceServer::new(service);

    // 4. ä½¿ç”¨ Tonic çš„ ServerBuilder å•Ÿå‹• gRPC æœå‹™å™¨
    let builder = ServerBuilder::new(&options.server_options)?;
    builder
        .serve(service, options.shutdown_signal)
        .await
        .map_err(Into::into)
}
```

æª”æ¡ˆä½ç½®ï¼š`crates/sail-server/src/builder.rs:101-124`

```rust
pub async fn serve<S>(
    self,
    service: S,
    shutdown_signal: ShutdownSignal,
) -> Result<(), Box<dyn std::error::Error>>
where
    S: Service<Request<Body>, Response = Response<BoxBody>, Error = Infallible>
        + NamedService
        + Clone
        + Send
        + 'static,
    S::Future: Send + 'static,
{
    let listener = TcpListener::bind(&self.addr).await?;
    let addr = listener.local_addr()?;
    info!("Sail Spark Connect server listening on {addr}");

    // ğŸ”¥ å•Ÿå‹• Tonic gRPC æœå‹™å™¨
    tonic::transport::Server::builder()
        .http2_keepalive_interval(self.options.http2_keepalive_interval)
        .http2_keepalive_timeout(self.options.http2_keepalive_timeout)
        .http2_adaptive_window(self.options.http2_adaptive_window)
        .tcp_nodelay(self.options.nodelay)
        .tcp_keepalive(self.options.keepalive)
        .add_service(service)  // ğŸ”¥ è¨»å†Šæˆ‘å€‘çš„ gRPC service
        .serve_with_incoming_shutdown(
            TcpListenerStream::new(listener),
            shutdown_signal.create_future(),
        )
        .await?;

    Ok(())
}
```

---

## ç¸½çµï¼šå®Œæ•´çš„ä»£ç¢¼å°æ‡‰è¡¨

| æ­¥é©Ÿ | ä½ç½® | èªªæ˜ |
|------|------|------|
| **1. å®šç¾© .proto** | `crates/sail-spark-connect/proto/spark/connect/base.proto:1092-1135` | å®šç¾© `service SparkConnectService` å’Œæ‰€æœ‰ RPC æ–¹æ³• |
| **1. å®šç¾© Message** | `crates/sail-spark-connect/proto/spark/connect/base.proto` | å®šç¾© `ExecutePlanRequest`, `ExecutePlanResponse` ç­‰æ¶ˆæ¯ |
| **2. é…ç½®ä»£ç¢¼ç”Ÿæˆ** | `crates/sail-spark-connect/build.rs:7-42` | ä½¿ç”¨ `tonic-build` åœ¨ç·¨è­¯æ™‚ç”Ÿæˆä»£ç¢¼ |
| **2. ç”Ÿæˆçš„ä»£ç¢¼** | `target/debug/build/sail-spark-connect-*/out/spark.connect.rs` | åŒ…å« Message structsã€Service traitã€Clientã€Server builder |
| **2. Include ä»£ç¢¼** | `crates/sail-spark-connect/src/lib.rs:18-32` | ä½¿ç”¨ `tonic::include_proto!` å¼•å…¥ç”Ÿæˆçš„ä»£ç¢¼ |
| **3. å¯¦ä½œ Server Trait** | `crates/sail-spark-connect/src/server.rs:50-474` | å¯¦ä½œ `SparkConnectService` trait çš„æ‰€æœ‰æ–¹æ³• |
| **3. å•Ÿå‹• Server** | `crates/sail-spark-connect/src/entrypoint.rs:13-36` | ä½¿ç”¨ç”Ÿæˆçš„ `SparkConnectServiceServer` å•Ÿå‹•æœå‹™ |

---

## gRPC é€šè¨Šæµç¨‹åœ–

```
PySpark Client                                          Sail Server
     |                                                       |
     | 1. å‰µå»º gRPC channel                                  |
     |    grpc.insecure_channel('localhost:50051')          |
     |------------------------------------------------       |
     |    å»ºç«‹ TCP é€£æ¥ (port 50051)                         |
     |    HTTP/2 SETTINGS å”å•†                               |
     |    <---------- TCP Handshake -----------------------> |
     |                                                       |
     | 2. å‰µå»º gRPC stub                                     |
     |    stub = SparkConnectServiceStub(channel)           |
     |                                                       |
     | 3. èª¿ç”¨ RPC æ–¹æ³•                                      |
     |    response = stub.ExecutePlan(request)              |
     |                                                       |
     | 4. åºåˆ—åŒ– Protobuf                                    |
     |    request_bytes = serialize(ExecutePlanRequest)     |
     |                                                       |
     | 5. ç™¼é€ HTTP/2 POST                                   |
     |    POST /spark.connect.SparkConnectService/ExecutePlan |
     |    Content-Type: application/grpc                    |
     |    Body: [protobuf bytes]                            |
     |-----------------------------------------------------> |
     |                                                       | 6. Tonic æ¥æ”¶è«‹æ±‚
     |                                                       |    router.route("/ExecutePlan")
     |                                                       |    â†“
     |                                                       | 7. ååºåˆ—åŒ– Protobuf
     |                                                       |    request = deserialize(bytes)
     |                                                       |    â†“
     |                                                       | 8. èª¿ç”¨å¯¦ä½œæ–¹æ³•
     |                                                       |    SparkConnectServer::execute_plan()
     |                                                       |    â†“
     |                                                       | 9. è™•ç†é‚è¼¯
     |                                                       |    session_manager.get_or_create()
     |                                                       |    resolve_and_execute_plan()
     |                                                       |    â†“
     |                                                       | 10. è¿”å› Stream
     | 11. æ¥æ”¶ç¬¬ä¸€å€‹éŸ¿æ‡‰                                     |     â†“
     | <-----------------------------------------------------| 11. åºåˆ—åŒ–ä¸¦ç™¼é€
     |    HTTP/2 Response (stream ID 1)                     |     ExecutePlanResponse #1
     |    [protobuf bytes]                                  |
     |                                                       |
     | 12. ååºåˆ—åŒ–                                          |
     |     response1 = deserialize(bytes)                   |
     |                                                       |
     | 13. æ¥æ”¶æ›´å¤šéŸ¿æ‡‰                                       |
     | <-----------------------------------------------------|
     |    ExecutePlanResponse #2                            |
     | <-----------------------------------------------------|
     |    ExecutePlanResponse #3 (ResultComplete)           |
     |                                                       |
     | 14. Stream çµæŸ                                       |
     |     for batch in response_stream: ...                |
```

---

## Sail ä¸­çš„ gRPC é…ç½®

ğŸ”¸ **æœå‹™å™¨é…ç½®**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-server/src/builder.rs:22-33`

```rust
pub struct ServerBuilderOptions {
    // ç¦ç”¨ Nagle ç®—æ³•ï¼ˆæ¸›å°‘å»¶é²ï¼Œé©åˆå°åŒ…é »ç¹ç™¼é€ï¼‰
    pub nodelay: bool,  // true

    // TCP keepaliveï¼ˆ60 ç§’ç™¼é€ä¸€æ¬¡æ¢æ¸¬åŒ…ï¼Œé˜²æ­¢é€£æ¥è¢«ä¸­é–“è¨­å‚™é—œé–‰ï¼‰
    pub keepalive: Option<Duration>,  // 60s

    // HTTP/2 keepaliveï¼ˆ60 ç§’ç™¼é€ PING frameï¼‰
    pub http2_keepalive_interval: Option<Duration>,  // 60s

    // HTTP/2 keepalive è¶…æ™‚ï¼ˆ10 ç§’æœªæ”¶åˆ° PONG å‰‡é—œé–‰é€£æ¥ï¼‰
    pub http2_keepalive_timeout: Option<Duration>,  // 10s

    // HTTP/2 è‡ªé©æ‡‰çª—å£ï¼ˆå‹•æ…‹èª¿æ•´æµé‡æ§åˆ¶çª—å£å¤§å°ï¼‰
    pub http2_adaptive_window: Option<bool>,  // true
}
```

ğŸ”¸ **æ¶ˆæ¯å¤§å°é™åˆ¶**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-common/src/config.rs`

```rust
pub const GRPC_MAX_MESSAGE_LENGTH_DEFAULT: usize = 128 * 1024 * 1024;  // 128 MB
```

ç‚ºä»€éº¼éœ€è¦é™åˆ¶ï¼Ÿ
- é˜²æ­¢æƒ¡æ„å®¢æˆ¶ç«¯ç™¼é€è¶…å¤§æ¶ˆæ¯ï¼ˆDoS æ”»æ“Šï¼‰
- é˜²æ­¢å…§å­˜æº¢å‡º

ğŸ”¸ **å£“ç¸®æ”¯æ´**

æª”æ¡ˆä½ç½®ï¼š`crates/sail-spark-connect/src/entrypoint.rs:27-30`

```rust
let service = SparkConnectServiceServer::new(server)
    .accept_compressed(CompressionEncoding::Gzip)   // æ¥å— Gzip å£“ç¸®çš„è«‹æ±‚
    .accept_compressed(CompressionEncoding::Zstd)   // æ¥å— Zstd å£“ç¸®çš„è«‹æ±‚
    .send_compressed(CompressionEncoding::Gzip)     // éŸ¿æ‡‰ä½¿ç”¨ Gzip å£“ç¸®
    .send_compressed(CompressionEncoding::Zstd);    // éŸ¿æ‡‰ä½¿ç”¨ Zstd å£“ç¸®
```

Zstd vs Gzipï¼š
- Zstd æ›´å¿«ï¼ˆå£“ç¸®/è§£å£“é€Ÿåº¦ï¼‰
- Zstd å£“ç¸®ç‡ç¨å¥½
- Gzip æ›´å»£æ³›æ”¯æ´

---

## å¦‚ä½•èª¿è©¦ gRPC

ğŸ”¸ **ä½¿ç”¨ grpcurlï¼ˆé¡ä¼¼ curl çš„ gRPC å·¥å…·ï¼‰**

```bash
# åˆ—å‡ºæ‰€æœ‰æœå‹™
grpcurl -plaintext localhost:50051 list

# æŸ¥çœ‹æœå‹™æ–¹æ³•
grpcurl -plaintext localhost:50051 list spark.connect.SparkConnectService

# èª¿ç”¨æ–¹æ³•
grpcurl -plaintext -d '{
  "session_id": "test-session",
  "user_context": {"user_id": "test-user"},
  "operation": {
    "get_all": {"prefix": ""}
  }
}' localhost:50051 spark.connect.SparkConnectService/Config
```

ğŸ”¸ **æŸ¥çœ‹ gRPC æ—¥èªŒ**

Tonic ä½¿ç”¨ `tracing` æ¡†æ¶ï¼Œå¯ä»¥å•Ÿç”¨è©³ç´°æ—¥èªŒï¼š

```bash
RUST_LOG=tonic=debug,sail=debug sail spark server
```

ä½ æœƒçœ‹åˆ°ï¼š
```
[tonic] received request: path="/spark.connect.SparkConnectService/ExecutePlan"
[tonic] sending response: status=OK, stream=true
[sail] SessionManager: creating session abc-123-def-456
```

---

## ç¸½çµï¼šç‚ºä»€éº¼ Spark Connect é¸æ“‡ gRPCï¼Ÿ

1. **é«˜æ€§èƒ½**ï¼šäºŒé€²åˆ¶åºåˆ—åŒ– + HTTP/2 å¤šè·¯å¾©ç”¨
2. **åŸç”Ÿæµå¼æ”¯æ´**ï¼šæŸ¥è©¢çµæœå¯èƒ½æœ‰ GB ç´šåˆ¥ï¼Œéœ€è¦æµå¼è¿”å›
3. **å¼·é¡å‹**ï¼šProtocol Buffers ç¢ºä¿å®¢æˆ¶ç«¯å’Œæœå‹™å™¨å¥‘ç´„ä¸€è‡´
4. **è·¨èªè¨€**ï¼šPySparkã€Scala Sparkã€Java Spark éƒ½å¯ä»¥é€£æ¥åŒä¸€å€‹æœå‹™å™¨
5. **é›™å‘æµ**ï¼šæ”¯æ´ä¸­æ–·ã€é‡é€£ç­‰é«˜ç´šåŠŸèƒ½

ç¾åœ¨ä½ å·²ç¶“ç†è§£ gRPC çš„åŸºç¤äº†ï¼Œè®“æˆ‘å€‘é€²å…¥ Sail çš„å¯¦éš›æµç¨‹ï¼
